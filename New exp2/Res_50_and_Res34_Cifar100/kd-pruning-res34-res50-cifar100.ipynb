{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomCrop(32, padding=4),  # Randomly crop the image\n",
    "    transforms.Resize(224),  # Resize to 224x224 for ResNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# No augmentation for validation and test\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Resize to 224x224 for ResNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_val_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=val_test_transform)\n",
    "\n",
    "# Split train_val_dataset into train and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# Apply val_test_transform to the validation set\n",
    "val_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "24.0%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "85.0%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet-50 (Teacher Model)\n",
    "teacher = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer for 10 classes (CIFAR-10)\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, 100)\n",
    "# Move models to device\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'best_teacher_model.pth'\n",
    "# Load the model weights\n",
    "\n",
    "teacher.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Model Test Accuracy: 80.89%\n"
     ]
    }
   ],
   "source": [
    "teacher_accuracy = evaluate(teacher, test_loader, device)\n",
    "print(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "28.1%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet-18 (Student Model)\n",
    "student = models.resnet34(pretrained=True)\n",
    "# Modify the final fully connected layer for 10 classes (CIFAR-10)\n",
    "student.fc = nn.Linear(student.fc.in_features, 100)\n",
    "student = student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_path = 'student_before_pruning.pth'\n",
    "# # Load the model weights\n",
    "# student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits normalization function\n",
    "def normalize(logit):\n",
    "    mean = logit.mean(dim=-1, keepdim=True)\n",
    "    stdv = logit.std(dim=-1, keepdim=True)\n",
    "    return (logit - mean) / (1e-7 + stdv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA-KLD Loss for Classification\n",
    "def cakld_loss(student_logits, teacher_logits, beta_prob):\n",
    "    # Forward KL (student || teacher)\n",
    "    student_log_prob = F.log_softmax(student_logits, dim=1)\n",
    "    teacher_prob = F.softmax(teacher_logits, dim=1)\n",
    "    forward_kl = F.kl_div(student_log_prob, teacher_prob, reduction='batchmean')\n",
    "\n",
    "    # Reverse KL (teacher || student)\n",
    "    teacher_log_prob = F.log_softmax(teacher_logits, dim=1)\n",
    "    student_prob = F.softmax(student_logits, dim=1)\n",
    "    reverse_kl = F.kl_div(teacher_log_prob, student_prob, reduction='batchmean')\n",
    "\n",
    "    # Combined KL loss\n",
    "    kl_loss = beta_prob * reverse_kl + (1 - beta_prob) * forward_kl\n",
    "    return kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    model = model.to(device)  # Ensure model is on the correct device\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model):\n",
    "    total_zeros = 0\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_zeros += torch.sum(param == 0).item()\n",
    "            total_params += param.numel()\n",
    "    return total_zeros / total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "def measure_inference_time(model, test_loader, num_runs=5):\n",
    "    device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Warm-up (one batch to avoid startup cost)\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model(inputs)\n",
    "            break\n",
    "\n",
    "    total_time = 0\n",
    "    total_images = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            for inputs, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                start_time = time.time()\n",
    "                _ = model(inputs)\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += (end_time - start_time)\n",
    "                total_images += batch_size\n",
    "\n",
    "    avg_time_per_image = total_time / total_images\n",
    "    return avg_time_per_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def calculate_model_size(model, filename=\"temp.pth\"):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    size = os.path.getsize(filename) / (1024 * 1024)  # Size in MB\n",
    "    os.remove(filename)\n",
    "    return size\n",
    "\n",
    "def compare_model_sizes(teacher, student, pruned_student):\n",
    "    # Count parameters\n",
    "    teacher_params = count_parameters(teacher)\n",
    "    student_params = count_parameters(student)\n",
    "    pruned_params = count_parameters(pruned_student)\n",
    "    \n",
    "    # Calculate disk size\n",
    "    teacher_size = calculate_model_size(teacher, \"teacher.pth\")\n",
    "    student_size = calculate_model_size(student, \"student.pth\")\n",
    "    pruned_size = calculate_model_size(pruned_student, \"pruned_student.pth\")\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n--- Model Size Comparison ---\")\n",
    "    print(f\"Teacher Model: {teacher_params} parameters, {teacher_size:.2f} MB\")\n",
    "    print(f\"Student Model (Before Pruning): {student_params} parameters, {student_size:.2f} MB\")\n",
    "    print(f\"Student Model (After Pruning): {pruned_params} parameters, {pruned_size:.2f} MB\")\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    compression_ratio = student_size / pruned_size\n",
    "    print(f\"\\nCompression Ratio: {compression_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001, patience=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluate on the validation set\n",
    "        val_accuracy = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(train_loader):.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            torch.save(model.state_dict(), 'best_teacher_model.pth')  # Save the best model\n",
    "            print(f\" New best model saved with validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" No improvement in validation accuracy ({patience_counter}/{patience})\")\n",
    "            \n",
    "            # Stop training if no improvement for 'patience' epochs\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered! No improvement for {patience} epochs.\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model state\n",
    "    model.load_state_dict(torch.load('best_teacher_model.pth'))\n",
    "    print(\"\\nLoading the best model for final evaluation.\")\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_accuracy = evaluate(model, test_loader, device)\n",
    "    print(f\"Test Accuracy with Best Model: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_importance(\n",
    "    teacher, student, data_loader, device, temperature=4.0, alpha=0.5, beta_prob=0.5, accumulation_epochs=3\n",
    "):\n",
    "    importance_scores = {}\n",
    "\n",
    "    # Initialize importance score storage for conv layer weights only\n",
    "    for name, param in student.named_parameters():\n",
    "        if 'weight' in name and len(param.shape) == 4:  # Conv weights only\n",
    "            importance_scores[name] = torch.zeros_like(param.data, device=device)\n",
    "\n",
    "    teacher.to(device).eval()\n",
    "    student.to(device).train()\n",
    "\n",
    "    # Add momentum for gradient accumulation smoothing\n",
    "    momentum = 0.9  # Controls exponential moving average\n",
    "    accumulated_batches = 0  # Track for bias correction\n",
    "\n",
    "    for epoch in range(accumulation_epochs):\n",
    "        print(f\"Accumulation Epoch {epoch+1}/{accumulation_epochs}\")\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            student.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Temperature scaling\n",
    "            student_logits_temp = student_logits / temperature\n",
    "            teacher_logits_temp = teacher_logits / temperature\n",
    "\n",
    "            # Compute losses\n",
    "            distillation_loss = cakld_loss(student_logits_temp, teacher_logits_temp, beta_prob) * (temperature ** 2)\n",
    "            ce_loss = F.cross_entropy(student_logits, labels)\n",
    "            loss = alpha * distillation_loss + (1 - alpha) * ce_loss\n",
    "\n",
    "            # Modified backward propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Accumulate importance scores with parameter-gradient product\n",
    "            accumulated_batches += 1\n",
    "            for name, param in student.named_parameters():\n",
    "                if name in importance_scores and param.grad is not None:\n",
    "                    # Key modification: Use parameter-gradient product magnitude\n",
    "                    grad_product = (param.data * param.grad).abs_()\n",
    "                    \n",
    "                    # Exponential moving average with bias correction\n",
    "                    if accumulated_batches == 1:\n",
    "                        importance_scores[name] = grad_product\n",
    "                    else:\n",
    "                        importance_scores[name] = momentum * importance_scores[name] + (1 - momentum) * grad_product\n",
    "\n",
    "    # Apply bias correction for EMA\n",
    "    for name in importance_scores:\n",
    "        importance_scores[name] /= (1 - momentum**accumulated_batches)\n",
    "\n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_based_global_prune(model, importance_scores, prune_ratio=0.95):\n",
    "    all_scores = torch.cat([score.flatten() for score in importance_scores.values()])\n",
    "    threshold = torch.topk(all_scores, k=int(prune_ratio * all_scores.numel()), largest=False)[0][-1]\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in importance_scores:\n",
    "            mask = (importance_scores[name] > threshold).float()\n",
    "            param.data.mul_(mask)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def retrain_with_sparsity(student, train_loader, val_loader, epochs=5, save_path=\"retrained_student_model.pt\", patience=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # 1. Store masks AND zero momentum buffers for pruned weights\n",
    "    masks = {}\n",
    "    for name, param in student.named_parameters():\n",
    "        if 'weight' in name and param.dim() == 4:  # Consider only conv layers\n",
    "            mask = (param != 0).float().to(device)\n",
    "            masks[name] = mask\n",
    "            # Zero momentum buffers for pruned weights\n",
    "            if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n",
    "                optimizer.state[param]['momentum_buffer'] *= mask\n",
    "\n",
    "    student = student.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    best_model = None\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "\n",
    "    # 2. Add gradient clipping to prevent NaN\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        total_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = student(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Apply masks to gradients\n",
    "            for name, param in student.named_parameters():\n",
    "                if name in masks:\n",
    "                    param.grad.data *= masks[name]\n",
    "\n",
    "            # Gradient clipping before optimizer step\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Reapply masks and update momentum buffers\n",
    "            for name, param in student.named_parameters():\n",
    "                if name in masks:\n",
    "                    param.data *= masks[name]\n",
    "                    if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n",
    "                        optimizer.state[param]['momentum_buffer'] *= masks[name]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        student.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        # Track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = student.state_dict()\n",
    "            torch.save(best_model, save_path)\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            print(f\"New best model saved with Val Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
    "                break  # Stop training\n",
    "\n",
    "        # Print results\n",
    "        sparsity = calculate_sparsity(student)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f} | Validation Acc: {val_acc:.2f}% | Sparsity: {sparsity*100:.2f}%\\n\")\n",
    "\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}% | Best Model Saved at: {save_path}\")\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# KD training with CA-KLD loss and mask-based momentum handling\n",
    "def retrain_with_KD(teacher, student, train_loader, val_loader, epochs=50,\n",
    "                    temperature=5.0, alpha=0.5, beta_prob=0.5, patience=5,\n",
    "                    save_path=\"student_before_pruning.pth\"):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # 1. Store masks and zero momentum buffers\n",
    "    masks = {}\n",
    "    for name, param in student.named_parameters():\n",
    "        if 'weight' in name and param.dim() == 4:\n",
    "            mask = (param != 0).float().to(device)\n",
    "            masks[name] = mask\n",
    "            if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n",
    "                optimizer.state[param]['momentum_buffer'] *= mask\n",
    "\n",
    "    teacher = teacher.to(device).eval()\n",
    "    student = student.to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Apply temperature\n",
    "            teacher_logits_temp = teacher_logits / temperature\n",
    "            student_logits_temp = student_logits / temperature\n",
    "\n",
    "            # Logits normalization\n",
    "            teacher_logits_temp = normalize(teacher_logits_temp)\n",
    "            student_logits_temp = normalize(student_logits_temp)\n",
    "\n",
    "\n",
    "            # CA-KLD loss\n",
    "            kd_loss = cakld_loss(student_logits_temp, teacher_logits_temp, beta_prob) * (temperature ** 2)\n",
    "            ce_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Reapply masks and update momentum\n",
    "            for name, param in student.named_parameters():\n",
    "                if name in masks:\n",
    "                    param.data *= masks[name]\n",
    "                    if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n",
    "                        optimizer.state[param]['momentum_buffer'] *= masks[name]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = student_logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        student.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        sparsity = calculate_sparsity(student) * 100.0  # Assuming this function is defined elsewhere\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Sparsity: {sparsity:.2f}%\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = student.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "    # Restore and save best model\n",
    "    student.load_state_dict(best_model_state)\n",
    "    torch.save(student.state_dict(), save_path)\n",
    "    print(f\"Student model saved before pruning at: {save_path}\")\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total Training Time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training function with KD + CA-KLD and logits normalization\n",
    "def train_kd_pruning(teacher, student, train_loader, val_loader, epochs=50, temperature=5.0, alpha=0.5,\n",
    "                     beta_prob=0.5, patience=5, save_path=\"student_before_pruning.pth\"):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    teacher.eval()  # Freeze teacher\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        total_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Temperature scaling\n",
    "            teacher_logits_temp = teacher_logits / temperature\n",
    "            student_logits_temp = student_logits / temperature\n",
    "\n",
    "            # Logits normalization\n",
    "            teacher_logits_temp = normalize(teacher_logits_temp)\n",
    "            student_logits_temp = normalize(student_logits_temp)\n",
    "\n",
    "            # CA-KLD loss (normalized logits)\n",
    "            distillation_loss = cakld_loss(student_logits_temp, teacher_logits_temp, beta_prob) * (temperature ** 2)\n",
    "\n",
    "            # Cross-entropy loss\n",
    "            ground_truth_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = alpha * distillation_loss + (1 - alpha) * ground_truth_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = student_logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation accuracy\n",
    "        val_acc = evaluate(student, val_loader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = student.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "    # Load best model state and save\n",
    "    student.load_state_dict(best_model_state)\n",
    "    torch.save(student.state_dict(), save_path)\n",
    "    print(f\"Student model saved before pruning at: {save_path}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total Training Time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 8.9872 | Train Acc: 55.35% | Val Acc: 61.43%\n",
      "Epoch 2/50 | Train Loss: 4.2724 | Train Acc: 74.34% | Val Acc: 69.79%\n",
      "Epoch 3/50 | Train Loss: 2.6246 | Train Acc: 82.81% | Val Acc: 75.91%\n",
      "Epoch 4/50 | Train Loss: 1.6964 | Train Acc: 88.44% | Val Acc: 77.66%\n",
      "Epoch 5/50 | Train Loss: 1.1719 | Train Acc: 91.85% | Val Acc: 81.27%\n",
      "Epoch 6/50 | Train Loss: 0.8926 | Train Acc: 94.14% | Val Acc: 82.41%\n",
      "Epoch 7/50 | Train Loss: 0.7277 | Train Acc: 95.22% | Val Acc: 82.70%\n",
      "Epoch 8/50 | Train Loss: 0.6366 | Train Acc: 95.91% | Val Acc: 82.97%\n",
      "Epoch 9/50 | Train Loss: 0.5762 | Train Acc: 96.11% | Val Acc: 83.16%\n",
      "Epoch 10/50 | Train Loss: 0.5320 | Train Acc: 96.41% | Val Acc: 83.47%\n",
      "Epoch 11/50 | Train Loss: 0.4964 | Train Acc: 96.52% | Val Acc: 83.72%\n",
      "Epoch 12/50 | Train Loss: 0.4735 | Train Acc: 96.66% | Val Acc: 83.62%\n",
      "Epoch 13/50 | Train Loss: 0.4522 | Train Acc: 96.73% | Val Acc: 83.54%\n",
      "Epoch 14/50 | Train Loss: 0.4309 | Train Acc: 96.88% | Val Acc: 83.80%\n",
      "Epoch 15/50 | Train Loss: 0.4118 | Train Acc: 96.90% | Val Acc: 83.74%\n",
      "Epoch 16/50 | Train Loss: 0.4022 | Train Acc: 96.86% | Val Acc: 84.13%\n",
      "Epoch 17/50 | Train Loss: 0.3907 | Train Acc: 96.96% | Val Acc: 84.10%\n",
      "Epoch 18/50 | Train Loss: 0.3824 | Train Acc: 97.01% | Val Acc: 83.98%\n",
      "Epoch 19/50 | Train Loss: 0.3657 | Train Acc: 97.02% | Val Acc: 83.98%\n",
      "Epoch 20/50 | Train Loss: 0.3575 | Train Acc: 97.08% | Val Acc: 84.14%\n",
      "Epoch 21/50 | Train Loss: 0.3492 | Train Acc: 97.10% | Val Acc: 84.21%\n",
      "Epoch 22/50 | Train Loss: 0.3393 | Train Acc: 97.08% | Val Acc: 84.14%\n",
      "Epoch 23/50 | Train Loss: 0.3318 | Train Acc: 97.19% | Val Acc: 84.37%\n",
      "Epoch 24/50 | Train Loss: 0.3250 | Train Acc: 97.17% | Val Acc: 84.44%\n",
      "Epoch 25/50 | Train Loss: 0.3195 | Train Acc: 97.23% | Val Acc: 84.72%\n",
      "Epoch 26/50 | Train Loss: 0.3076 | Train Acc: 97.30% | Val Acc: 84.35%\n",
      "Epoch 27/50 | Train Loss: 0.3062 | Train Acc: 97.18% | Val Acc: 84.33%\n",
      "Epoch 28/50 | Train Loss: 0.3030 | Train Acc: 97.25% | Val Acc: 84.45%\n",
      "Epoch 29/50 | Train Loss: 0.2972 | Train Acc: 97.27% | Val Acc: 84.28%\n",
      "Epoch 30/50 | Train Loss: 0.2920 | Train Acc: 97.27% | Val Acc: 84.37%\n",
      "Early stopping triggered at epoch 30. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: student_before_pruning.pth\n",
      "Total Training Time: 33m 46s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "student = train_kd_pruning(\n",
    "    teacher, student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.5,beta_prob=0.5, patience=5,save_path=\"student_before_pruning.pth\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Before Pruning: 0.00%\n",
      "Teacher Model Test Accuracy: 80.89%\n",
      "Student Model Test Accuracy Before Pruning: 82.77%\n"
     ]
    }
   ],
   "source": [
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(student)\n",
    "print(f\"Sparsity Before Pruning: {sparsity * 100:.2f}%\")\n",
    "\n",
    "teacher_accuracy = evaluate(teacher, test_loader, device)\n",
    "student_accuracy = evaluate(student, test_loader, device)\n",
    "print(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\n",
    "print(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 93% Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 7s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.94)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Model Test Accuracy After Pruning: 1.00%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Student Model Test Accuracy After Pruning: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with Val Accuracy: 70.24%\n",
      "Epoch 1/200 | Train Loss: 1.8297 | Train Acc: 58.28%\n",
      "Validation Loss: 1.0416 | Validation Acc: 70.24% | Sparsity: 93.74%\n",
      "\n",
      "New best model saved with Val Accuracy: 74.30%\n",
      "Epoch 2/200 | Train Loss: 0.6932 | Train Acc: 79.52%\n",
      "Validation Loss: 0.8664 | Validation Acc: 74.30% | Sparsity: 93.74%\n",
      "\n",
      "New best model saved with Val Accuracy: 75.65%\n",
      "Epoch 3/200 | Train Loss: 0.5022 | Train Acc: 85.05%\n",
      "Validation Loss: 0.8188 | Validation Acc: 75.65% | Sparsity: 93.74%\n",
      "\n",
      "New best model saved with Val Accuracy: 76.22%\n",
      "Epoch 4/200 | Train Loss: 0.3816 | Train Acc: 88.84%\n",
      "Validation Loss: 0.7976 | Validation Acc: 76.22% | Sparsity: 93.74%\n",
      "\n",
      "New best model saved with Val Accuracy: 76.60%\n",
      "Epoch 5/200 | Train Loss: 0.2913 | Train Acc: 91.92%\n",
      "Validation Loss: 0.8050 | Validation Acc: 76.60% | Sparsity: 93.74%\n",
      "\n",
      "New best model saved with Val Accuracy: 76.66%\n",
      "Epoch 6/200 | Train Loss: 0.2217 | Train Acc: 94.08%\n",
      "Validation Loss: 0.8079 | Validation Acc: 76.66% | Sparsity: 93.74%\n",
      "\n",
      "Epoch 7/200 | Train Loss: 0.1673 | Train Acc: 95.89%\n",
      "Validation Loss: 0.8415 | Validation Acc: 76.61% | Sparsity: 93.74%\n",
      "\n",
      "Epoch 8/200 | Train Loss: 0.1226 | Train Acc: 97.37%\n",
      "Validation Loss: 0.8664 | Validation Acc: 76.65% | Sparsity: 93.74%\n",
      "\n",
      "Epoch 9/200 | Train Loss: 0.0907 | Train Acc: 98.27%\n",
      "Validation Loss: 0.8914 | Validation Acc: 76.51% | Sparsity: 93.74%\n",
      "\n",
      "Epoch 10/200 | Train Loss: 0.0672 | Train Acc: 98.95%\n",
      "Validation Loss: 0.9225 | Validation Acc: 76.26% | Sparsity: 93.74%\n",
      "\n",
      "Early stopping triggered at epoch 11. No improvement for 5 epochs.\n",
      "Best Validation Accuracy: 76.66% | Best Model Saved at: retrained_student_model.pt\n",
      "Retraining completed in 8.71 minutes (522.77 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "retrained_student = retrain_with_sparsity(\n",
    "    pruned_student, train_loader, val_loader,\n",
    "    epochs=200,  save_path='retrained_student_model.pt',patience=5\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 75.35%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(retrained_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 8s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.94)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 7.6947 | Train Acc: 68.52% | Val Loss: 1.2570 | Val Acc: 69.73% | Sparsity: 93.74%\n",
      "Epoch 2/50 | Train Loss: 3.9268 | Train Acc: 80.91% | Val Loss: 1.1996 | Val Acc: 72.09% | Sparsity: 93.74%\n",
      "Epoch 3/50 | Train Loss: 2.7308 | Train Acc: 86.07% | Val Loss: 0.9457 | Val Acc: 75.52% | Sparsity: 93.74%\n",
      "Epoch 4/50 | Train Loss: 2.0303 | Train Acc: 89.30% | Val Loss: 0.9088 | Val Acc: 77.41% | Sparsity: 93.74%\n",
      "Epoch 5/50 | Train Loss: 1.5740 | Train Acc: 91.55% | Val Loss: 0.8357 | Val Acc: 78.34% | Sparsity: 93.74%\n",
      "Epoch 6/50 | Train Loss: 1.3034 | Train Acc: 93.00% | Val Loss: 0.7681 | Val Acc: 79.74% | Sparsity: 93.74%\n",
      "Epoch 7/50 | Train Loss: 1.1211 | Train Acc: 93.82% | Val Loss: 0.7619 | Val Acc: 79.94% | Sparsity: 93.74%\n",
      "Epoch 8/50 | Train Loss: 1.0169 | Train Acc: 94.32% | Val Loss: 0.7269 | Val Acc: 80.64% | Sparsity: 93.74%\n",
      "Epoch 9/50 | Train Loss: 0.9292 | Train Acc: 94.70% | Val Loss: 0.7386 | Val Acc: 80.26% | Sparsity: 93.74%\n",
      "Epoch 10/50 | Train Loss: 0.8692 | Train Acc: 94.86% | Val Loss: 0.7180 | Val Acc: 80.33% | Sparsity: 93.74%\n",
      "Epoch 11/50 | Train Loss: 0.8143 | Train Acc: 95.19% | Val Loss: 0.7024 | Val Acc: 81.15% | Sparsity: 93.74%\n",
      "Epoch 12/50 | Train Loss: 0.7840 | Train Acc: 95.20% | Val Loss: 0.7088 | Val Acc: 80.65% | Sparsity: 93.74%\n",
      "Epoch 13/50 | Train Loss: 0.7482 | Train Acc: 95.39% | Val Loss: 0.7357 | Val Acc: 80.46% | Sparsity: 93.74%\n",
      "Epoch 14/50 | Train Loss: 0.7201 | Train Acc: 95.39% | Val Loss: 0.7113 | Val Acc: 81.18% | Sparsity: 93.74%\n",
      "Epoch 15/50 | Train Loss: 0.7007 | Train Acc: 95.55% | Val Loss: 0.6919 | Val Acc: 81.26% | Sparsity: 93.74%\n",
      "Epoch 16/50 | Train Loss: 0.6780 | Train Acc: 95.56% | Val Loss: 0.6970 | Val Acc: 81.23% | Sparsity: 93.74%\n",
      "Epoch 17/50 | Train Loss: 0.6578 | Train Acc: 95.52% | Val Loss: 0.6810 | Val Acc: 81.30% | Sparsity: 93.74%\n",
      "Epoch 18/50 | Train Loss: 0.6426 | Train Acc: 95.64% | Val Loss: 0.6878 | Val Acc: 80.80% | Sparsity: 93.74%\n",
      "Epoch 19/50 | Train Loss: 0.6239 | Train Acc: 95.64% | Val Loss: 0.6792 | Val Acc: 81.20% | Sparsity: 93.74%\n",
      "Epoch 20/50 | Train Loss: 0.6124 | Train Acc: 95.76% | Val Loss: 0.6888 | Val Acc: 80.96% | Sparsity: 93.74%\n",
      "Epoch 21/50 | Train Loss: 0.6000 | Train Acc: 95.71% | Val Loss: 0.6805 | Val Acc: 81.20% | Sparsity: 93.74%\n",
      "Epoch 22/50 | Train Loss: 0.5917 | Train Acc: 95.72% | Val Loss: 0.6790 | Val Acc: 81.28% | Sparsity: 93.74%\n",
      "Early stopping triggered at epoch 22. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 24m 53s\n",
      "Retraining completed in 24.89 minutes (1493.37 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 79.78%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 5s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.94)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 2.3252 | Train Acc: 75.41% | Val Loss: 0.8594 | Val Acc: 76.06% | Sparsity: 93.74%\n",
      "Epoch 2/50 | Train Loss: 1.0102 | Train Acc: 86.93% | Val Loss: 0.7885 | Val Acc: 78.24% | Sparsity: 93.74%\n",
      "Epoch 3/50 | Train Loss: 0.7246 | Train Acc: 90.75% | Val Loss: 0.7452 | Val Acc: 78.97% | Sparsity: 93.74%\n",
      "Epoch 4/50 | Train Loss: 0.5727 | Train Acc: 92.87% | Val Loss: 0.7167 | Val Acc: 79.53% | Sparsity: 93.74%\n",
      "Epoch 5/50 | Train Loss: 0.4865 | Train Acc: 94.19% | Val Loss: 0.7155 | Val Acc: 80.35% | Sparsity: 93.74%\n",
      "Epoch 6/50 | Train Loss: 0.4306 | Train Acc: 94.96% | Val Loss: 0.7092 | Val Acc: 80.02% | Sparsity: 93.74%\n",
      "Epoch 7/50 | Train Loss: 0.3925 | Train Acc: 95.42% | Val Loss: 0.6973 | Val Acc: 80.73% | Sparsity: 93.74%\n",
      "Epoch 8/50 | Train Loss: 0.3674 | Train Acc: 95.63% | Val Loss: 0.7033 | Val Acc: 80.72% | Sparsity: 93.74%\n",
      "Epoch 9/50 | Train Loss: 0.3425 | Train Acc: 96.08% | Val Loss: 0.7000 | Val Acc: 80.46% | Sparsity: 93.74%\n",
      "Epoch 10/50 | Train Loss: 0.3257 | Train Acc: 96.12% | Val Loss: 0.6928 | Val Acc: 80.53% | Sparsity: 93.74%\n",
      "Epoch 11/50 | Train Loss: 0.3134 | Train Acc: 96.35% | Val Loss: 0.6858 | Val Acc: 80.81% | Sparsity: 93.74%\n",
      "Epoch 12/50 | Train Loss: 0.3011 | Train Acc: 96.36% | Val Loss: 0.6856 | Val Acc: 80.82% | Sparsity: 93.74%\n",
      "Epoch 13/50 | Train Loss: 0.2934 | Train Acc: 96.54% | Val Loss: 0.6847 | Val Acc: 80.98% | Sparsity: 93.74%\n",
      "Epoch 14/50 | Train Loss: 0.2826 | Train Acc: 96.59% | Val Loss: 0.6791 | Val Acc: 80.91% | Sparsity: 93.74%\n",
      "Epoch 15/50 | Train Loss: 0.2743 | Train Acc: 96.70% | Val Loss: 0.6735 | Val Acc: 81.10% | Sparsity: 93.74%\n",
      "Epoch 16/50 | Train Loss: 0.2663 | Train Acc: 96.74% | Val Loss: 0.6842 | Val Acc: 80.94% | Sparsity: 93.74%\n",
      "Epoch 17/50 | Train Loss: 0.2628 | Train Acc: 96.76% | Val Loss: 0.6845 | Val Acc: 81.04% | Sparsity: 93.74%\n",
      "Epoch 18/50 | Train Loss: 0.2570 | Train Acc: 96.75% | Val Loss: 0.6796 | Val Acc: 81.21% | Sparsity: 93.74%\n",
      "Epoch 19/50 | Train Loss: 0.2491 | Train Acc: 96.90% | Val Loss: 0.6815 | Val Acc: 80.97% | Sparsity: 93.74%\n",
      "Epoch 20/50 | Train Loss: 0.2466 | Train Acc: 96.93% | Val Loss: 0.6864 | Val Acc: 81.19% | Sparsity: 93.74%\n",
      "Epoch 21/50 | Train Loss: 0.2450 | Train Acc: 96.89% | Val Loss: 0.6729 | Val Acc: 81.34% | Sparsity: 93.74%\n",
      "Epoch 22/50 | Train Loss: 0.2426 | Train Acc: 96.98% | Val Loss: 0.6855 | Val Acc: 81.20% | Sparsity: 93.74%\n",
      "Epoch 23/50 | Train Loss: 0.2348 | Train Acc: 96.98% | Val Loss: 0.6828 | Val Acc: 81.19% | Sparsity: 93.74%\n",
      "Epoch 24/50 | Train Loss: 0.2329 | Train Acc: 97.07% | Val Loss: 0.6837 | Val Acc: 81.28% | Sparsity: 93.74%\n",
      "Epoch 25/50 | Train Loss: 0.2278 | Train Acc: 97.00% | Val Loss: 0.6819 | Val Acc: 81.12% | Sparsity: 93.74%\n",
      "Epoch 26/50 | Train Loss: 0.2254 | Train Acc: 97.00% | Val Loss: 0.6794 | Val Acc: 81.55% | Sparsity: 93.74%\n",
      "Epoch 27/50 | Train Loss: 0.2205 | Train Acc: 97.18% | Val Loss: 0.6816 | Val Acc: 81.21% | Sparsity: 93.74%\n",
      "Epoch 28/50 | Train Loss: 0.2198 | Train Acc: 97.12% | Val Loss: 0.6798 | Val Acc: 81.28% | Sparsity: 93.74%\n",
      "Epoch 29/50 | Train Loss: 0.2175 | Train Acc: 97.14% | Val Loss: 0.6788 | Val Acc: 81.58% | Sparsity: 93.74%\n",
      "Epoch 30/50 | Train Loss: 0.2157 | Train Acc: 97.03% | Val Loss: 0.6735 | Val Acc: 81.17% | Sparsity: 93.74%\n",
      "Epoch 31/50 | Train Loss: 0.2132 | Train Acc: 97.10% | Val Loss: 0.6751 | Val Acc: 81.35% | Sparsity: 93.74%\n",
      "Epoch 32/50 | Train Loss: 0.2093 | Train Acc: 97.22% | Val Loss: 0.6718 | Val Acc: 81.39% | Sparsity: 93.74%\n",
      "Epoch 33/50 | Train Loss: 0.2092 | Train Acc: 97.20% | Val Loss: 0.6751 | Val Acc: 81.45% | Sparsity: 93.74%\n",
      "Epoch 34/50 | Train Loss: 0.2062 | Train Acc: 97.20% | Val Loss: 0.6890 | Val Acc: 81.36% | Sparsity: 93.74%\n",
      "Early stopping triggered at epoch 34. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 38m 17s\n",
      "Retraining completed in 38.28 minutes (2296.80 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 79.54%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 98% of Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 7s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.985)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with Val Accuracy: 9.81%\n",
      "Epoch 1/200 | Train Loss: 4.3498 | Train Acc: 5.29%\n",
      "Validation Loss: 3.9287 | Validation Acc: 9.81% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 21.33%\n",
      "Epoch 2/200 | Train Loss: 3.5276 | Train Acc: 15.96%\n",
      "Validation Loss: 3.1741 | Validation Acc: 21.33% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 42.70%\n",
      "Epoch 3/200 | Train Loss: 2.6189 | Train Acc: 32.60%\n",
      "Validation Loss: 2.1166 | Validation Acc: 42.70% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 53.83%\n",
      "Epoch 4/200 | Train Loss: 1.8147 | Train Acc: 50.09%\n",
      "Validation Loss: 1.6431 | Validation Acc: 53.83% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 58.58%\n",
      "Epoch 5/200 | Train Loss: 1.4557 | Train Acc: 58.74%\n",
      "Validation Loss: 1.4420 | Validation Acc: 58.58% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 61.69%\n",
      "Epoch 6/200 | Train Loss: 1.2473 | Train Acc: 64.17%\n",
      "Validation Loss: 1.3328 | Validation Acc: 61.69% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 63.76%\n",
      "Epoch 7/200 | Train Loss: 1.1031 | Train Acc: 67.92%\n",
      "Validation Loss: 1.2443 | Validation Acc: 63.76% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 64.65%\n",
      "Epoch 8/200 | Train Loss: 0.9920 | Train Acc: 71.32%\n",
      "Validation Loss: 1.2055 | Validation Acc: 64.65% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 65.40%\n",
      "Epoch 9/200 | Train Loss: 0.9040 | Train Acc: 73.62%\n",
      "Validation Loss: 1.1774 | Validation Acc: 65.40% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 65.85%\n",
      "Epoch 10/200 | Train Loss: 0.8214 | Train Acc: 75.76%\n",
      "Validation Loss: 1.1539 | Validation Acc: 65.85% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 66.65%\n",
      "Epoch 11/200 | Train Loss: 0.7530 | Train Acc: 77.89%\n",
      "Validation Loss: 1.1428 | Validation Acc: 66.65% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 67.32%\n",
      "Epoch 12/200 | Train Loss: 0.6900 | Train Acc: 79.78%\n",
      "Validation Loss: 1.1293 | Validation Acc: 67.32% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 67.33%\n",
      "Epoch 13/200 | Train Loss: 0.6321 | Train Acc: 81.66%\n",
      "Validation Loss: 1.1239 | Validation Acc: 67.33% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 67.34%\n",
      "Epoch 14/200 | Train Loss: 0.5759 | Train Acc: 83.34%\n",
      "Validation Loss: 1.1392 | Validation Acc: 67.34% | Sparsity: 98.22%\n",
      "\n",
      "New best model saved with Val Accuracy: 67.45%\n",
      "Epoch 15/200 | Train Loss: 0.5275 | Train Acc: 84.82%\n",
      "Validation Loss: 1.1416 | Validation Acc: 67.45% | Sparsity: 98.22%\n",
      "\n",
      "Epoch 16/200 | Train Loss: 0.4856 | Train Acc: 86.19%\n",
      "Validation Loss: 1.1583 | Validation Acc: 66.97% | Sparsity: 98.22%\n",
      "\n",
      "Epoch 17/200 | Train Loss: 0.4412 | Train Acc: 87.79%\n",
      "Validation Loss: 1.1651 | Validation Acc: 67.29% | Sparsity: 98.22%\n",
      "\n",
      "Epoch 18/200 | Train Loss: 0.4006 | Train Acc: 89.08%\n",
      "Validation Loss: 1.1861 | Validation Acc: 67.18% | Sparsity: 98.22%\n",
      "\n",
      "Epoch 19/200 | Train Loss: 0.3656 | Train Acc: 90.18%\n",
      "Validation Loss: 1.2136 | Validation Acc: 66.94% | Sparsity: 98.22%\n",
      "\n",
      "Epoch 20/200 | Train Loss: 0.3306 | Train Acc: 91.25%\n",
      "Validation Loss: 1.2404 | Validation Acc: 66.56% | Sparsity: 98.22%\n",
      "\n",
      "Epoch 21/200 | Train Loss: 0.3002 | Train Acc: 92.29%\n",
      "Validation Loss: 1.2465 | Validation Acc: 66.62% | Sparsity: 98.22%\n",
      "\n",
      "Early stopping triggered at epoch 22. No improvement for 7 epochs.\n",
      "Best Validation Accuracy: 67.45% | Best Model Saved at: retrained_student_model.pt\n",
      "Retraining completed in 17.40 minutes (1043.72 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "retrained_student = retrain_with_sparsity(\n",
    "    pruned_student, train_loader, val_loader,\n",
    "    epochs=200,  save_path='retrained_student_model.pt',patience=7\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 66.14%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(retrained_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pruned_student_inference_time = measure_inference_time(retrained_student, test_loader,)\n",
    "# print(f\"Pruned Student Model Inference Time: {pruned_student_inference_time * 1000:.2f} ms per batch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrained with KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Model Test Accuracy: 82.77%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(student, test_loader, device)\n",
    "print(f\"Student Model Test Accuracy: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 2s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.985)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 17.0927 | Train Acc: 41.51% | Val Loss: 1.8627 | Val Acc: 53.75% | Sparsity: 98.22%\n",
      "Epoch 2/50 | Train Loss: 9.1428 | Train Acc: 62.62% | Val Loss: 1.5847 | Val Acc: 59.67% | Sparsity: 98.22%\n",
      "Epoch 3/50 | Train Loss: 7.2775 | Train Acc: 69.38% | Val Loss: 1.3506 | Val Acc: 64.69% | Sparsity: 98.22%\n",
      "Epoch 4/50 | Train Loss: 6.1327 | Train Acc: 73.05% | Val Loss: 1.2992 | Val Acc: 66.63% | Sparsity: 98.22%\n",
      "Epoch 5/50 | Train Loss: 5.3373 | Train Acc: 75.99% | Val Loss: 1.2075 | Val Acc: 67.96% | Sparsity: 98.22%\n",
      "Epoch 6/50 | Train Loss: 4.7834 | Train Acc: 78.03% | Val Loss: 1.2672 | Val Acc: 67.49% | Sparsity: 98.22%\n",
      "Epoch 7/50 | Train Loss: 4.3619 | Train Acc: 79.83% | Val Loss: 1.1970 | Val Acc: 68.54% | Sparsity: 98.22%\n",
      "Epoch 8/50 | Train Loss: 3.9138 | Train Acc: 81.86% | Val Loss: 1.1688 | Val Acc: 69.82% | Sparsity: 98.22%\n",
      "Epoch 9/50 | Train Loss: 3.5504 | Train Acc: 83.03% | Val Loss: 1.1340 | Val Acc: 70.51% | Sparsity: 98.22%\n",
      "Epoch 10/50 | Train Loss: 3.2293 | Train Acc: 84.59% | Val Loss: 1.0920 | Val Acc: 70.70% | Sparsity: 98.22%\n",
      "Epoch 11/50 | Train Loss: 3.0070 | Train Acc: 85.72% | Val Loss: 1.0562 | Val Acc: 71.36% | Sparsity: 98.22%\n",
      "Epoch 12/50 | Train Loss: 2.7772 | Train Acc: 86.81% | Val Loss: 1.0790 | Val Acc: 71.27% | Sparsity: 98.22%\n",
      "Epoch 13/50 | Train Loss: 2.6080 | Train Acc: 87.51% | Val Loss: 1.0644 | Val Acc: 72.01% | Sparsity: 98.22%\n",
      "Epoch 14/50 | Train Loss: 2.4476 | Train Acc: 88.52% | Val Loss: 1.0537 | Val Acc: 72.09% | Sparsity: 98.22%\n",
      "Epoch 15/50 | Train Loss: 2.2826 | Train Acc: 89.22% | Val Loss: 1.0516 | Val Acc: 72.50% | Sparsity: 98.22%\n",
      "Epoch 16/50 | Train Loss: 2.1328 | Train Acc: 89.76% | Val Loss: 1.0572 | Val Acc: 71.90% | Sparsity: 98.22%\n",
      "Epoch 17/50 | Train Loss: 2.0519 | Train Acc: 90.38% | Val Loss: 1.0240 | Val Acc: 72.68% | Sparsity: 98.22%\n",
      "Epoch 18/50 | Train Loss: 1.9117 | Train Acc: 90.97% | Val Loss: 1.0378 | Val Acc: 73.05% | Sparsity: 98.22%\n",
      "Epoch 19/50 | Train Loss: 1.8569 | Train Acc: 91.21% | Val Loss: 1.0637 | Val Acc: 71.82% | Sparsity: 98.22%\n",
      "Epoch 20/50 | Train Loss: 1.7950 | Train Acc: 91.42% | Val Loss: 1.0525 | Val Acc: 72.15% | Sparsity: 98.22%\n",
      "Epoch 21/50 | Train Loss: 1.6967 | Train Acc: 91.97% | Val Loss: 1.0135 | Val Acc: 73.10% | Sparsity: 98.22%\n",
      "Epoch 22/50 | Train Loss: 1.6268 | Train Acc: 92.33% | Val Loss: 1.0375 | Val Acc: 72.82% | Sparsity: 98.22%\n",
      "Epoch 23/50 | Train Loss: 1.5548 | Train Acc: 92.64% | Val Loss: 1.0228 | Val Acc: 72.44% | Sparsity: 98.22%\n",
      "Epoch 24/50 | Train Loss: 1.5419 | Train Acc: 92.76% | Val Loss: 1.0409 | Val Acc: 72.44% | Sparsity: 98.22%\n",
      "Epoch 25/50 | Train Loss: 1.4849 | Train Acc: 93.05% | Val Loss: 1.0547 | Val Acc: 72.41% | Sparsity: 98.22%\n",
      "Epoch 26/50 | Train Loss: 1.4506 | Train Acc: 93.11% | Val Loss: 1.0549 | Val Acc: 72.82% | Sparsity: 98.22%\n",
      "Epoch 27/50 | Train Loss: 1.4062 | Train Acc: 93.23% | Val Loss: 1.0091 | Val Acc: 72.83% | Sparsity: 98.22%\n",
      "Epoch 28/50 | Train Loss: 1.3524 | Train Acc: 93.50% | Val Loss: 1.0080 | Val Acc: 73.23% | Sparsity: 98.22%\n",
      "Epoch 29/50 | Train Loss: 1.3202 | Train Acc: 93.61% | Val Loss: 1.0310 | Val Acc: 72.66% | Sparsity: 98.22%\n",
      "Epoch 30/50 | Train Loss: 1.3066 | Train Acc: 93.56% | Val Loss: 1.0227 | Val Acc: 73.45% | Sparsity: 98.22%\n",
      "Epoch 31/50 | Train Loss: 1.2622 | Train Acc: 93.85% | Val Loss: 1.0110 | Val Acc: 72.99% | Sparsity: 98.22%\n",
      "Epoch 32/50 | Train Loss: 1.2431 | Train Acc: 93.92% | Val Loss: 0.9885 | Val Acc: 73.86% | Sparsity: 98.22%\n",
      "Epoch 33/50 | Train Loss: 1.2299 | Train Acc: 93.81% | Val Loss: 0.9947 | Val Acc: 73.36% | Sparsity: 98.22%\n",
      "Epoch 34/50 | Train Loss: 1.1972 | Train Acc: 93.98% | Val Loss: 1.0139 | Val Acc: 73.12% | Sparsity: 98.22%\n",
      "Epoch 35/50 | Train Loss: 1.1882 | Train Acc: 94.12% | Val Loss: 1.0257 | Val Acc: 73.14% | Sparsity: 98.22%\n",
      "Epoch 36/50 | Train Loss: 1.1663 | Train Acc: 94.19% | Val Loss: 1.0164 | Val Acc: 73.38% | Sparsity: 98.22%\n",
      "Epoch 37/50 | Train Loss: 1.1428 | Train Acc: 94.22% | Val Loss: 1.0253 | Val Acc: 73.15% | Sparsity: 98.22%\n",
      "Epoch 38/50 | Train Loss: 1.1273 | Train Acc: 94.29% | Val Loss: 1.0164 | Val Acc: 73.42% | Sparsity: 98.22%\n",
      "Epoch 39/50 | Train Loss: 1.1221 | Train Acc: 94.31% | Val Loss: 1.0035 | Val Acc: 73.46% | Sparsity: 98.22%\n",
      "Early stopping triggered at epoch 39. No improvement for 7 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 44m 16s\n",
      "Retraining completed in 44.27 minutes (2656.33 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=7,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(Retrain with KD): 72.79%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(Retrain with KD): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 6s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.985)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 6.8956 | Train Acc: 40.48% | Val Loss: 1.5925 | Val Acc: 57.35% | Sparsity: 98.22%\n",
      "Epoch 2/50 | Train Loss: 3.3404 | Train Acc: 64.96% | Val Loss: 1.2948 | Val Acc: 64.17% | Sparsity: 98.22%\n",
      "Epoch 3/50 | Train Loss: 2.5846 | Train Acc: 71.34% | Val Loss: 1.1690 | Val Acc: 67.97% | Sparsity: 98.22%\n",
      "Epoch 4/50 | Train Loss: 2.1853 | Train Acc: 75.09% | Val Loss: 1.1041 | Val Acc: 69.52% | Sparsity: 98.22%\n",
      "Epoch 5/50 | Train Loss: 1.9072 | Train Acc: 78.19% | Val Loss: 1.1432 | Val Acc: 68.39% | Sparsity: 98.22%\n",
      "Epoch 6/50 | Train Loss: 1.7025 | Train Acc: 80.27% | Val Loss: 1.0923 | Val Acc: 69.96% | Sparsity: 98.22%\n",
      "Epoch 7/50 | Train Loss: 1.5268 | Train Acc: 82.30% | Val Loss: 1.0550 | Val Acc: 70.74% | Sparsity: 98.22%\n",
      "Epoch 8/50 | Train Loss: 1.4092 | Train Acc: 83.67% | Val Loss: 1.0414 | Val Acc: 71.15% | Sparsity: 98.22%\n",
      "Epoch 9/50 | Train Loss: 1.2815 | Train Acc: 85.17% | Val Loss: 1.0263 | Val Acc: 71.36% | Sparsity: 98.22%\n",
      "Epoch 10/50 | Train Loss: 1.1792 | Train Acc: 86.53% | Val Loss: 0.9961 | Val Acc: 72.23% | Sparsity: 98.22%\n",
      "Epoch 11/50 | Train Loss: 1.1014 | Train Acc: 87.55% | Val Loss: 1.0316 | Val Acc: 71.53% | Sparsity: 98.22%\n",
      "Epoch 12/50 | Train Loss: 1.0100 | Train Acc: 88.76% | Val Loss: 0.9998 | Val Acc: 72.36% | Sparsity: 98.22%\n",
      "Epoch 13/50 | Train Loss: 0.9469 | Train Acc: 89.45% | Val Loss: 0.9906 | Val Acc: 72.61% | Sparsity: 98.22%\n",
      "Epoch 14/50 | Train Loss: 0.9075 | Train Acc: 90.19% | Val Loss: 1.0126 | Val Acc: 72.28% | Sparsity: 98.22%\n",
      "Epoch 15/50 | Train Loss: 0.8596 | Train Acc: 90.50% | Val Loss: 0.9895 | Val Acc: 72.66% | Sparsity: 98.22%\n",
      "Epoch 16/50 | Train Loss: 0.8236 | Train Acc: 90.99% | Val Loss: 1.0001 | Val Acc: 72.58% | Sparsity: 98.22%\n",
      "Epoch 17/50 | Train Loss: 0.7765 | Train Acc: 91.86% | Val Loss: 1.0007 | Val Acc: 72.22% | Sparsity: 98.22%\n",
      "Epoch 18/50 | Train Loss: 0.7459 | Train Acc: 92.03% | Val Loss: 0.9928 | Val Acc: 72.54% | Sparsity: 98.22%\n",
      "Epoch 19/50 | Train Loss: 0.7169 | Train Acc: 92.41% | Val Loss: 0.9951 | Val Acc: 72.68% | Sparsity: 98.22%\n",
      "Epoch 20/50 | Train Loss: 0.6912 | Train Acc: 92.71% | Val Loss: 1.0028 | Val Acc: 72.62% | Sparsity: 98.22%\n",
      "Epoch 21/50 | Train Loss: 0.6650 | Train Acc: 93.03% | Val Loss: 0.9979 | Val Acc: 72.68% | Sparsity: 98.22%\n",
      "Epoch 22/50 | Train Loss: 0.6483 | Train Acc: 93.31% | Val Loss: 1.0077 | Val Acc: 72.71% | Sparsity: 98.22%\n",
      "Epoch 23/50 | Train Loss: 0.6344 | Train Acc: 93.58% | Val Loss: 0.9830 | Val Acc: 72.74% | Sparsity: 98.22%\n",
      "Epoch 24/50 | Train Loss: 0.6170 | Train Acc: 93.65% | Val Loss: 1.0065 | Val Acc: 72.30% | Sparsity: 98.22%\n",
      "Epoch 25/50 | Train Loss: 0.6002 | Train Acc: 93.95% | Val Loss: 1.0039 | Val Acc: 72.63% | Sparsity: 98.22%\n",
      "Epoch 26/50 | Train Loss: 0.5844 | Train Acc: 94.08% | Val Loss: 0.9845 | Val Acc: 72.89% | Sparsity: 98.22%\n",
      "Epoch 27/50 | Train Loss: 0.5759 | Train Acc: 94.17% | Val Loss: 0.9828 | Val Acc: 72.95% | Sparsity: 98.22%\n",
      "Epoch 28/50 | Train Loss: 0.5611 | Train Acc: 94.28% | Val Loss: 1.0094 | Val Acc: 72.69% | Sparsity: 98.22%\n",
      "Epoch 29/50 | Train Loss: 0.5437 | Train Acc: 94.62% | Val Loss: 0.9933 | Val Acc: 73.13% | Sparsity: 98.22%\n",
      "Epoch 30/50 | Train Loss: 0.5399 | Train Acc: 94.67% | Val Loss: 1.0242 | Val Acc: 72.45% | Sparsity: 98.22%\n",
      "Epoch 31/50 | Train Loss: 0.5281 | Train Acc: 94.82% | Val Loss: 0.9919 | Val Acc: 73.23% | Sparsity: 98.22%\n",
      "Epoch 32/50 | Train Loss: 0.5170 | Train Acc: 94.83% | Val Loss: 0.9961 | Val Acc: 73.02% | Sparsity: 98.22%\n",
      "Epoch 33/50 | Train Loss: 0.5098 | Train Acc: 95.00% | Val Loss: 0.9915 | Val Acc: 73.17% | Sparsity: 98.22%\n",
      "Epoch 34/50 | Train Loss: 0.5016 | Train Acc: 95.02% | Val Loss: 1.0038 | Val Acc: 72.97% | Sparsity: 98.22%\n",
      "Epoch 35/50 | Train Loss: 0.4919 | Train Acc: 95.08% | Val Loss: 1.0160 | Val Acc: 72.76% | Sparsity: 98.22%\n",
      "Epoch 36/50 | Train Loss: 0.4860 | Train Acc: 95.27% | Val Loss: 0.9989 | Val Acc: 73.13% | Sparsity: 98.22%\n",
      "Epoch 37/50 | Train Loss: 0.4741 | Train Acc: 95.17% | Val Loss: 0.9967 | Val Acc: 73.04% | Sparsity: 98.22%\n",
      "Epoch 38/50 | Train Loss: 0.4719 | Train Acc: 95.31% | Val Loss: 0.9785 | Val Acc: 72.97% | Sparsity: 98.22%\n",
      "Early stopping triggered at epoch 38. No improvement for 7 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 42m 45s\n",
      "Retraining completed in 42.75 minutes (2565.11 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=7,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(Retrain with KD): 72.38%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(Retrain with KD): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90% Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 5s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.91)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with Val Accuracy: 75.54%\n",
      "Epoch 1/100 | Train Loss: 1.1331 | Train Acc: 74.40%\n",
      "Validation Loss: 0.8176 | Validation Acc: 75.54% | Sparsity: 90.75%\n",
      "\n",
      "New best model saved with Val Accuracy: 77.57%\n",
      "Epoch 2/100 | Train Loss: 0.4314 | Train Acc: 87.36%\n",
      "Validation Loss: 0.7451 | Validation Acc: 77.57% | Sparsity: 90.75%\n",
      "\n",
      "New best model saved with Val Accuracy: 78.02%\n",
      "Epoch 3/100 | Train Loss: 0.2947 | Train Acc: 91.50%\n",
      "Validation Loss: 0.7213 | Validation Acc: 78.02% | Sparsity: 90.75%\n",
      "\n",
      "New best model saved with Val Accuracy: 78.66%\n",
      "Epoch 4/100 | Train Loss: 0.2043 | Train Acc: 94.47%\n",
      "Validation Loss: 0.7312 | Validation Acc: 78.66% | Sparsity: 90.75%\n",
      "\n",
      "Epoch 5/100 | Train Loss: 0.1413 | Train Acc: 96.50%\n",
      "Validation Loss: 0.7575 | Validation Acc: 78.62% | Sparsity: 90.75%\n",
      "\n",
      "Epoch 6/100 | Train Loss: 0.0958 | Train Acc: 98.10%\n",
      "Validation Loss: 0.7824 | Validation Acc: 78.45% | Sparsity: 90.75%\n",
      "\n",
      "Epoch 7/100 | Train Loss: 0.0646 | Train Acc: 98.96%\n",
      "Validation Loss: 0.8168 | Validation Acc: 78.45% | Sparsity: 90.75%\n",
      "\n",
      "Epoch 8/100 | Train Loss: 0.0423 | Train Acc: 99.47%\n",
      "Validation Loss: 0.8328 | Validation Acc: 78.58% | Sparsity: 90.75%\n",
      "\n",
      "Early stopping triggered at epoch 9. No improvement for 5 epochs.\n",
      "Best Validation Accuracy: 78.66% | Best Model Saved at: retrained_student_model_80.pt\n",
      "Retraining completed in 7.00 minutes (420.29 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "retrained_student = retrain_with_sparsity(\n",
    "    pruned_student, train_loader, val_loader,\n",
    "    epochs=100,  save_path='retrained_student_model_80.pt',patience=5\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(Retrain without KD): 77.89%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(retrained_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(Retrain without KD): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 3s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.91)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 5.3654 | Train Acc: 76.50% | Val Loss: 1.1000 | Val Acc: 71.91% | Sparsity: 90.75%\n",
      "Epoch 2/50 | Train Loss: 2.6587 | Train Acc: 86.45% | Val Loss: 0.9261 | Val Acc: 76.34% | Sparsity: 90.75%\n",
      "Epoch 3/50 | Train Loss: 1.7471 | Train Acc: 90.65% | Val Loss: 0.8116 | Val Acc: 79.22% | Sparsity: 90.75%\n",
      "Epoch 4/50 | Train Loss: 1.3214 | Train Acc: 92.89% | Val Loss: 0.7560 | Val Acc: 80.03% | Sparsity: 90.75%\n",
      "Epoch 5/50 | Train Loss: 1.0560 | Train Acc: 94.11% | Val Loss: 0.7010 | Val Acc: 81.12% | Sparsity: 90.75%\n",
      "Epoch 6/50 | Train Loss: 0.9199 | Train Acc: 94.76% | Val Loss: 0.6706 | Val Acc: 81.66% | Sparsity: 90.75%\n",
      "Epoch 7/50 | Train Loss: 0.8292 | Train Acc: 95.08% | Val Loss: 0.6528 | Val Acc: 82.02% | Sparsity: 90.75%\n",
      "Epoch 8/50 | Train Loss: 0.7669 | Train Acc: 95.18% | Val Loss: 0.6556 | Val Acc: 82.05% | Sparsity: 90.75%\n",
      "Epoch 9/50 | Train Loss: 0.7148 | Train Acc: 95.48% | Val Loss: 0.6431 | Val Acc: 82.11% | Sparsity: 90.75%\n",
      "Epoch 10/50 | Train Loss: 0.6754 | Train Acc: 95.43% | Val Loss: 0.6580 | Val Acc: 82.05% | Sparsity: 90.75%\n",
      "Epoch 11/50 | Train Loss: 0.6484 | Train Acc: 95.60% | Val Loss: 0.6329 | Val Acc: 82.33% | Sparsity: 90.75%\n",
      "Epoch 12/50 | Train Loss: 0.6197 | Train Acc: 95.60% | Val Loss: 0.6380 | Val Acc: 82.48% | Sparsity: 90.75%\n",
      "Epoch 13/50 | Train Loss: 0.5988 | Train Acc: 95.74% | Val Loss: 0.6349 | Val Acc: 82.02% | Sparsity: 90.75%\n",
      "Epoch 14/50 | Train Loss: 0.5835 | Train Acc: 95.79% | Val Loss: 0.6315 | Val Acc: 82.36% | Sparsity: 90.75%\n",
      "Epoch 15/50 | Train Loss: 0.5664 | Train Acc: 95.81% | Val Loss: 0.6366 | Val Acc: 82.18% | Sparsity: 90.75%\n",
      "Epoch 16/50 | Train Loss: 0.5483 | Train Acc: 95.92% | Val Loss: 0.6351 | Val Acc: 82.34% | Sparsity: 90.75%\n",
      "Epoch 17/50 | Train Loss: 0.5392 | Train Acc: 95.88% | Val Loss: 0.6219 | Val Acc: 82.60% | Sparsity: 90.75%\n",
      "Epoch 18/50 | Train Loss: 0.5285 | Train Acc: 95.92% | Val Loss: 0.6189 | Val Acc: 82.20% | Sparsity: 90.75%\n",
      "Epoch 19/50 | Train Loss: 0.5212 | Train Acc: 95.87% | Val Loss: 0.6287 | Val Acc: 82.19% | Sparsity: 90.75%\n",
      "Epoch 20/50 | Train Loss: 0.5101 | Train Acc: 95.82% | Val Loss: 0.6198 | Val Acc: 82.59% | Sparsity: 90.75%\n",
      "Epoch 21/50 | Train Loss: 0.5066 | Train Acc: 95.97% | Val Loss: 0.6157 | Val Acc: 82.66% | Sparsity: 90.75%\n",
      "Epoch 22/50 | Train Loss: 0.4931 | Train Acc: 95.93% | Val Loss: 0.6186 | Val Acc: 82.54% | Sparsity: 90.75%\n",
      "Epoch 23/50 | Train Loss: 0.4735 | Train Acc: 95.95% | Val Loss: 0.6186 | Val Acc: 82.53% | Sparsity: 90.75%\n",
      "Epoch 24/50 | Train Loss: 0.4700 | Train Acc: 96.03% | Val Loss: 0.6179 | Val Acc: 82.27% | Sparsity: 90.75%\n",
      "Epoch 25/50 | Train Loss: 0.4581 | Train Acc: 95.94% | Val Loss: 0.6144 | Val Acc: 82.76% | Sparsity: 90.75%\n",
      "Epoch 26/50 | Train Loss: 0.4539 | Train Acc: 95.97% | Val Loss: 0.6217 | Val Acc: 82.47% | Sparsity: 90.75%\n",
      "Epoch 27/50 | Train Loss: 0.4545 | Train Acc: 96.03% | Val Loss: 0.6114 | Val Acc: 82.69% | Sparsity: 90.75%\n",
      "Epoch 28/50 | Train Loss: 0.4462 | Train Acc: 95.96% | Val Loss: 0.6081 | Val Acc: 82.68% | Sparsity: 90.75%\n",
      "Epoch 29/50 | Train Loss: 0.4390 | Train Acc: 96.02% | Val Loss: 0.6101 | Val Acc: 82.71% | Sparsity: 90.75%\n",
      "Epoch 30/50 | Train Loss: 0.4327 | Train Acc: 96.07% | Val Loss: 0.6082 | Val Acc: 82.58% | Sparsity: 90.75%\n",
      "Early stopping triggered at epoch 30. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 33m 39s\n",
      "Retraining completed in 33.66 minutes (2019.34 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(Retrain with KD): 81.44%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(Retrain with KD): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 4s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.91)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.5190 | Train Acc: 83.26% | Val Loss: 0.7699 | Val Acc: 78.36% | Sparsity: 90.75%\n",
      "Epoch 2/50 | Train Loss: 0.6432 | Train Acc: 91.43% | Val Loss: 0.7063 | Val Acc: 79.79% | Sparsity: 90.75%\n",
      "Epoch 3/50 | Train Loss: 0.4737 | Train Acc: 93.89% | Val Loss: 0.6700 | Val Acc: 80.91% | Sparsity: 90.75%\n",
      "Epoch 4/50 | Train Loss: 0.3940 | Train Acc: 95.08% | Val Loss: 0.6481 | Val Acc: 81.64% | Sparsity: 90.75%\n",
      "Epoch 5/50 | Train Loss: 0.3504 | Train Acc: 95.68% | Val Loss: 0.6513 | Val Acc: 81.81% | Sparsity: 90.75%\n",
      "Epoch 6/50 | Train Loss: 0.3183 | Train Acc: 95.97% | Val Loss: 0.6422 | Val Acc: 82.08% | Sparsity: 90.75%\n",
      "Epoch 7/50 | Train Loss: 0.2998 | Train Acc: 96.29% | Val Loss: 0.6433 | Val Acc: 81.94% | Sparsity: 90.75%\n",
      "Epoch 8/50 | Train Loss: 0.2834 | Train Acc: 96.48% | Val Loss: 0.6470 | Val Acc: 81.78% | Sparsity: 90.75%\n",
      "Epoch 9/50 | Train Loss: 0.2721 | Train Acc: 96.57% | Val Loss: 0.6489 | Val Acc: 81.63% | Sparsity: 90.75%\n",
      "Epoch 10/50 | Train Loss: 0.2605 | Train Acc: 96.68% | Val Loss: 0.6386 | Val Acc: 82.07% | Sparsity: 90.75%\n",
      "Epoch 11/50 | Train Loss: 0.2506 | Train Acc: 96.82% | Val Loss: 0.6352 | Val Acc: 82.07% | Sparsity: 90.75%\n",
      "Early stopping triggered at epoch 11. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 12m 20s\n",
      "Retraining completed in 12.34 minutes (740.26 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(Retrain with KD): 81.09%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(Retrain with KD): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80% Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 3s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.81)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Model Test Accuracy After Pruning: 1.14%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Student Model Test Accuracy After Pruning: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with Val Accuracy: 81.55%\n",
      "Epoch 1/100 | Train Loss: 0.3405 | Train Acc: 92.36%\n",
      "Validation Loss: 0.6221 | Validation Acc: 81.55% | Sparsity: 80.77%\n",
      "\n",
      "New best model saved with Val Accuracy: 81.92%\n",
      "Epoch 2/100 | Train Loss: 0.1411 | Train Acc: 96.31%\n",
      "Validation Loss: 0.6223 | Validation Acc: 81.92% | Sparsity: 80.77%\n",
      "\n",
      "New best model saved with Val Accuracy: 82.24%\n",
      "Epoch 3/100 | Train Loss: 0.0777 | Train Acc: 98.30%\n",
      "Validation Loss: 0.6408 | Validation Acc: 82.24% | Sparsity: 80.77%\n",
      "\n",
      "New best model saved with Val Accuracy: 82.27%\n",
      "Epoch 4/100 | Train Loss: 0.0411 | Train Acc: 99.38%\n",
      "Validation Loss: 0.6640 | Validation Acc: 82.27% | Sparsity: 80.77%\n",
      "\n",
      "New best model saved with Val Accuracy: 82.38%\n",
      "Epoch 5/100 | Train Loss: 0.0223 | Train Acc: 99.81%\n",
      "Validation Loss: 0.6837 | Validation Acc: 82.38% | Sparsity: 80.77%\n",
      "\n",
      "Epoch 6/100 | Train Loss: 0.0144 | Train Acc: 99.92%\n",
      "Validation Loss: 0.7026 | Validation Acc: 82.16% | Sparsity: 80.77%\n",
      "\n",
      "Epoch 7/100 | Train Loss: 0.0101 | Train Acc: 99.96%\n",
      "Validation Loss: 0.7101 | Validation Acc: 82.19% | Sparsity: 80.77%\n",
      "\n",
      "Epoch 8/100 | Train Loss: 0.0079 | Train Acc: 99.96%\n",
      "Validation Loss: 0.7257 | Validation Acc: 82.14% | Sparsity: 80.77%\n",
      "\n",
      "New best model saved with Val Accuracy: 82.41%\n",
      "Epoch 9/100 | Train Loss: 0.0059 | Train Acc: 99.97%\n",
      "Validation Loss: 0.7261 | Validation Acc: 82.41% | Sparsity: 80.77%\n",
      "\n",
      "Epoch 10/100 | Train Loss: 0.0051 | Train Acc: 99.98%\n",
      "Validation Loss: 0.7274 | Validation Acc: 82.17% | Sparsity: 80.77%\n",
      "\n",
      "Epoch 11/100 | Train Loss: 0.0043 | Train Acc: 99.98%\n",
      "Validation Loss: 0.7382 | Validation Acc: 82.21% | Sparsity: 80.77%\n",
      "\n",
      "Epoch 12/100 | Train Loss: 0.0039 | Train Acc: 99.98%\n",
      "Validation Loss: 0.7401 | Validation Acc: 82.25% | Sparsity: 80.77%\n",
      "\n",
      "Epoch 13/100 | Train Loss: 0.0036 | Train Acc: 99.98%\n",
      "Validation Loss: 0.7491 | Validation Acc: 82.20% | Sparsity: 80.77%\n",
      "\n",
      "Early stopping triggered at epoch 14. No improvement for 5 epochs.\n",
      "Best Validation Accuracy: 82.41% | Best Model Saved at: retrained_student_model_80.pt\n",
      "Retraining completed in 10.95 minutes (657.03 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "retrained_student = retrain_with_sparsity(\n",
    "    pruned_student, train_loader, val_loader,\n",
    "    epochs=100,  save_path='retrained_student_model_80.pt',patience=5\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(Retrain without KD): 81.09%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(retrained_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(Retrain without KD): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Measure inference times\n",
    "# pruned_student_inference_time = measure_inference_time(pruned_student, test_loader, device)\n",
    "# print(f\"Student Model Inference Time(After Pruning): {pruned_student_inference_time * 1000:.2f} ms per batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 4s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.81)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.7182 | Train Acc: 91.61% | Val Loss: 0.7152 | Val Acc: 80.61% | Sparsity: 80.77%\n",
      "Epoch 2/50 | Train Loss: 1.0003 | Train Acc: 94.42% | Val Loss: 0.6445 | Val Acc: 81.98% | Sparsity: 80.77%\n",
      "Epoch 3/50 | Train Loss: 0.7656 | Train Acc: 95.24% | Val Loss: 0.6271 | Val Acc: 82.60% | Sparsity: 80.77%\n",
      "Epoch 4/50 | Train Loss: 0.6587 | Train Acc: 95.48% | Val Loss: 0.5900 | Val Acc: 83.28% | Sparsity: 80.77%\n",
      "Epoch 5/50 | Train Loss: 0.6001 | Train Acc: 95.95% | Val Loss: 0.5946 | Val Acc: 83.38% | Sparsity: 80.77%\n",
      "Epoch 6/50 | Train Loss: 0.5556 | Train Acc: 95.78% | Val Loss: 0.5800 | Val Acc: 83.55% | Sparsity: 80.77%\n",
      "Epoch 7/50 | Train Loss: 0.5190 | Train Acc: 95.97% | Val Loss: 0.5825 | Val Acc: 83.68% | Sparsity: 80.77%\n",
      "Epoch 8/50 | Train Loss: 0.5023 | Train Acc: 95.98% | Val Loss: 0.5856 | Val Acc: 83.31% | Sparsity: 80.77%\n",
      "Epoch 9/50 | Train Loss: 0.4875 | Train Acc: 96.07% | Val Loss: 0.5814 | Val Acc: 83.72% | Sparsity: 80.77%\n",
      "Epoch 10/50 | Train Loss: 0.4659 | Train Acc: 96.11% | Val Loss: 0.5757 | Val Acc: 83.68% | Sparsity: 80.77%\n",
      "Epoch 11/50 | Train Loss: 0.4533 | Train Acc: 96.03% | Val Loss: 0.5801 | Val Acc: 83.40% | Sparsity: 80.77%\n",
      "Epoch 12/50 | Train Loss: 0.4457 | Train Acc: 96.08% | Val Loss: 0.5758 | Val Acc: 83.30% | Sparsity: 80.77%\n",
      "Epoch 13/50 | Train Loss: 0.4355 | Train Acc: 96.10% | Val Loss: 0.5786 | Val Acc: 83.41% | Sparsity: 80.77%\n",
      "Epoch 14/50 | Train Loss: 0.4170 | Train Acc: 96.06% | Val Loss: 0.5836 | Val Acc: 83.23% | Sparsity: 80.77%\n",
      "Early stopping triggered at epoch 14. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 15m 51s\n",
      "Retraining completed in 15.86 minutes (951.31 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(Retrain with KD): 81.91%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(Retrain with KD): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 6s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.81)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.5290 | Train Acc: 93.96% | Val Loss: 0.6048 | Val Acc: 82.81% | Sparsity: 80.77%\n",
      "Epoch 2/50 | Train Loss: 0.2883 | Train Acc: 96.08% | Val Loss: 0.5885 | Val Acc: 82.87% | Sparsity: 80.77%\n",
      "Epoch 3/50 | Train Loss: 0.2448 | Train Acc: 96.66% | Val Loss: 0.5770 | Val Acc: 83.28% | Sparsity: 80.77%\n",
      "Epoch 4/50 | Train Loss: 0.2234 | Train Acc: 96.84% | Val Loss: 0.5761 | Val Acc: 83.48% | Sparsity: 80.77%\n",
      "Epoch 5/50 | Train Loss: 0.2075 | Train Acc: 97.10% | Val Loss: 0.5773 | Val Acc: 83.37% | Sparsity: 80.77%\n",
      "Epoch 6/50 | Train Loss: 0.1992 | Train Acc: 97.07% | Val Loss: 0.5767 | Val Acc: 83.16% | Sparsity: 80.77%\n",
      "Epoch 7/50 | Train Loss: 0.1920 | Train Acc: 97.20% | Val Loss: 0.5763 | Val Acc: 83.42% | Sparsity: 80.77%\n",
      "Epoch 8/50 | Train Loss: 0.1870 | Train Acc: 97.19% | Val Loss: 0.5773 | Val Acc: 83.50% | Sparsity: 80.77%\n",
      "Epoch 9/50 | Train Loss: 0.1818 | Train Acc: 97.20% | Val Loss: 0.5756 | Val Acc: 83.34% | Sparsity: 80.77%\n",
      "Epoch 10/50 | Train Loss: 0.1754 | Train Acc: 97.39% | Val Loss: 0.5755 | Val Acc: 83.22% | Sparsity: 80.77%\n",
      "Epoch 11/50 | Train Loss: 0.1724 | Train Acc: 97.33% | Val Loss: 0.5776 | Val Acc: 83.54% | Sparsity: 80.77%\n",
      "Epoch 12/50 | Train Loss: 0.1697 | Train Acc: 97.39% | Val Loss: 0.5747 | Val Acc: 83.32% | Sparsity: 80.77%\n",
      "Epoch 13/50 | Train Loss: 0.1653 | Train Acc: 97.40% | Val Loss: 0.5722 | Val Acc: 83.66% | Sparsity: 80.77%\n",
      "Epoch 14/50 | Train Loss: 0.1619 | Train Acc: 97.46% | Val Loss: 0.5765 | Val Acc: 83.54% | Sparsity: 80.77%\n",
      "Epoch 15/50 | Train Loss: 0.1602 | Train Acc: 97.42% | Val Loss: 0.5756 | Val Acc: 83.38% | Sparsity: 80.77%\n",
      "Epoch 16/50 | Train Loss: 0.1587 | Train Acc: 97.49% | Val Loss: 0.5755 | Val Acc: 83.57% | Sparsity: 80.77%\n",
      "Epoch 17/50 | Train Loss: 0.1573 | Train Acc: 97.46% | Val Loss: 0.5768 | Val Acc: 83.37% | Sparsity: 80.77%\n",
      "Epoch 18/50 | Train Loss: 0.1558 | Train Acc: 97.50% | Val Loss: 0.5774 | Val Acc: 83.52% | Sparsity: 80.77%\n",
      "Early stopping triggered at epoch 18. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 20m 19s\n",
      "Retraining completed in 20.31 minutes (1218.60 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(Retrain with KD): 82.21%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(Retrain with KD): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 75% of Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 4s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.75062)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Model Test Accuracy After Pruning: 3.73%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Student Model Test Accuracy After Pruning: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with Val Accuracy: 82.74%\n",
      "Epoch 1/100 | Train Loss: 0.2018 | Train Acc: 95.19%\n",
      "Validation Loss: 0.5919 | Validation Acc: 82.74% | Sparsity: 74.85%\n",
      "\n",
      "New best model saved with Val Accuracy: 82.89%\n",
      "Epoch 2/100 | Train Loss: 0.0895 | Train Acc: 97.83%\n",
      "Validation Loss: 0.6123 | Validation Acc: 82.89% | Sparsity: 74.85%\n",
      "\n",
      "Epoch 3/100 | Train Loss: 0.0444 | Train Acc: 99.23%\n",
      "Validation Loss: 0.6392 | Validation Acc: 82.70% | Sparsity: 74.85%\n",
      "\n",
      "New best model saved with Val Accuracy: 82.99%\n",
      "Epoch 4/100 | Train Loss: 0.0217 | Train Acc: 99.81%\n",
      "Validation Loss: 0.6550 | Validation Acc: 82.99% | Sparsity: 74.85%\n",
      "\n",
      "New best model saved with Val Accuracy: 83.20%\n",
      "Epoch 5/100 | Train Loss: 0.0137 | Train Acc: 99.92%\n",
      "Validation Loss: 0.6587 | Validation Acc: 83.20% | Sparsity: 74.85%\n",
      "\n",
      "Epoch 6/100 | Train Loss: 0.0092 | Train Acc: 99.95%\n",
      "Validation Loss: 0.6694 | Validation Acc: 83.09% | Sparsity: 74.85%\n",
      "\n",
      "Epoch 7/100 | Train Loss: 0.0069 | Train Acc: 99.97%\n",
      "Validation Loss: 0.6815 | Validation Acc: 82.97% | Sparsity: 74.85%\n",
      "\n",
      "Epoch 8/100 | Train Loss: 0.0055 | Train Acc: 99.98%\n",
      "Validation Loss: 0.6861 | Validation Acc: 83.10% | Sparsity: 74.85%\n",
      "\n",
      "Epoch 9/100 | Train Loss: 0.0046 | Train Acc: 99.97%\n",
      "Validation Loss: 0.6916 | Validation Acc: 83.20% | Sparsity: 74.85%\n",
      "\n",
      "Early stopping triggered at epoch 10. No improvement for 5 epochs.\n",
      "Best Validation Accuracy: 83.20% | Best Model Saved at: retrained_student_model_75.pt\n",
      "Retraining completed in 7.77 minutes (466.44 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "retrained_student = retrain_with_sparsity(\n",
    "    pruned_student, train_loader, val_loader,\n",
    "    epochs=100,  save_path='retrained_student_model_75.pt',patience=5\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrained Pruned Student Model Test Accuracy: 81.70%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(retrained_student, test_loader, device)\n",
    "print(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Measure inference times\n",
    "# pruned_student_inference_time = measure_inference_time(retrained_student, test_loader,)\n",
    "# print(f\" Retrained pruned Student Model Inference Time: {pruned_student_inference_time * 1000:.2f} ms per batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain with KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 3s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.75062)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.0007 | Train Acc: 94.87% | Val Loss: 0.6224 | Val Acc: 82.23% | Sparsity: 74.85%\n",
      "Epoch 2/50 | Train Loss: 0.6937 | Train Acc: 95.58% | Val Loss: 0.6009 | Val Acc: 83.03% | Sparsity: 74.85%\n",
      "Epoch 3/50 | Train Loss: 0.5904 | Train Acc: 95.77% | Val Loss: 0.5831 | Val Acc: 83.58% | Sparsity: 74.85%\n",
      "Epoch 4/50 | Train Loss: 0.5346 | Train Acc: 95.99% | Val Loss: 0.5719 | Val Acc: 83.46% | Sparsity: 74.85%\n",
      "Epoch 5/50 | Train Loss: 0.5063 | Train Acc: 95.94% | Val Loss: 0.5622 | Val Acc: 84.02% | Sparsity: 74.85%\n",
      "Epoch 6/50 | Train Loss: 0.4791 | Train Acc: 96.03% | Val Loss: 0.5722 | Val Acc: 83.67% | Sparsity: 74.85%\n",
      "Epoch 7/50 | Train Loss: 0.4567 | Train Acc: 96.14% | Val Loss: 0.5636 | Val Acc: 83.75% | Sparsity: 74.85%\n",
      "Epoch 8/50 | Train Loss: 0.4442 | Train Acc: 96.12% | Val Loss: 0.5624 | Val Acc: 83.72% | Sparsity: 74.85%\n",
      "Epoch 9/50 | Train Loss: 0.4273 | Train Acc: 96.19% | Val Loss: 0.5700 | Val Acc: 83.37% | Sparsity: 74.85%\n",
      "Epoch 10/50 | Train Loss: 0.4157 | Train Acc: 96.17% | Val Loss: 0.5607 | Val Acc: 83.82% | Sparsity: 74.85%\n",
      "Early stopping triggered at epoch 10. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 11m 11s\n",
      "Retraining completed in 11.18 minutes (670.88 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrained Pruned Student Model Test Accuracy: 82.25%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Measure inference times\n",
    "# pruned_student_inference_time = measure_inference_time(pruned_student, test_loader, device)\n",
    "# print(f\" Retrained pruned Student Model Inference Time: {pruned_student_inference_time * 1000:.2f} ms per batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 4s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.75062)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.3480 | Train Acc: 95.70% | Val Loss: 0.5682 | Val Acc: 83.23% | Sparsity: 74.85%\n",
      "Epoch 2/50 | Train Loss: 0.2246 | Train Acc: 96.70% | Val Loss: 0.5605 | Val Acc: 83.62% | Sparsity: 74.85%\n",
      "Epoch 3/50 | Train Loss: 0.1993 | Train Acc: 97.04% | Val Loss: 0.5564 | Val Acc: 83.54% | Sparsity: 74.85%\n",
      "Epoch 4/50 | Train Loss: 0.1857 | Train Acc: 97.17% | Val Loss: 0.5544 | Val Acc: 83.63% | Sparsity: 74.85%\n",
      "Epoch 5/50 | Train Loss: 0.1767 | Train Acc: 97.27% | Val Loss: 0.5539 | Val Acc: 83.93% | Sparsity: 74.85%\n",
      "Epoch 6/50 | Train Loss: 0.1706 | Train Acc: 97.27% | Val Loss: 0.5525 | Val Acc: 84.20% | Sparsity: 74.85%\n",
      "Epoch 7/50 | Train Loss: 0.1656 | Train Acc: 97.33% | Val Loss: 0.5545 | Val Acc: 83.89% | Sparsity: 74.85%\n",
      "Epoch 8/50 | Train Loss: 0.1607 | Train Acc: 97.32% | Val Loss: 0.5532 | Val Acc: 83.69% | Sparsity: 74.85%\n",
      "Epoch 9/50 | Train Loss: 0.1583 | Train Acc: 97.41% | Val Loss: 0.5531 | Val Acc: 84.11% | Sparsity: 74.85%\n",
      "Epoch 10/50 | Train Loss: 0.1545 | Train Acc: 97.47% | Val Loss: 0.5546 | Val Acc: 83.90% | Sparsity: 74.85%\n",
      "Epoch 11/50 | Train Loss: 0.1524 | Train Acc: 97.50% | Val Loss: 0.5568 | Val Acc: 83.81% | Sparsity: 74.85%\n",
      "Early stopping triggered at epoch 11. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 12m 28s\n",
      "Retraining completed in 12.46 minutes (747.53 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrained Pruned Student Model Test Accuracy: 82.41%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50% Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 5s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.51)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Model Test Accuracy After Pruning: 70.74%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Student Model Test Accuracy After Pruning: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with Val Accuracy: 84.56%\n",
      "Epoch 1/200 | Train Loss: 0.0912 | Train Acc: 97.17%\n",
      "Validation Loss: 0.5540 | Validation Acc: 84.56% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 2/200 | Train Loss: 0.0382 | Train Acc: 99.04%\n",
      "Validation Loss: 0.5882 | Validation Acc: 84.34% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 3/200 | Train Loss: 0.0168 | Train Acc: 99.79%\n",
      "Validation Loss: 0.6084 | Validation Acc: 84.27% | Sparsity: 50.86%\n",
      "\n",
      "New best model saved with Val Accuracy: 84.64%\n",
      "Epoch 4/200 | Train Loss: 0.0098 | Train Acc: 99.92%\n",
      "Validation Loss: 0.6134 | Validation Acc: 84.64% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 5/200 | Train Loss: 0.0065 | Train Acc: 99.96%\n",
      "Validation Loss: 0.6214 | Validation Acc: 84.56% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 6/200 | Train Loss: 0.0050 | Train Acc: 99.97%\n",
      "Validation Loss: 0.6364 | Validation Acc: 84.45% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 7/200 | Train Loss: 0.0040 | Train Acc: 99.98%\n",
      "Validation Loss: 0.6402 | Validation Acc: 84.61% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 8/200 | Train Loss: 0.0035 | Train Acc: 99.98%\n",
      "Validation Loss: 0.6442 | Validation Acc: 84.53% | Sparsity: 50.86%\n",
      "\n",
      "New best model saved with Val Accuracy: 84.65%\n",
      "Epoch 9/200 | Train Loss: 0.0030 | Train Acc: 99.97%\n",
      "Validation Loss: 0.6527 | Validation Acc: 84.65% | Sparsity: 50.86%\n",
      "\n",
      "New best model saved with Val Accuracy: 84.67%\n",
      "Epoch 10/200 | Train Loss: 0.0026 | Train Acc: 99.98%\n",
      "Validation Loss: 0.6542 | Validation Acc: 84.67% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 11/200 | Train Loss: 0.0023 | Train Acc: 99.98%\n",
      "Validation Loss: 0.6557 | Validation Acc: 84.58% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 12/200 | Train Loss: 0.0022 | Train Acc: 99.98%\n",
      "Validation Loss: 0.6575 | Validation Acc: 84.55% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 13/200 | Train Loss: 0.0019 | Train Acc: 99.98%\n",
      "Validation Loss: 0.6632 | Validation Acc: 84.51% | Sparsity: 50.86%\n",
      "\n",
      "Epoch 14/200 | Train Loss: 0.0019 | Train Acc: 99.97%\n",
      "Validation Loss: 0.6674 | Validation Acc: 84.49% | Sparsity: 50.86%\n",
      "\n",
      "Early stopping triggered at epoch 15. No improvement for 5 epochs.\n",
      "Best Validation Accuracy: 84.67% | Best Model Saved at: retrained_student_model_50%.pt\n",
      "Retraining completed in 11.86 minutes (711.73 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "retrained_student = retrain_with_sparsity(\n",
    "    pruned_student, train_loader, val_loader,\n",
    "    epochs=200,  save_path='retrained_student_model_50%.pt',patience=5\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrained Pruned Student Model Test Accuracy: 83.18%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 4s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.51)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.4399 | Train Acc: 97.09% | Val Loss: 0.5499 | Val Acc: 84.19% | Sparsity: 50.86%\n",
      "Epoch 2/50 | Train Loss: 0.4146 | Train Acc: 96.54% | Val Loss: 0.5534 | Val Acc: 84.66% | Sparsity: 50.86%\n",
      "Epoch 3/50 | Train Loss: 0.3946 | Train Acc: 96.41% | Val Loss: 0.5542 | Val Acc: 83.99% | Sparsity: 50.86%\n",
      "Epoch 4/50 | Train Loss: 0.3818 | Train Acc: 96.38% | Val Loss: 0.5532 | Val Acc: 84.46% | Sparsity: 50.86%\n",
      "Epoch 5/50 | Train Loss: 0.3733 | Train Acc: 96.37% | Val Loss: 0.5522 | Val Acc: 84.11% | Sparsity: 50.86%\n",
      "Epoch 6/50 | Train Loss: 0.3678 | Train Acc: 96.42% | Val Loss: 0.5505 | Val Acc: 84.48% | Sparsity: 50.86%\n",
      "Epoch 7/50 | Train Loss: 0.3570 | Train Acc: 96.34% | Val Loss: 0.5513 | Val Acc: 84.36% | Sparsity: 50.86%\n",
      "Early stopping triggered at epoch 7. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 7m 59s\n",
      "Retraining completed in 7.98 minutes (478.80 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrained Pruned Student Model Test Accuracy: 82.83%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 3m 3s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.51)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.1621 | Train Acc: 97.33% | Val Loss: 0.5357 | Val Acc: 84.59% | Sparsity: 50.86%\n",
      "Epoch 2/50 | Train Loss: 0.1420 | Train Acc: 97.46% | Val Loss: 0.5385 | Val Acc: 84.65% | Sparsity: 50.86%\n",
      "Epoch 3/50 | Train Loss: 0.1350 | Train Acc: 97.65% | Val Loss: 0.5383 | Val Acc: 84.52% | Sparsity: 50.86%\n",
      "Epoch 4/50 | Train Loss: 0.1310 | Train Acc: 97.62% | Val Loss: 0.5379 | Val Acc: 84.61% | Sparsity: 50.86%\n",
      "Epoch 5/50 | Train Loss: 0.1287 | Train Acc: 97.69% | Val Loss: 0.5361 | Val Acc: 84.73% | Sparsity: 50.86%\n",
      "Epoch 6/50 | Train Loss: 0.1273 | Train Acc: 97.64% | Val Loss: 0.5403 | Val Acc: 84.82% | Sparsity: 50.86%\n",
      "Epoch 7/50 | Train Loss: 0.1258 | Train Acc: 97.72% | Val Loss: 0.5384 | Val Acc: 84.62% | Sparsity: 50.86%\n",
      "Epoch 8/50 | Train Loss: 0.1249 | Train Acc: 97.75% | Val Loss: 0.5423 | Val Acc: 84.56% | Sparsity: 50.86%\n",
      "Epoch 9/50 | Train Loss: 0.1226 | Train Acc: 97.73% | Val Loss: 0.5415 | Val Acc: 84.76% | Sparsity: 50.86%\n",
      "Epoch 10/50 | Train Loss: 0.1225 | Train Acc: 97.79% | Val Loss: 0.5410 | Val Acc: 84.66% | Sparsity: 50.86%\n",
      "Epoch 11/50 | Train Loss: 0.1222 | Train Acc: 97.74% | Val Loss: 0.5442 | Val Acc: 84.67% | Sparsity: 50.86%\n",
      "Early stopping triggered at epoch 11. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 12m 27s\n",
      "Retraining completed in 12.46 minutes (747.43 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrained Pruned Student Model Test Accuracy: 83.12%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 268576,
     "modelInstanceId": 247034,
     "sourceId": 288333,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
