{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomCrop(32, padding=4),  # Randomly crop the image\n",
    "    transforms.Resize(224),  # Resize to 224x224 for ResNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# No augmentation for validation and test\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # Resize to 224x224 for ResNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_val_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=val_test_transform)\n",
    "\n",
    "# Split train_val_dataset into train and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# Apply val_test_transform to the validation set\n",
    "val_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet-50 (Teacher Model)\n",
    "teacher = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer for 10 classes (CIFAR-10)\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, 10)\n",
    "# Move models to device\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'Best_Teacher.pth'\n",
    "# Load the model weights\n",
    "teacher.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet-18 (Student Model)\n",
    "student = models.resnet18(pretrained=True)\n",
    "# Modify the final fully connected layer for 10 classes (CIFAR-10)\n",
    "student.fc = nn.Linear(student.fc.in_features, 10)\n",
    "student = student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits normalization function\n",
    "def normalize(logit):\n",
    "    mean = logit.mean(dim=-1, keepdim=True)\n",
    "    stdv = logit.std(dim=-1, keepdim=True)\n",
    "    return (logit - mean) / (1e-7 + stdv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA-KLD Loss for Classification\n",
    "def cakld_loss(student_logits, teacher_logits, beta_prob):\n",
    "    # Forward KL (student || teacher)\n",
    "    student_log_prob = F.log_softmax(student_logits, dim=1)\n",
    "    teacher_prob = F.softmax(teacher_logits, dim=1)\n",
    "    forward_kl = F.kl_div(student_log_prob, teacher_prob, reduction='batchmean')\n",
    "\n",
    "    # Reverse KL (teacher || student)\n",
    "    teacher_log_prob = F.log_softmax(teacher_logits, dim=1)\n",
    "    student_prob = F.softmax(student_logits, dim=1)\n",
    "    reverse_kl = F.kl_div(teacher_log_prob, student_prob, reduction='batchmean')\n",
    "\n",
    "    # Combined KL loss\n",
    "    kl_loss = beta_prob * reverse_kl + (1 - beta_prob) * forward_kl\n",
    "    return kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    model = model.to(device)  # Ensure model is on the correct device\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model):\n",
    "    total_zeros = 0\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_zeros += torch.sum(param == 0).item()\n",
    "            total_params += param.numel()\n",
    "    return total_zeros / total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "def measure_inference_time(model, test_loader, num_runs=5):\n",
    "    device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Warm-up (one batch to avoid startup cost)\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model(inputs)\n",
    "            break\n",
    "\n",
    "    total_time = 0\n",
    "    total_images = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            for inputs, _ in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                batch_size = inputs.size(0)\n",
    "                start_time = time.time()\n",
    "                _ = model(inputs)\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += (end_time - start_time)\n",
    "                total_images += batch_size\n",
    "\n",
    "    avg_time_per_image = total_time / total_images\n",
    "    return avg_time_per_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def calculate_model_size(model, filename=\"temp.pth\"):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    size = os.path.getsize(filename) / (1024 * 1024)  # Size in MB\n",
    "    os.remove(filename)\n",
    "    return size\n",
    "\n",
    "def compare_model_sizes(teacher, student, pruned_student):\n",
    "    # Count parameters\n",
    "    teacher_params = count_parameters(teacher)\n",
    "    student_params = count_parameters(student)\n",
    "    pruned_params = count_parameters(pruned_student)\n",
    "    \n",
    "    # Calculate disk size\n",
    "    teacher_size = calculate_model_size(teacher, \"teacher.pth\")\n",
    "    student_size = calculate_model_size(student, \"student.pth\")\n",
    "    pruned_size = calculate_model_size(pruned_student, \"pruned_student.pth\")\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n--- Model Size Comparison ---\")\n",
    "    print(f\"Teacher Model: {teacher_params} parameters, {teacher_size:.2f} MB\")\n",
    "    print(f\"Student Model (Before Pruning): {student_params} parameters, {student_size:.2f} MB\")\n",
    "    print(f\"Student Model (After Pruning): {pruned_params} parameters, {pruned_size:.2f} MB\")\n",
    "    \n",
    "    # Calculate compression ratio\n",
    "    compression_ratio = student_size / pruned_size\n",
    "    print(f\"\\nCompression Ratio: {compression_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001, patience=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluate on the validation set\n",
    "        val_accuracy = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(train_loader):.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            torch.save(model.state_dict(), 'best_teacher_model.pth')  # Save the best model\n",
    "            print(f\" New best model saved with validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" No improvement in validation accuracy ({patience_counter}/{patience})\")\n",
    "            \n",
    "            # Stop training if no improvement for 'patience' epochs\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered! No improvement for {patience} epochs.\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model state\n",
    "    model.load_state_dict(torch.load('best_teacher_model.pth'))\n",
    "    print(\"\\nLoading the best model for final evaluation.\")\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_accuracy = evaluate(model, test_loader, device)\n",
    "    print(f\"Test Accuracy with Best Model: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_importance(\n",
    "    teacher, student, data_loader, device, temperature=4.0, alpha=0.5, beta_prob=0.5, accumulation_epochs=3\n",
    "):\n",
    "    importance_scores = {}\n",
    "\n",
    "    # Initialize importance score storage for conv layer weights only\n",
    "    for name, param in student.named_parameters():\n",
    "        if 'weight' in name and len(param.shape) == 4:  # Conv weights only\n",
    "            importance_scores[name] = torch.zeros_like(param.data, device=device)\n",
    "\n",
    "    teacher.to(device).eval()\n",
    "    student.to(device).train()\n",
    "\n",
    "    # Add momentum for gradient accumulation smoothing\n",
    "    momentum = 0.9  # Controls exponential moving average\n",
    "    accumulated_batches = 0  # Track for bias correction\n",
    "\n",
    "    for epoch in range(accumulation_epochs):\n",
    "        print(f\"Accumulation Epoch {epoch+1}/{accumulation_epochs}\")\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            student.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Temperature scaling\n",
    "            student_logits_temp = student_logits / temperature\n",
    "            teacher_logits_temp = teacher_logits / temperature\n",
    "\n",
    "\n",
    "            # Compute losses\n",
    "            distillation_loss = cakld_loss(student_logits_temp, teacher_logits_temp, beta_prob) * (temperature ** 2)\n",
    "            ce_loss = F.cross_entropy(student_logits, labels)\n",
    "            loss = alpha * distillation_loss + (1 - alpha) * ce_loss\n",
    "\n",
    "            # Modified backward propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Accumulate importance scores with parameter-gradient product\n",
    "            accumulated_batches += 1\n",
    "            for name, param in student.named_parameters():\n",
    "                if name in importance_scores and param.grad is not None:\n",
    "                    # Key modification: Use parameter-gradient product magnitude\n",
    "                    grad_product = (param.data * param.grad).abs_()\n",
    "                    \n",
    "                    # Exponential moving average with bias correction\n",
    "                    if accumulated_batches == 1:\n",
    "                        importance_scores[name] = grad_product\n",
    "                    else:\n",
    "                        importance_scores[name] = momentum * importance_scores[name] + (1 - momentum) * grad_product\n",
    "\n",
    "    # Apply bias correction for EMA\n",
    "    for name in importance_scores:\n",
    "        importance_scores[name] /= (1 - momentum**accumulated_batches)\n",
    "\n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_based_global_prune(model, importance_scores, prune_ratio=0.95):\n",
    "    all_scores = torch.cat([score.flatten() for score in importance_scores.values()])\n",
    "    threshold = torch.topk(all_scores, k=int(prune_ratio * all_scores.numel()), largest=False)[0][-1]\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in importance_scores:\n",
    "            mask = (importance_scores[name] > threshold).float()\n",
    "            param.data.mul_(mask)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def retrain_with_sparsity(student, train_loader, val_loader, epochs=5, save_path=\"retrained_student_model.pt\", patience=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # 1. Store masks AND zero momentum buffers for pruned weights\n",
    "    masks = {}\n",
    "    for name, param in student.named_parameters():\n",
    "        if 'weight' in name and param.dim() == 4:  # Consider only conv layers\n",
    "            mask = (param != 0).float().to(device)\n",
    "            masks[name] = mask\n",
    "            # Zero momentum buffers for pruned weights\n",
    "            if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n",
    "                optimizer.state[param]['momentum_buffer'] *= mask\n",
    "\n",
    "    student = student.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    best_model = None\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "\n",
    "    # 2. Add gradient clipping to prevent NaN\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        total_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = student(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Apply masks to gradients\n",
    "            for name, param in student.named_parameters():\n",
    "                if name in masks:\n",
    "                    param.grad.data *= masks[name]\n",
    "\n",
    "            # Gradient clipping before optimizer step\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Reapply masks and update momentum buffers\n",
    "            for name, param in student.named_parameters():\n",
    "                if name in masks:\n",
    "                    param.data *= masks[name]\n",
    "                    if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n",
    "                        optimizer.state[param]['momentum_buffer'] *= masks[name]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        student.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        # Track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = student.state_dict()\n",
    "            torch.save(best_model, save_path)\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            print(f\"New best model saved with Val Accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
    "                break  # Stop training\n",
    "\n",
    "        # Print results\n",
    "        sparsity = calculate_sparsity(student)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f} | Validation Acc: {val_acc:.2f}% | Sparsity: {sparsity*100:.2f}%\\n\")\n",
    "\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}% | Best Model Saved at: {save_path}\")\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# KD training with CA-KLD loss and mask-based momentum handling\n",
    "def retrain_with_KD(teacher, student, train_loader, val_loader, epochs=50,\n",
    "                    temperature=5.0, alpha=0.5, beta_prob=0.5, patience=5,\n",
    "                    save_path=\"student_before_pruning.pth\"):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    # 1. Store masks and zero momentum buffers\n",
    "    masks = {}\n",
    "    for name, param in student.named_parameters():\n",
    "        if 'weight' in name and param.dim() == 4:\n",
    "            mask = (param != 0).float().to(device)\n",
    "            masks[name] = mask\n",
    "            if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n",
    "                optimizer.state[param]['momentum_buffer'] *= mask\n",
    "\n",
    "    teacher = teacher.to(device).eval()\n",
    "    student = student.to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Apply temperature\n",
    "            teacher_logits_temp = teacher_logits / temperature\n",
    "            student_logits_temp = student_logits / temperature\n",
    "\n",
    "            # Logits normalization\n",
    "            teacher_logits_temp = normalize(teacher_logits_temp)\n",
    "            student_logits_temp = normalize(student_logits_temp)\n",
    "\n",
    "\n",
    "            # CA-KLD loss\n",
    "            kd_loss = cakld_loss(student_logits_temp, teacher_logits_temp, beta_prob) * (temperature ** 2)\n",
    "            ce_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Reapply masks and update momentum\n",
    "            for name, param in student.named_parameters():\n",
    "                if name in masks:\n",
    "                    param.data *= masks[name]\n",
    "                    if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n",
    "                        optimizer.state[param]['momentum_buffer'] *= masks[name]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = student_logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation\n",
    "        student.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = student(inputs)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        sparsity = calculate_sparsity(student) * 100.0  # Assuming this function is defined elsewhere\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Sparsity: {sparsity:.2f}%\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = student.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "    # Restore and save best model\n",
    "    student.load_state_dict(best_model_state)\n",
    "    torch.save(student.state_dict(), save_path)\n",
    "    print(f\"Student model saved before pruning at: {save_path}\")\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total Training Time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training function with KD + CA-KLD and logits normalization\n",
    "def train_kd_pruning(teacher, student, train_loader, val_loader, epochs=50, temperature=5.0, alpha=0.5,\n",
    "                     beta_prob=0.5, patience=5, save_path=\"student_before_pruning.pth\"):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    teacher.eval()  # Freeze teacher\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        total_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Temperature scaling\n",
    "            teacher_logits_temp = teacher_logits / temperature\n",
    "            student_logits_temp = student_logits / temperature\n",
    "\n",
    "            # Logits normalization\n",
    "            teacher_logits_temp = normalize(teacher_logits_temp)\n",
    "            student_logits_temp = normalize(student_logits_temp)\n",
    "\n",
    "            # CA-KLD loss (normalized logits)\n",
    "            distillation_loss = cakld_loss(student_logits_temp, teacher_logits_temp, beta_prob) * (temperature ** 2)\n",
    "\n",
    "            # Cross-entropy loss\n",
    "            ground_truth_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = alpha * distillation_loss + (1 - alpha) * ground_truth_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = student_logits.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # Validation accuracy\n",
    "        val_acc = evaluate(student, val_loader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | \"\n",
    "              f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = student.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "    # Load best model state and save\n",
    "    student.load_state_dict(best_model_state)\n",
    "    torch.save(student.state_dict(), save_path)\n",
    "    print(f\"Student model saved before pruning at: {save_path}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total Training Time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "    return student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95% Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 29s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.9508)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 95.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.6680 | Train Acc: 91.05% | Val Loss: 0.2156 | Val Acc: 93.42% | Sparsity: 95.00%\n",
      "Epoch 2/50 | Train Loss: 0.3079 | Train Acc: 96.12% | Val Loss: 0.1725 | Val Acc: 94.64% | Sparsity: 95.00%\n",
      "Epoch 3/50 | Train Loss: 0.2172 | Train Acc: 97.70% | Val Loss: 0.1779 | Val Acc: 94.52% | Sparsity: 95.00%\n",
      "Epoch 4/50 | Train Loss: 0.1719 | Train Acc: 98.44% | Val Loss: 0.1378 | Val Acc: 95.55% | Sparsity: 95.00%\n",
      "Epoch 5/50 | Train Loss: 0.1452 | Train Acc: 98.81% | Val Loss: 0.1471 | Val Acc: 95.51% | Sparsity: 95.00%\n",
      "Epoch 6/50 | Train Loss: 0.1294 | Train Acc: 98.97% | Val Loss: 0.1434 | Val Acc: 95.49% | Sparsity: 95.00%\n",
      "Epoch 7/50 | Train Loss: 0.1177 | Train Acc: 99.19% | Val Loss: 0.1342 | Val Acc: 95.68% | Sparsity: 95.00%\n",
      "Epoch 8/50 | Train Loss: 0.1090 | Train Acc: 99.22% | Val Loss: 0.1325 | Val Acc: 95.59% | Sparsity: 95.00%\n",
      "Epoch 9/50 | Train Loss: 0.1049 | Train Acc: 99.19% | Val Loss: 0.1319 | Val Acc: 95.85% | Sparsity: 95.00%\n",
      "Epoch 10/50 | Train Loss: 0.1016 | Train Acc: 99.26% | Val Loss: 0.1321 | Val Acc: 95.82% | Sparsity: 95.00%\n",
      "Epoch 11/50 | Train Loss: 0.0978 | Train Acc: 99.25% | Val Loss: 0.1385 | Val Acc: 95.78% | Sparsity: 95.00%\n",
      "Epoch 12/50 | Train Loss: 0.0935 | Train Acc: 99.26% | Val Loss: 0.1335 | Val Acc: 95.67% | Sparsity: 95.00%\n",
      "Epoch 13/50 | Train Loss: 0.0896 | Train Acc: 99.33% | Val Loss: 0.1245 | Val Acc: 96.03% | Sparsity: 95.00%\n",
      "Epoch 14/50 | Train Loss: 0.0885 | Train Acc: 99.25% | Val Loss: 0.1340 | Val Acc: 95.67% | Sparsity: 95.00%\n",
      "Epoch 15/50 | Train Loss: 0.0845 | Train Acc: 99.32% | Val Loss: 0.1278 | Val Acc: 95.91% | Sparsity: 95.00%\n",
      "Epoch 16/50 | Train Loss: 0.0829 | Train Acc: 99.31% | Val Loss: 0.1342 | Val Acc: 95.69% | Sparsity: 95.00%\n",
      "Epoch 17/50 | Train Loss: 0.0810 | Train Acc: 99.36% | Val Loss: 0.1299 | Val Acc: 95.87% | Sparsity: 95.00%\n",
      "Epoch 18/50 | Train Loss: 0.0799 | Train Acc: 99.30% | Val Loss: 0.1288 | Val Acc: 95.82% | Sparsity: 95.00%\n",
      "Early stopping triggered at epoch 18. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 16m 23s\n",
      "Retraining completed in 16.39 minutes (983.28 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 94.09%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 28s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.9508)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 95.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 2.3030 | Train Acc: 87.75% | Val Loss: 0.5408 | Val Acc: 88.66% | Sparsity: 95.00%\n",
      "Epoch 2/50 | Train Loss: 1.1570 | Train Acc: 93.82% | Val Loss: 0.4443 | Val Acc: 90.19% | Sparsity: 95.00%\n",
      "Epoch 3/50 | Train Loss: 0.8297 | Train Acc: 95.83% | Val Loss: 0.3078 | Val Acc: 92.52% | Sparsity: 95.00%\n",
      "Epoch 4/50 | Train Loss: 0.6606 | Train Acc: 96.88% | Val Loss: 0.2449 | Val Acc: 93.40% | Sparsity: 95.00%\n",
      "Epoch 5/50 | Train Loss: 0.5237 | Train Acc: 97.78% | Val Loss: 0.2710 | Val Acc: 92.98% | Sparsity: 95.00%\n",
      "Epoch 6/50 | Train Loss: 0.4255 | Train Acc: 98.42% | Val Loss: 0.2072 | Val Acc: 94.40% | Sparsity: 95.00%\n",
      "Epoch 7/50 | Train Loss: 0.3572 | Train Acc: 98.78% | Val Loss: 0.1955 | Val Acc: 94.47% | Sparsity: 95.00%\n",
      "Epoch 8/50 | Train Loss: 0.3228 | Train Acc: 98.81% | Val Loss: 0.1756 | Val Acc: 95.06% | Sparsity: 95.00%\n",
      "Epoch 9/50 | Train Loss: 0.3016 | Train Acc: 99.02% | Val Loss: 0.1736 | Val Acc: 94.83% | Sparsity: 95.00%\n",
      "Epoch 10/50 | Train Loss: 0.2756 | Train Acc: 98.98% | Val Loss: 0.1664 | Val Acc: 95.10% | Sparsity: 95.00%\n",
      "Epoch 11/50 | Train Loss: 0.2607 | Train Acc: 99.02% | Val Loss: 0.1657 | Val Acc: 95.05% | Sparsity: 95.00%\n",
      "Epoch 12/50 | Train Loss: 0.2453 | Train Acc: 99.08% | Val Loss: 0.1605 | Val Acc: 95.17% | Sparsity: 95.00%\n",
      "Epoch 13/50 | Train Loss: 0.2345 | Train Acc: 99.08% | Val Loss: 0.1643 | Val Acc: 95.08% | Sparsity: 95.00%\n",
      "Epoch 14/50 | Train Loss: 0.2280 | Train Acc: 99.13% | Val Loss: 0.1660 | Val Acc: 95.05% | Sparsity: 95.00%\n",
      "Epoch 15/50 | Train Loss: 0.2225 | Train Acc: 98.98% | Val Loss: 0.1660 | Val Acc: 95.06% | Sparsity: 95.00%\n",
      "Epoch 16/50 | Train Loss: 0.2112 | Train Acc: 99.13% | Val Loss: 0.1555 | Val Acc: 95.30% | Sparsity: 95.00%\n",
      "Epoch 17/50 | Train Loss: 0.2038 | Train Acc: 99.10% | Val Loss: 0.1702 | Val Acc: 95.28% | Sparsity: 95.00%\n",
      "Epoch 18/50 | Train Loss: 0.1980 | Train Acc: 99.11% | Val Loss: 0.1575 | Val Acc: 95.21% | Sparsity: 95.00%\n",
      "Epoch 19/50 | Train Loss: 0.1935 | Train Acc: 99.11% | Val Loss: 0.1517 | Val Acc: 95.33% | Sparsity: 95.00%\n",
      "Epoch 20/50 | Train Loss: 0.1917 | Train Acc: 99.08% | Val Loss: 0.1607 | Val Acc: 95.27% | Sparsity: 95.00%\n",
      "Epoch 21/50 | Train Loss: 0.1866 | Train Acc: 99.12% | Val Loss: 0.1570 | Val Acc: 95.20% | Sparsity: 95.00%\n",
      "Epoch 22/50 | Train Loss: 0.1801 | Train Acc: 99.10% | Val Loss: 0.1547 | Val Acc: 95.41% | Sparsity: 95.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 95.88%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90% Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 28s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n",
      "Sparsity: 90.00%\n"
     ]
    }
   ],
   "source": [
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.9008)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.3262 | Train Acc: 95.87% | Val Loss: 0.1165 | Val Acc: 96.43% | Sparsity: 90.00%\n",
      "Epoch 2/50 | Train Loss: 0.1695 | Train Acc: 98.32% | Val Loss: 0.0930 | Val Acc: 97.19% | Sparsity: 90.00%\n",
      "Epoch 3/50 | Train Loss: 0.1240 | Train Acc: 98.98% | Val Loss: 0.0776 | Val Acc: 97.63% | Sparsity: 90.00%\n",
      "Epoch 4/50 | Train Loss: 0.1073 | Train Acc: 99.20% | Val Loss: 0.0784 | Val Acc: 97.53% | Sparsity: 90.00%\n",
      "Epoch 5/50 | Train Loss: 0.0975 | Train Acc: 99.21% | Val Loss: 0.0801 | Val Acc: 97.51% | Sparsity: 90.00%\n",
      "Epoch 6/50 | Train Loss: 0.0881 | Train Acc: 99.29% | Val Loss: 0.0755 | Val Acc: 97.67% | Sparsity: 90.00%\n",
      "Epoch 7/50 | Train Loss: 0.0852 | Train Acc: 99.30% | Val Loss: 0.0753 | Val Acc: 97.68% | Sparsity: 90.00%\n",
      "Epoch 8/50 | Train Loss: 0.0799 | Train Acc: 99.38% | Val Loss: 0.0761 | Val Acc: 97.64% | Sparsity: 90.00%\n",
      "Epoch 9/50 | Train Loss: 0.0777 | Train Acc: 99.33% | Val Loss: 0.0731 | Val Acc: 97.77% | Sparsity: 90.00%\n",
      "Epoch 10/50 | Train Loss: 0.0758 | Train Acc: 99.31% | Val Loss: 0.0734 | Val Acc: 97.77% | Sparsity: 90.00%\n",
      "Epoch 11/50 | Train Loss: 0.0727 | Train Acc: 99.35% | Val Loss: 0.0763 | Val Acc: 97.65% | Sparsity: 90.00%\n",
      "Epoch 12/50 | Train Loss: 0.0692 | Train Acc: 99.37% | Val Loss: 0.0735 | Val Acc: 97.61% | Sparsity: 90.00%\n",
      "Epoch 13/50 | Train Loss: 0.0695 | Train Acc: 99.37% | Val Loss: 0.0723 | Val Acc: 97.68% | Sparsity: 90.00%\n",
      "Epoch 14/50 | Train Loss: 0.0676 | Train Acc: 99.37% | Val Loss: 0.0718 | Val Acc: 97.86% | Sparsity: 90.00%\n",
      "Epoch 15/50 | Train Loss: 0.0660 | Train Acc: 99.34% | Val Loss: 0.0718 | Val Acc: 97.76% | Sparsity: 90.00%\n",
      "Epoch 16/50 | Train Loss: 0.0633 | Train Acc: 99.33% | Val Loss: 0.0718 | Val Acc: 97.80% | Sparsity: 90.00%\n",
      "Epoch 17/50 | Train Loss: 0.0621 | Train Acc: 99.37% | Val Loss: 0.0720 | Val Acc: 97.83% | Sparsity: 90.00%\n",
      "Epoch 18/50 | Train Loss: 0.0615 | Train Acc: 99.36% | Val Loss: 0.0714 | Val Acc: 97.86% | Sparsity: 90.00%\n",
      "Epoch 19/50 | Train Loss: 0.0604 | Train Acc: 99.35% | Val Loss: 0.0757 | Val Acc: 97.56% | Sparsity: 90.00%\n",
      "Early stopping triggered at epoch 19. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 17m 13s\n",
      "Retraining completed in 17.22 minutes (1033.37 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 94.68%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 28s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n",
      "Sparsity: 90.00%\n"
     ]
    }
   ],
   "source": [
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.9008)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.3378 | Train Acc: 92.73% | Val Loss: 0.2690 | Val Acc: 93.05% | Sparsity: 90.00%\n",
      "Epoch 2/50 | Train Loss: 0.7029 | Train Acc: 96.57% | Val Loss: 0.1775 | Val Acc: 94.99% | Sparsity: 90.00%\n",
      "Epoch 3/50 | Train Loss: 0.4881 | Train Acc: 97.92% | Val Loss: 0.1637 | Val Acc: 95.56% | Sparsity: 90.00%\n",
      "Epoch 4/50 | Train Loss: 0.3708 | Train Acc: 98.60% | Val Loss: 0.1260 | Val Acc: 96.32% | Sparsity: 90.00%\n",
      "Epoch 5/50 | Train Loss: 0.2967 | Train Acc: 98.97% | Val Loss: 0.1204 | Val Acc: 96.33% | Sparsity: 90.00%\n",
      "Epoch 6/50 | Train Loss: 0.2589 | Train Acc: 99.06% | Val Loss: 0.1070 | Val Acc: 96.83% | Sparsity: 90.00%\n",
      "Epoch 7/50 | Train Loss: 0.2348 | Train Acc: 99.07% | Val Loss: 0.1014 | Val Acc: 96.82% | Sparsity: 90.00%\n",
      "Epoch 8/50 | Train Loss: 0.2183 | Train Acc: 99.11% | Val Loss: 0.1056 | Val Acc: 96.78% | Sparsity: 90.00%\n",
      "Epoch 9/50 | Train Loss: 0.2065 | Train Acc: 99.09% | Val Loss: 0.0995 | Val Acc: 96.87% | Sparsity: 90.00%\n",
      "Epoch 10/50 | Train Loss: 0.1949 | Train Acc: 99.07% | Val Loss: 0.0983 | Val Acc: 97.04% | Sparsity: 90.00%\n",
      "Epoch 11/50 | Train Loss: 0.1848 | Train Acc: 99.11% | Val Loss: 0.1088 | Val Acc: 96.68% | Sparsity: 90.00%\n",
      "Epoch 12/50 | Train Loss: 0.1784 | Train Acc: 99.16% | Val Loss: 0.0994 | Val Acc: 96.88% | Sparsity: 90.00%\n",
      "Epoch 13/50 | Train Loss: 0.1723 | Train Acc: 99.13% | Val Loss: 0.1017 | Val Acc: 96.83% | Sparsity: 90.00%\n",
      "Epoch 14/50 | Train Loss: 0.1648 | Train Acc: 99.11% | Val Loss: 0.0998 | Val Acc: 96.91% | Sparsity: 90.00%\n",
      "Epoch 15/50 | Train Loss: 0.1617 | Train Acc: 99.19% | Val Loss: 0.0988 | Val Acc: 96.85% | Sparsity: 90.00%\n",
      "Early stopping triggered at epoch 15. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 13m 54s\n",
      "Retraining completed in 13.90 minutes (833.73 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 94.83%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 79% Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 28s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n",
      "Sparsity: 79.00%\n"
     ]
    }
   ],
   "source": [
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.7907)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.1645 | Train Acc: 98.17% | Val Loss: 0.0591 | Val Acc: 98.33% | Sparsity: 79.00%\n",
      "Epoch 2/50 | Train Loss: 0.1029 | Train Acc: 99.00% | Val Loss: 0.0507 | Val Acc: 98.44% | Sparsity: 79.00%\n",
      "Epoch 3/50 | Train Loss: 0.0830 | Train Acc: 99.30% | Val Loss: 0.0477 | Val Acc: 98.63% | Sparsity: 79.00%\n",
      "Epoch 4/50 | Train Loss: 0.0745 | Train Acc: 99.32% | Val Loss: 0.0457 | Val Acc: 98.58% | Sparsity: 79.00%\n",
      "Epoch 5/50 | Train Loss: 0.0690 | Train Acc: 99.40% | Val Loss: 0.0457 | Val Acc: 98.65% | Sparsity: 79.00%\n",
      "Epoch 6/50 | Train Loss: 0.0644 | Train Acc: 99.38% | Val Loss: 0.0467 | Val Acc: 98.62% | Sparsity: 79.00%\n",
      "Epoch 7/50 | Train Loss: 0.0638 | Train Acc: 99.37% | Val Loss: 0.0454 | Val Acc: 98.67% | Sparsity: 79.00%\n",
      "Epoch 8/50 | Train Loss: 0.0601 | Train Acc: 99.38% | Val Loss: 0.0464 | Val Acc: 98.66% | Sparsity: 79.00%\n",
      "Epoch 9/50 | Train Loss: 0.0583 | Train Acc: 99.40% | Val Loss: 0.0462 | Val Acc: 98.72% | Sparsity: 79.00%\n",
      "Epoch 10/50 | Train Loss: 0.0568 | Train Acc: 99.38% | Val Loss: 0.0472 | Val Acc: 98.65% | Sparsity: 79.00%\n",
      "Epoch 11/50 | Train Loss: 0.0547 | Train Acc: 99.42% | Val Loss: 0.0443 | Val Acc: 98.66% | Sparsity: 79.00%\n",
      "Epoch 12/50 | Train Loss: 0.0531 | Train Acc: 99.36% | Val Loss: 0.0458 | Val Acc: 98.65% | Sparsity: 79.00%\n",
      "Epoch 13/50 | Train Loss: 0.0522 | Train Acc: 99.46% | Val Loss: 0.0457 | Val Acc: 98.61% | Sparsity: 79.00%\n",
      "Epoch 14/50 | Train Loss: 0.0523 | Train Acc: 99.36% | Val Loss: 0.0446 | Val Acc: 98.61% | Sparsity: 79.00%\n",
      "Early stopping triggered at epoch 14. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 12m 54s\n",
      "Retraining completed in 12.90 minutes (774.12 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 95.68%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 28s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n",
      "Sparsity: 79.00%\n"
     ]
    }
   ],
   "source": [
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.7907)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.6436 | Train Acc: 96.80% | Val Loss: 0.1194 | Val Acc: 96.77% | Sparsity: 79.00%\n",
      "Epoch 2/50 | Train Loss: 0.3933 | Train Acc: 98.37% | Val Loss: 0.0922 | Val Acc: 97.27% | Sparsity: 79.00%\n",
      "Epoch 3/50 | Train Loss: 0.2880 | Train Acc: 98.88% | Val Loss: 0.0750 | Val Acc: 97.75% | Sparsity: 79.00%\n",
      "Epoch 4/50 | Train Loss: 0.2278 | Train Acc: 99.08% | Val Loss: 0.0632 | Val Acc: 98.08% | Sparsity: 79.00%\n",
      "Epoch 5/50 | Train Loss: 0.1989 | Train Acc: 99.14% | Val Loss: 0.0607 | Val Acc: 98.02% | Sparsity: 79.00%\n",
      "Epoch 6/50 | Train Loss: 0.1831 | Train Acc: 99.14% | Val Loss: 0.0594 | Val Acc: 98.22% | Sparsity: 79.00%\n",
      "Epoch 7/50 | Train Loss: 0.1708 | Train Acc: 99.15% | Val Loss: 0.0570 | Val Acc: 98.27% | Sparsity: 79.00%\n",
      "Epoch 8/50 | Train Loss: 0.1612 | Train Acc: 99.11% | Val Loss: 0.0580 | Val Acc: 98.36% | Sparsity: 79.00%\n",
      "Epoch 9/50 | Train Loss: 0.1528 | Train Acc: 99.16% | Val Loss: 0.0529 | Val Acc: 98.44% | Sparsity: 79.00%\n",
      "Epoch 10/50 | Train Loss: 0.1461 | Train Acc: 99.14% | Val Loss: 0.0574 | Val Acc: 98.34% | Sparsity: 79.00%\n",
      "Epoch 11/50 | Train Loss: 0.1370 | Train Acc: 99.22% | Val Loss: 0.0541 | Val Acc: 98.36% | Sparsity: 79.00%\n",
      "Epoch 12/50 | Train Loss: 0.1347 | Train Acc: 99.13% | Val Loss: 0.0536 | Val Acc: 98.41% | Sparsity: 79.00%\n",
      "Epoch 13/50 | Train Loss: 0.1301 | Train Acc: 99.17% | Val Loss: 0.0542 | Val Acc: 98.43% | Sparsity: 79.00%\n",
      "Epoch 14/50 | Train Loss: 0.1269 | Train Acc: 99.15% | Val Loss: 0.0543 | Val Acc: 98.38% | Sparsity: 79.00%\n",
      "Early stopping triggered at epoch 14. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 12m 46s\n",
      "Retraining completed in 12.76 minutes (765.76 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 95.52%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 59% Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 30s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n",
      "Sparsity: 59.00%\n"
     ]
    }
   ],
   "source": [
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.5905)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.1113 | Train Acc: 98.66% | Val Loss: 0.0461 | Val Acc: 98.64% | Sparsity: 59.00%\n",
      "Epoch 2/50 | Train Loss: 0.0738 | Train Acc: 99.22% | Val Loss: 0.0401 | Val Acc: 98.87% | Sparsity: 59.00%\n",
      "Epoch 3/50 | Train Loss: 0.0610 | Train Acc: 99.40% | Val Loss: 0.0398 | Val Acc: 98.84% | Sparsity: 59.00%\n",
      "Epoch 4/50 | Train Loss: 0.0569 | Train Acc: 99.36% | Val Loss: 0.0385 | Val Acc: 98.90% | Sparsity: 59.00%\n",
      "Epoch 5/50 | Train Loss: 0.0532 | Train Acc: 99.43% | Val Loss: 0.0380 | Val Acc: 98.84% | Sparsity: 59.00%\n",
      "Epoch 6/50 | Train Loss: 0.0511 | Train Acc: 99.43% | Val Loss: 0.0385 | Val Acc: 98.82% | Sparsity: 59.00%\n",
      "Epoch 7/50 | Train Loss: 0.0497 | Train Acc: 99.42% | Val Loss: 0.0396 | Val Acc: 98.82% | Sparsity: 59.00%\n",
      "Epoch 8/50 | Train Loss: 0.0472 | Train Acc: 99.41% | Val Loss: 0.0380 | Val Acc: 98.83% | Sparsity: 59.00%\n",
      "Epoch 9/50 | Train Loss: 0.0469 | Train Acc: 99.44% | Val Loss: 0.0376 | Val Acc: 98.86% | Sparsity: 59.00%\n",
      "Early stopping triggered at epoch 9. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 8m 17s\n",
      "Retraining completed in 8.29 minutes (497.34 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 95.84%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 28s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n",
      "Sparsity: 59.00%\n"
     ]
    }
   ],
   "source": [
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.5905)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.3811 | Train Acc: 98.25% | Val Loss: 0.0721 | Val Acc: 98.08% | Sparsity: 59.00%\n",
      "Epoch 2/50 | Train Loss: 0.2529 | Train Acc: 98.89% | Val Loss: 0.0452 | Val Acc: 98.69% | Sparsity: 59.00%\n",
      "Epoch 3/50 | Train Loss: 0.1925 | Train Acc: 99.10% | Val Loss: 0.0468 | Val Acc: 98.67% | Sparsity: 59.00%\n",
      "Epoch 4/50 | Train Loss: 0.1669 | Train Acc: 99.11% | Val Loss: 0.0466 | Val Acc: 98.71% | Sparsity: 59.00%\n",
      "Epoch 5/50 | Train Loss: 0.1516 | Train Acc: 99.15% | Val Loss: 0.0449 | Val Acc: 98.67% | Sparsity: 59.00%\n",
      "Epoch 6/50 | Train Loss: 0.1403 | Train Acc: 99.14% | Val Loss: 0.0411 | Val Acc: 98.87% | Sparsity: 59.00%\n",
      "Epoch 7/50 | Train Loss: 0.1319 | Train Acc: 99.17% | Val Loss: 0.0433 | Val Acc: 98.72% | Sparsity: 59.00%\n",
      "Epoch 8/50 | Train Loss: 0.1256 | Train Acc: 99.14% | Val Loss: 0.0415 | Val Acc: 98.83% | Sparsity: 59.00%\n",
      "Epoch 9/50 | Train Loss: 0.1213 | Train Acc: 99.19% | Val Loss: 0.0401 | Val Acc: 98.80% | Sparsity: 59.00%\n",
      "Epoch 10/50 | Train Loss: 0.1147 | Train Acc: 99.18% | Val Loss: 0.0407 | Val Acc: 98.89% | Sparsity: 59.00%\n",
      "Epoch 11/50 | Train Loss: 0.1124 | Train Acc: 99.17% | Val Loss: 0.0407 | Val Acc: 98.84% | Sparsity: 59.00%\n",
      "Epoch 12/50 | Train Loss: 0.1082 | Train Acc: 99.20% | Val Loss: 0.0390 | Val Acc: 98.88% | Sparsity: 59.00%\n",
      "Epoch 13/50 | Train Loss: 0.1037 | Train Acc: 99.18% | Val Loss: 0.0423 | Val Acc: 98.75% | Sparsity: 59.00%\n",
      "Epoch 14/50 | Train Loss: 0.1015 | Train Acc: 99.19% | Val Loss: 0.0431 | Val Acc: 98.85% | Sparsity: 59.00%\n",
      "Epoch 15/50 | Train Loss: 0.0985 | Train Acc: 99.22% | Val Loss: 0.0415 | Val Acc: 98.84% | Sparsity: 59.00%\n",
      "Early stopping triggered at epoch 15. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 13m 43s\n",
      "Retraining completed in 13.72 minutes (823.40 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 95.80%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36% Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 29s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n",
      "Sparsity: 36.00%\n"
     ]
    }
   ],
   "source": [
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=3.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.3603)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.0591 | Train Acc: 99.16% | Val Loss: 0.0370 | Val Acc: 98.94% | Sparsity: 36.00%\n",
      "Epoch 2/50 | Train Loss: 0.0507 | Train Acc: 99.38% | Val Loss: 0.0354 | Val Acc: 98.97% | Sparsity: 36.00%\n",
      "Epoch 3/50 | Train Loss: 0.0478 | Train Acc: 99.38% | Val Loss: 0.0350 | Val Acc: 98.94% | Sparsity: 36.00%\n",
      "Epoch 4/50 | Train Loss: 0.0452 | Train Acc: 99.45% | Val Loss: 0.0357 | Val Acc: 98.94% | Sparsity: 36.00%\n",
      "Epoch 5/50 | Train Loss: 0.0430 | Train Acc: 99.42% | Val Loss: 0.0356 | Val Acc: 98.94% | Sparsity: 36.00%\n",
      "Epoch 6/50 | Train Loss: 0.0416 | Train Acc: 99.48% | Val Loss: 0.0362 | Val Acc: 98.93% | Sparsity: 36.00%\n",
      "Epoch 7/50 | Train Loss: 0.0410 | Train Acc: 99.42% | Val Loss: 0.0352 | Val Acc: 98.93% | Sparsity: 36.00%\n",
      "Early stopping triggered at epoch 7. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 6m 24s\n",
      "Retraining completed in 6.40 minutes (383.97 seconds)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=3.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 96.05%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Important Scores\n",
      "Accumulation Epoch 1/3\n",
      "Accumulation Epoch 2/3\n",
      "Accumulation Epoch 3/3\n",
      "Total Time take to calculate Important scores: 2m 29s\n",
      "Pruning the model\n",
      "Total Time take to prune the model scores: 0m 0s\n",
      "Sparsity: 36.00%\n"
     ]
    }
   ],
   "source": [
    "model_path = 'student_before_pruning.pth'\n",
    "# Load the model weights\n",
    "student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Pruning\n",
    "print(\"Calculating Important Scores\")\n",
    "start_time = time.time()\n",
    "importance_scores = compute_gradient_importance(\n",
    "    teacher, student, train_loader, device, temperature=5.0, alpha=0.7,beta_prob=0.5, accumulation_epochs=3\n",
    ")\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "\n",
    "print(\"Pruning the model\")\n",
    "start_time = time.time()\n",
    "pruned_student = gradient_based_global_prune(student, importance_scores, prune_ratio=0.3603)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "student = student.to(device)\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = calculate_sparsity(pruned_student)\n",
    "print(f\"Sparsity: {sparsity * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.3268 | Train Acc: 98.47% | Val Loss: 0.0557 | Val Acc: 98.28% | Sparsity: 36.00%\n",
      "Epoch 2/50 | Train Loss: 0.2215 | Train Acc: 99.00% | Val Loss: 0.0457 | Val Acc: 98.68% | Sparsity: 36.00%\n",
      "Epoch 3/50 | Train Loss: 0.1718 | Train Acc: 99.08% | Val Loss: 0.0408 | Val Acc: 98.79% | Sparsity: 36.00%\n",
      "Epoch 4/50 | Train Loss: 0.1502 | Train Acc: 99.16% | Val Loss: 0.0413 | Val Acc: 98.86% | Sparsity: 36.00%\n",
      "Epoch 5/50 | Train Loss: 0.1366 | Train Acc: 99.17% | Val Loss: 0.0399 | Val Acc: 98.89% | Sparsity: 36.00%\n",
      "Epoch 6/50 | Train Loss: 0.1278 | Train Acc: 99.19% | Val Loss: 0.0377 | Val Acc: 98.87% | Sparsity: 36.00%\n",
      "Epoch 7/50 | Train Loss: 0.1159 | Train Acc: 99.17% | Val Loss: 0.0375 | Val Acc: 98.86% | Sparsity: 36.00%\n",
      "Epoch 8/50 | Train Loss: 0.1123 | Train Acc: 99.22% | Val Loss: 0.0381 | Val Acc: 98.93% | Sparsity: 36.00%\n",
      "Epoch 9/50 | Train Loss: 0.1091 | Train Acc: 99.18% | Val Loss: 0.0395 | Val Acc: 98.89% | Sparsity: 36.00%\n",
      "Epoch 10/50 | Train Loss: 0.1068 | Train Acc: 99.18% | Val Loss: 0.0388 | Val Acc: 98.87% | Sparsity: 36.00%\n",
      "Epoch 11/50 | Train Loss: 0.1025 | Train Acc: 99.22% | Val Loss: 0.0388 | Val Acc: 98.91% | Sparsity: 36.00%\n",
      "Epoch 12/50 | Train Loss: 0.0997 | Train Acc: 99.18% | Val Loss: 0.0380 | Val Acc: 98.92% | Sparsity: 36.00%\n",
      "Epoch 13/50 | Train Loss: 0.0968 | Train Acc: 99.20% | Val Loss: 0.0404 | Val Acc: 98.88% | Sparsity: 36.00%\n",
      "Early stopping triggered at epoch 13. No improvement for 5 epochs.\n",
      "Student model saved before pruning at: pruned_student_retrain_KD_90%.pth\n",
      "Total Training Time: 11m 53s\n",
      "Retraining completed in 11.89 minutes (713.46 seconds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "pruned_student = retrain_with_KD(\n",
    "    teacher, pruned_student, train_loader, val_loader,\n",
    "    epochs=50, temperature=5.0, alpha=0.7, beta_prob=0.5,patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n",
    ")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Retraining completed in {elapsed_time / 60:.2f} minutes ({elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Student Model Test Accuracy(After Retrain): 95.74%\n"
     ]
    }
   ],
   "source": [
    "student_accuracy = evaluate(pruned_student, test_loader, device)\n",
    "print(f\"Pruned Student Model Test Accuracy(After Retrain): {student_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 268576,
     "modelInstanceId": 247034,
     "sourceId": 288333,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
