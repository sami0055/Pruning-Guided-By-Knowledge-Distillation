{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":267855,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":229231,"modelId":250975},{"sourceId":271899,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":232784,"modelId":254512},{"sourceId":286844,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":245801,"modelId":267412}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader, random_split\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:52:48.733921Z","iopub.execute_input":"2025-03-16T18:52:48.734245Z","iopub.status.idle":"2025-03-16T18:52:54.601889Z","shell.execute_reply.started":"2025-03-16T18:52:48.734217Z","shell.execute_reply":"2025-03-16T18:52:54.600982Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:52:54.602978Z","iopub.execute_input":"2025-03-16T18:52:54.603413Z","iopub.status.idle":"2025-03-16T18:52:54.650638Z","shell.execute_reply.started":"2025-03-16T18:52:54.603380Z","shell.execute_reply":"2025-03-16T18:52:54.649320Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Data augmentation for training\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n    transforms.RandomCrop(32, padding=4),  # Randomly crop the image\n    transforms.Resize(224),  # Resize to 224x224 for ResNet\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# No augmentation for validation and test\nval_test_transform = transforms.Compose([\n    transforms.Resize(224),  # Resize to 224x224 for ResNet\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load CIFAR-10 dataset\ntrain_val_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=val_test_transform)\n\n# Split train_val_dataset into train and validation sets (80% train, 20% validation)\ntrain_size = int(0.8 * len(train_val_dataset))\nval_size = len(train_val_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])\n\n# Apply val_test_transform to the validation set\nval_dataset.dataset.transform = val_test_transform\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:52:54.651950Z","iopub.execute_input":"2025-03-16T18:52:54.652164Z","iopub.status.idle":"2025-03-16T18:53:00.933910Z","shell.execute_reply.started":"2025-03-16T18:52:54.652147Z","shell.execute_reply":"2025-03-16T18:53:00.933186Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:02<00:00, 59.1MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load pretrained ResNet-50 (Teacher Model)\nteacher = models.resnet50(pretrained=True)\n\n# Modify the final fully connected layer for 10 classes (CIFAR-10)\nteacher.fc = nn.Linear(teacher.fc.in_features, 10)\n# Move models to device\nteacher = teacher.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:00.935026Z","iopub.execute_input":"2025-03-16T18:53:00.935273Z","iopub.status.idle":"2025-03-16T18:53:02.180685Z","shell.execute_reply.started":"2025-03-16T18:53:00.935253Z","shell.execute_reply":"2025-03-16T18:53:02.179989Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 198MB/s] \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\nmodel_path = '/kaggle/input/best_teacher/pytorch/default/1/Best_Teacher.pth'\n# Load the model weights\nteacher.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:02.181448Z","iopub.execute_input":"2025-03-16T18:53:02.181663Z","iopub.status.idle":"2025-03-16T18:53:03.581594Z","shell.execute_reply.started":"2025-03-16T18:53:02.181645Z","shell.execute_reply":"2025-03-16T18:53:03.580737Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-5-79fc1ac30e55>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  teacher.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Load pretrained ResNet-18 (Student Model)\nstudent = models.resnet34(pretrained=True)\n# Modify the final fully connected layer for 10 classes (CIFAR-10)\nstudent.fc = nn.Linear(student.fc.in_features, 10)\nstudent = student.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:03.582539Z","iopub.execute_input":"2025-03-16T18:53:03.582854Z","iopub.status.idle":"2025-03-16T18:53:04.802413Z","shell.execute_reply.started":"2025-03-16T18:53:03.582824Z","shell.execute_reply":"2025-03-16T18:53:04.801536Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 114MB/s] \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def evaluate(model, test_loader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:04.803893Z","iopub.execute_input":"2025-03-16T18:53:04.804128Z","iopub.status.idle":"2025-03-16T18:53:04.808724Z","shell.execute_reply.started":"2025-03-16T18:53:04.804109Z","shell.execute_reply":"2025-03-16T18:53:04.807936Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def calculate_sparsity(model):\n    total_zeros = 0\n    total_params = 0\n    for name, param in model.named_parameters():\n        if 'weight' in name:\n            total_zeros += torch.sum(param == 0).item()\n            total_params += param.numel()\n    return total_zeros / total_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:04.809893Z","iopub.execute_input":"2025-03-16T18:53:04.810252Z","iopub.status.idle":"2025-03-16T18:53:04.842237Z","shell.execute_reply.started":"2025-03-16T18:53:04.810217Z","shell.execute_reply":"2025-03-16T18:53:04.841300Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def measure_inference_time(model, test_loader, device, num_runs=5):\n    model.eval()\n    model.to(device)\n    \n    # Warm-up (to avoid initial overhead)\n    with torch.no_grad():\n        for inputs, _ in test_loader:\n            inputs = inputs.to(device)\n            _ = model(inputs)\n            break  # Only one batch for warm-up\n    \n    # Measure inference time\n    total_time = 0\n    with torch.no_grad():\n        for _ in range(num_runs):\n            for inputs, _ in test_loader:\n                inputs = inputs.to(device)\n                start_time = time.time()  # Start timer\n                _ = model(inputs)\n                end_time = time.time()  # End timer\n                total_time += (end_time - start_time)\n    \n    # Average inference time per batch\n    avg_time_per_batch = total_time / (num_runs * len(test_loader))\n    return avg_time_per_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:04.843184Z","iopub.execute_input":"2025-03-16T18:53:04.843504Z","iopub.status.idle":"2025-03-16T18:53:04.857933Z","shell.execute_reply.started":"2025-03-16T18:53:04.843476Z","shell.execute_reply":"2025-03-16T18:53:04.857261Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef calculate_model_size(model, filename=\"temp.pth\"):\n    torch.save(model.state_dict(), filename)\n    size = os.path.getsize(filename) / (1024 * 1024)  # Size in MB\n    os.remove(filename)\n    return size\n\ndef compare_model_sizes(teacher, student, pruned_student):\n    # Count parameters\n    teacher_params = count_parameters(teacher)\n    student_params = count_parameters(student)\n    pruned_params = count_parameters(pruned_student)\n    \n    # Calculate disk size\n    teacher_size = calculate_model_size(teacher, \"teacher.pth\")\n    student_size = calculate_model_size(student, \"student.pth\")\n    pruned_size = calculate_model_size(pruned_student, \"pruned_student.pth\")\n    \n    # Print comparison\n    print(\"\\n--- Model Size Comparison ---\")\n    print(f\"Teacher Model: {teacher_params} parameters, {teacher_size:.2f} MB\")\n    print(f\"Student Model (Before Pruning): {student_params} parameters, {student_size:.2f} MB\")\n    print(f\"Student Model (After Pruning): {pruned_params} parameters, {pruned_size:.2f} MB\")\n    \n    # Calculate compression ratio\n    compression_ratio = student_size / pruned_size\n    print(f\"\\nCompression Ratio: {compression_ratio:.2f}x\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:04.858656Z","iopub.execute_input":"2025-03-16T18:53:04.858862Z","iopub.status.idle":"2025-03-16T18:53:04.876742Z","shell.execute_reply.started":"2025-03-16T18:53:04.858836Z","shell.execute_reply":"2025-03-16T18:53:04.876005Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs=10, lr=0.001, patience=3):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n    \n    best_val_accuracy = 0.0\n    best_model_state = None\n    patience_counter = 0  # Counter for early stopping\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n        \n        # Evaluate on the validation set\n        val_accuracy = evaluate(model, val_loader, device)\n        print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(train_loader):.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n        \n        # Early stopping logic\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            best_model_state = model.state_dict()\n            patience_counter = 0  # Reset patience counter\n            torch.save(model.state_dict(), 'best_teacher_model.pth')  # Save the best model\n            print(f\" New best model saved with validation accuracy: {best_val_accuracy:.2f}%\")\n        else:\n            patience_counter += 1\n            print(f\" No improvement in validation accuracy ({patience_counter}/{patience})\")\n            \n            # Stop training if no improvement for 'patience' epochs\n            if patience_counter >= patience:\n                print(f\"\\nEarly stopping triggered! No improvement for {patience} epochs.\")\n                break\n    \n    # Load the best model state\n    model.load_state_dict(torch.load('best_teacher_model.pth'))\n    print(\"\\nLoading the best model for final evaluation.\")\n    \n    # Evaluate on the test set\n    test_accuracy = evaluate(model, test_loader, device)\n    print(f\"Test Accuracy with Best Model: {test_accuracy:.2f}%\")\n    \n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:04.877493Z","iopub.execute_input":"2025-03-16T18:53:04.877713Z","iopub.status.idle":"2025-03-16T18:53:04.895380Z","shell.execute_reply.started":"2025-03-16T18:53:04.877685Z","shell.execute_reply":"2025-03-16T18:53:04.894442Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Fine-tune the teacher model\nteacher = train_model(teacher, train_loader, val_loader, epochs=200, lr=0.01, patience=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T13:15:08.085787Z","iopub.execute_input":"2025-03-14T13:15:08.086232Z","iopub.status.idle":"2025-03-14T13:24:52.477969Z","shell.execute_reply.started":"2025-03-14T13:15:08.086199Z","shell.execute_reply":"2025-03-14T13:24:52.476673Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200 | Train Loss: 3.2385 | Train Acc: 21.75% | Val Loss: 2.8543 | Val Acc: 29.15%\nNew best model saved with validation accuracy: 29.15%\nEpoch 2/200 | Train Loss: 2.8950 | Train Acc: 27.93% | Val Loss: 2.7926 | Val Acc: 31.80%\nNew best model saved with validation accuracy: 31.80%\nEpoch 3/200 | Train Loss: 2.5480 | Train Acc: 34.21% | Val Loss: 2.5264 | Val Acc: 37.16%\nNew best model saved with validation accuracy: 37.16%\nEpoch 4/200 | Train Loss: 2.3018 | Train Acc: 39.28% | Val Loss: 2.5428 | Val Acc: 38.85%\nNew best model saved with validation accuracy: 38.85%\nEpoch 5/200 | Train Loss: 2.1294 | Train Acc: 43.27% | Val Loss: 2.3666 | Val Acc: 39.74%\nNew best model saved with validation accuracy: 39.74%\nEpoch 6/200 | Train Loss: 1.9874 | Train Acc: 46.10% | Val Loss: 2.1242 | Val Acc: 44.01%\nNew best model saved with validation accuracy: 44.01%\nEpoch 7/200 | Train Loss: 1.9121 | Train Acc: 48.23% | Val Loss: 4.1372 | Val Acc: 22.39%\nNo improvement in validation accuracy (1/10)\nEpoch 8/200 | Train Loss: 2.0020 | Train Acc: 46.07% | Val Loss: 3.5337 | Val Acc: 32.35%\nNo improvement in validation accuracy (2/10)\nEpoch 9/200 | Train Loss: 1.8325 | Train Acc: 49.62% | Val Loss: 1.9399 | Val Acc: 48.15%\nNew best model saved with validation accuracy: 48.15%\nEpoch 10/200 | Train Loss: 1.5691 | Train Acc: 55.72% | Val Loss: 2.0226 | Val Acc: 46.57%\nNo improvement in validation accuracy (1/10)\nEpoch 11/200 | Train Loss: 1.5903 | Train Acc: 55.41% | Val Loss: 1.9249 | Val Acc: 48.47%\nNew best model saved with validation accuracy: 48.47%\nEpoch 12/200 | Train Loss: 1.4553 | Train Acc: 58.46% | Val Loss: 1.8122 | Val Acc: 52.64%\nNew best model saved with validation accuracy: 52.64%\nEpoch 13/200 | Train Loss: 1.3229 | Train Acc: 61.68% | Val Loss: 1.7735 | Val Acc: 52.86%\nNew best model saved with validation accuracy: 52.86%\nEpoch 14/200 | Train Loss: 1.2149 | Train Acc: 64.30% | Val Loss: 1.7337 | Val Acc: 53.69%\nNew best model saved with validation accuracy: 53.69%\nEpoch 15/200 | Train Loss: 1.1831 | Train Acc: 65.22% | Val Loss: 1.7910 | Val Acc: 52.72%\nNo improvement in validation accuracy (1/10)\nEpoch 16/200 | Train Loss: 1.2631 | Train Acc: 63.10% | Val Loss: 1.8141 | Val Acc: 52.57%\nNo improvement in validation accuracy (2/10)\nEpoch 17/200 | Train Loss: 1.1110 | Train Acc: 67.02% | Val Loss: 2.0025 | Val Acc: 49.84%\nNo improvement in validation accuracy (3/10)\nEpoch 18/200 | Train Loss: 1.0392 | Train Acc: 68.90% | Val Loss: 1.9190 | Val Acc: 55.11%\nNew best model saved with validation accuracy: 55.11%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-9cd8a900b4ea>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fine-tune the teacher model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mteacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-cc95bca95c26>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, lr, patience, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get predicted labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"\ndef compute_gradient_importance(teacher, student, data_loader, device, temperature=4.0, alpha=0.5):\n    importance_scores = {}\n    \n    # Initialize importance storage for all Conv2d weights\n    for name, param in student.named_parameters():\n        if 'weight' in name and isinstance(param, nn.Parameter) and len(param.shape) == 4:\n            importance_scores[name] = torch.zeros_like(param.data, device=device)\n    \n    teacher.to(device)\n    student.to(device)\n    \n    for inputs, labels in data_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        student.zero_grad()\n        \n        with torch.no_grad():\n            # Apply temperature scaling to teacher logits\n            teacher_logits = teacher(inputs)\n            teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n            \n        # Apply temperature scaling to student logits\n        student_logits = student(inputs)\n        student_probs = F.softmax(student_logits / temperature, dim=1)\n        \n        # Compute KL divergence loss (distillation loss)\n        kl_loss = F.kl_div(\n            F.log_softmax(student_logits / temperature, dim=1),\n            teacher_probs,\n            reduction='batchmean'\n        ) * (temperature ** 2)  # Scale by temperature squared\n        \n        # Compute log-likelihood loss (ground truth alignment)\n        log_likelihood_loss = F.cross_entropy(student_logits, labels)\n        \n        # Combine losses\n        loss = alpha * kl_loss + (1 - alpha) * log_likelihood_loss\n        loss.backward()\n        \n        # Accumulate absolute gradients for weights\n        for name, param in student.named_parameters():\n            if name in importance_scores and param.grad is not None:\n                importance_scores[name] += param.grad.abs().detach()\n    \n    # Average across batches\n    for name in importance_scores:\n        importance_scores[name] /= len(data_loader)\n        \n    return importance_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:09.409387Z","iopub.execute_input":"2025-03-16T18:53:09.409681Z","iopub.status.idle":"2025-03-16T18:53:09.416567Z","shell.execute_reply.started":"2025-03-16T18:53:09.409661Z","shell.execute_reply":"2025-03-16T18:53:09.415600Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def gradient_based_unstructured_prune(model, importance_scores, prune_ratio=0.3):\n    \"\"\"\n    True unstructured pruning using per-weight importance scores\n    \"\"\"\n    for name, param in model.named_parameters():\n        if name in importance_scores:\n            scores = importance_scores[name]\n            n_prune = int(prune_ratio * scores.numel())\n            \n            if n_prune > 0:\n                # Flatten and find threshold\n                flat_scores = scores.flatten()\n                threshold = torch.topk(flat_scores, k=n_prune, largest=False)[0][-1]\n                \n                # Create mask and apply pruning\n                mask = (scores > threshold).float()\n                param.data.mul_(mask)\n    \n    return model\n\ndef prune_model(model, importance_scores, prune_ratio=0.3, pruning_type='structured'):\n    \"\"\"\n    Prune the model using either structured or unstructured pruning.\n    \"\"\"\n    if pruning_type == 'structured':\n        return channel_prune(model, importance_scores, prune_ratio)\n    elif pruning_type == 'unstructured':\n        return gradient_based_unstructured_prune(model, importance_scores, prune_ratio)\n    else:\n        raise ValueError(\"Invalid pruning type. Choose 'structured' or 'unstructured'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:09.618397Z","iopub.execute_input":"2025-03-16T18:53:09.618702Z","iopub.status.idle":"2025-03-16T18:53:09.802679Z","shell.execute_reply.started":"2025-03-16T18:53:09.618680Z","shell.execute_reply":"2025-03-16T18:53:09.801829Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\ndef train_kd_pruning(teacher, student, train_loader, val_loader, epochs=50, pruning_type='unstructured', temperature=5.0, alpha=0.5, patience=5, save_path=\"student_before_pruning.pth\"):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n    \n    teacher = teacher.to(device)\n    student = student.to(device)\n    \n    best_val_acc = 0.0\n    best_model_state = None\n    patience_counter = 0\n    start_time = time.time()\n    \n    # Training loop\n    for epoch in range(epochs):\n        student.train()\n        total_loss = 0.0\n        correct, total = 0, 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            with torch.no_grad():\n                # Apply temperature scaling to teacher logits\n                teacher_logits = teacher(inputs)\n                teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n            \n            # Apply temperature scaling to student logits\n            student_logits = student(inputs)\n            student_probs = F.log_softmax(student_logits / temperature, dim=1)\n            \n            # Compute distillation loss (KL divergence)\n            distillation_loss = F.kl_div(student_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n            \n            # Compute ground truth loss (cross-entropy)\n            ground_truth_loss = F.cross_entropy(student_logits, labels)\n            \n            # Combine losses using alpha\n            loss = alpha * distillation_loss + (1 - alpha) * ground_truth_loss\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            _, predicted = student_logits.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total += labels.size(0)\n        \n        train_loss = total_loss / len(train_loader)\n        train_acc = 100.0 * correct / total\n        \n        # Validation\n        val_acc = evaluate(student, val_loader, device)\n        \n        # Print metrics\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n              f\"Val Acc: {val_acc:.2f}%\")\n        \n        # Early stopping\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_state = student.state_dict()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n                break\n    \n    # Load the best model state\n    student.load_state_dict(best_model_state)\n    \n    # Save the student model before pruning\n    torch.save(student.state_dict(), save_path)\n    print(f\"Student model saved before pruning at: {save_path}\")\n    # Print total training time\n    total_time = time.time() - start_time\n    print(f\"Total Training Time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n    return student","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:11.576676Z","iopub.execute_input":"2025-03-16T18:53:11.576997Z","iopub.status.idle":"2025-03-16T18:53:11.586262Z","shell.execute_reply.started":"2025-03-16T18:53:11.576972Z","shell.execute_reply":"2025-03-16T18:53:11.585369Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# # Train the student model using KD and pruning\nstudent = train_kd_pruning(\n    teacher, student, train_loader, val_loader,\n    epochs=50, pruning_type='unstructured', temperature=5.0, alpha=0.5, patience=5,save_path=\"student_before_pruning.pth\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T13:27:29.450281Z","iopub.execute_input":"2025-03-14T13:27:29.450590Z","iopub.status.idle":"2025-03-14T14:21:35.111118Z","shell.execute_reply.started":"2025-03-14T13:27:29.450566Z","shell.execute_reply":"2025-03-14T14:21:35.109970Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50 | Train Loss: 0.8813 | Train Acc: 91.04% | Val Acc: 94.23%\nEpoch 2/50 | Train Loss: 0.3009 | Train Acc: 97.47% | Val Acc: 96.16%\nEpoch 3/50 | Train Loss: 0.1924 | Train Acc: 99.03% | Val Acc: 96.61%\nEpoch 4/50 | Train Loss: 0.1450 | Train Acc: 99.44% | Val Acc: 96.83%\nEpoch 5/50 | Train Loss: 0.1202 | Train Acc: 99.53% | Val Acc: 96.97%\nEpoch 6/50 | Train Loss: 0.1049 | Train Acc: 99.55% | Val Acc: 96.84%\nEpoch 7/50 | Train Loss: 0.0930 | Train Acc: 99.60% | Val Acc: 96.94%\nEpoch 8/50 | Train Loss: 0.0849 | Train Acc: 99.57% | Val Acc: 96.92%\nEpoch 9/50 | Train Loss: 0.0792 | Train Acc: 99.61% | Val Acc: 97.08%\nEpoch 10/50 | Train Loss: 0.0735 | Train Acc: 99.62% | Val Acc: 97.05%\nEpoch 11/50 | Train Loss: 0.0698 | Train Acc: 99.64% | Val Acc: 97.00%\nEpoch 12/50 | Train Loss: 0.0663 | Train Acc: 99.64% | Val Acc: 97.06%\nEpoch 13/50 | Train Loss: 0.0637 | Train Acc: 99.64% | Val Acc: 97.07%\nEpoch 14/50 | Train Loss: 0.0624 | Train Acc: 99.61% | Val Acc: 96.97%\nEarly stopping triggered at epoch 14. No improvement for 5 epochs.\nStudent model saved before pruning at: student_before_pruning.pth\nTotal Training Time: 54m 6s\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Measure inference times\nteacher_inference_time = measure_inference_time(teacher, test_loader, device)\nstudent_inference_time = measure_inference_time(student, test_loader, device)\nprint(f\"Teacher Model Inference Time: {teacher_inference_time * 1000:.2f} ms per batch\")\nprint(f\"Student Model Inference Time(Before Pruning): {student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T14:21:35.112658Z","iopub.execute_input":"2025-03-14T14:21:35.112964Z","iopub.status.idle":"2025-03-14T14:24:10.512286Z","shell.execute_reply.started":"2025-03-14T14:21:35.112938Z","shell.execute_reply":"2025-03-14T14:24:10.511429Z"}},"outputs":[{"name":"stdout","text":"Teacher Model Inference Time: 7.43 ms per batch\nStudent Model Inference Time(Before Pruning): 6.04 ms per batch\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Calculate sparsity\nsparsity = calculate_sparsity(student)\nprint(f\"Sparsity Before Pruning: {sparsity * 100:.2f}%\")\n\nteacher_accuracy = evaluate(teacher, test_loader, device)\nstudent_accuracy = evaluate(student, test_loader, device)\nprint(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\nprint(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T14:24:10.514045Z","iopub.execute_input":"2025-03-14T14:24:10.514304Z","iopub.status.idle":"2025-03-14T14:25:01.847524Z","shell.execute_reply.started":"2025-03-14T14:24:10.514282Z","shell.execute_reply":"2025-03-14T14:25:01.846615Z"}},"outputs":[{"name":"stdout","text":"Sparsity Before Pruning: 0.00%\nTeacher Model Test Accuracy: 95.31%\nStudent Model Test Accuracy Before Pruning: 96.55%\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Pruning\nprint(\"Calculating Important Scores\")\nstart_time = time.time()\n\nimportance_scores = compute_gradient_importance(\n    teacher, student, train_loader, device, temperature=5.0, alpha=0.5\n)\n\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n\nprint(\"Pruning the model\")\nstart_time = time.time()\n\npruned_student = prune_model(student, importance_scores, prune_ratio=0.9,pruning_type='unstructured')\n\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\nstudent = student.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T14:25:01.848471Z","iopub.execute_input":"2025-03-14T14:25:01.848689Z","iopub.status.idle":"2025-03-14T14:27:32.745652Z","shell.execute_reply.started":"2025-03-14T14:25:01.848670Z","shell.execute_reply":"2025-03-14T14:27:32.744742Z"}},"outputs":[{"name":"stdout","text":"Calculating Important Scores\nTotal Time take to calculate Important scores: 2m 31s\nPruning the model\nTotal Time take to prune the model scores: 0m 0s\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"student_accuracy = evaluate(pruned_student, test_loader, device)\nprint(f\"Student Model Test Accuracy After Pruning: {student_accuracy:.2f}%\")\nsparsity = calculate_sparsity(pruned_student)\nprint(f\"Sparsity After Pruning: {sparsity * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T14:27:32.746650Z","iopub.execute_input":"2025-03-14T14:27:32.746968Z","iopub.status.idle":"2025-03-14T14:27:54.097727Z","shell.execute_reply.started":"2025-03-14T14:27:32.746931Z","shell.execute_reply":"2025-03-14T14:27:54.096914Z"}},"outputs":[{"name":"stdout","text":"Student Model Test Accuracy After Pruning: 10.00%\nSparsity After Pruning: 89.94%\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Measure inference times\npruned_student_inference_time = measure_inference_time(pruned_student, test_loader, device)\nprint(f\"Student Model Inference Time(After Pruning): {pruned_student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T14:28:00.990862Z","iopub.execute_input":"2025-03-14T14:28:00.991188Z","iopub.status.idle":"2025-03-14T14:29:17.401815Z","shell.execute_reply.started":"2025-03-14T14:28:00.991162Z","shell.execute_reply":"2025-03-14T14:29:17.400920Z"}},"outputs":[{"name":"stdout","text":"Student Model Inference Time(After Pruning): 5.97 ms per batch\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"torch.save(pruned_student.state_dict(), \"pruned_student_unstructured_90%_res34.pth\")\nprint(\"Model saved as pruned_student_unstructured.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T14:29:17.403007Z","iopub.execute_input":"2025-03-14T14:29:17.403313Z","iopub.status.idle":"2025-03-14T14:29:17.529004Z","shell.execute_reply.started":"2025-03-14T14:29:17.403283Z","shell.execute_reply":"2025-03-14T14:29:17.528115Z"}},"outputs":[{"name":"stdout","text":"Model saved as pruned_student_unstructured.pth\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\ndef retrain_with_sparsity(student, train_loader, val_loader, epochs=5, save_path=\"retrained_student_model.pt\", patience=3):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n\n    # 1. Store masks AND zero momentum buffers for pruned weights\n    masks = {}\n    for name, param in student.named_parameters():\n        if 'weight' in name and param.dim() == 4:  # Consider only conv layers\n            mask = (param != 0).float().to(device)\n            masks[name] = mask\n            # Zero momentum buffers for pruned weights\n            if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n                optimizer.state[param]['momentum_buffer'] *= mask\n\n    student = student.to(device)\n    best_val_acc = 0.0\n    best_model = None\n    patience_counter = 0  # Counter for early stopping\n\n    # 2. Add gradient clipping to prevent NaN\n    max_grad_norm = 1.0\n\n    for epoch in range(epochs):\n        student.train()\n        total_loss = 0.0\n        correct, total = 0, 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = student(inputs)\n            loss = F.cross_entropy(outputs, labels)\n            loss.backward()\n\n            # Apply masks to gradients\n            for name, param in student.named_parameters():\n                if name in masks:\n                    param.grad.data *= masks[name]\n\n            # Gradient clipping before optimizer step\n            torch.nn.utils.clip_grad_norm_(student.parameters(), max_grad_norm)\n\n            optimizer.step()\n\n            # Reapply masks and update momentum buffers\n            for name, param in student.named_parameters():\n                if name in masks:\n                    param.data *= masks[name]\n                    if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n                        optimizer.state[param]['momentum_buffer'] *= masks[name]\n\n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total += labels.size(0)\n\n        train_loss = total_loss / len(train_loader)\n        train_acc = 100.0 * correct / total\n\n        # Validation phase\n        student.eval()\n        val_loss, val_correct, val_total = 0.0, 0, 0\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = student(inputs)\n                loss = F.cross_entropy(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_correct += predicted.eq(labels).sum().item()\n                val_total += labels.size(0)\n\n        val_loss /= len(val_loader)\n        val_acc = 100.0 * val_correct / val_total\n\n        # Track best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model = student.state_dict()\n            torch.save(best_model, save_path)\n            patience_counter = 0  # Reset patience counter\n            print(f\"New best model saved with Val Accuracy: {best_val_acc:.2f}%\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n                break  # Stop training\n\n        # Print results\n        sparsity = calculate_sparsity(student)\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n        print(f\"Validation Loss: {val_loss:.4f} | Validation Acc: {val_acc:.2f}% | Sparsity: {sparsity*100:.2f}%\\n\")\n\n    print(f\"Best Validation Accuracy: {best_val_acc:.2f}% | Best Model Saved at: {save_path}\")\n    return student","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:15.812669Z","iopub.execute_input":"2025-03-16T18:53:15.812969Z","iopub.status.idle":"2025-03-16T18:53:15.825131Z","shell.execute_reply.started":"2025-03-16T18:53:15.812948Z","shell.execute_reply":"2025-03-16T18:53:15.824266Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport time\n\ndef retrain_with_KD(teacher, student, train_loader, val_loader, epochs=50, temperature=5.0, alpha=0.5, patience=5, save_path=\"student_before_pruning.pth\"):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n     # 1. Store masks AND zero momentum buffers for pruned weights\n    masks = {}\n    for name, param in student.named_parameters():\n        if 'weight' in name and param.dim() == 4:  # Consider only conv layers\n            mask = (param != 0).float().to(device)\n            masks[name] = mask\n            # Zero momentum buffers for pruned weights\n            if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n                optimizer.state[param]['momentum_buffer'] *= mask\n    \n    teacher = teacher.to(device)\n    student = student.to(device)\n    \n    best_val_acc = 0.0\n    best_val_loss = float(\"inf\")\n    best_model_state = None\n    patience_counter = 0\n    start_time = time.time()\n    \n    # Training loop\n    for epoch in range(epochs):\n        student.train()\n        total_loss, correct, total = 0.0, 0, 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            with torch.no_grad():\n                # Apply temperature scaling to teacher logits\n                teacher_logits = teacher(inputs)\n                teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n            \n            # Apply temperature scaling to student logits\n            student_logits = student(inputs)\n            student_probs = F.log_softmax(student_logits / temperature, dim=1)\n            \n            # Compute distillation loss (KL divergence)\n            distillation_loss = F.kl_div(student_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n            \n            # Compute ground truth loss (cross-entropy)\n            ground_truth_loss = F.cross_entropy(student_logits, labels)\n            \n            # Combine losses using alpha\n            loss = alpha * distillation_loss + (1 - alpha) * ground_truth_loss\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # Reapply masks and update momentum buffers\n            for name, param in student.named_parameters():\n                if name in masks:\n                    param.data *= masks[name]\n                    if optimizer.state.get(param, None) and 'momentum_buffer' in optimizer.state[param]:\n                        optimizer.state[param]['momentum_buffer'] *= masks[name]\n            \n            total_loss += loss.item()\n            _, predicted = student_logits.max(1)\n            correct += predicted.eq(labels).sum().item()\n            total += labels.size(0)\n        \n        train_loss = total_loss / len(train_loader)\n        train_acc = 100.0 * correct / total\n        \n        # Validation\n        student.eval()\n        val_loss, val_correct, val_total = 0.0, 0, 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = student(inputs)\n                loss = F.cross_entropy(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_correct += predicted.eq(labels).sum().item()\n                val_total += labels.size(0)\n        \n        val_loss /= len(val_loader)\n        val_acc = 100.0 * val_correct / val_total\n        sparsity = calculate_sparsity(student)*100.0  # Function assumed to be defined\n        \n        # Print metrics\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Sparsity: {sparsity:.2f}%\")\n        \n        # Early stopping\n        if val_acc > best_val_acc or val_loss < best_val_loss:\n            best_val_acc = val_acc\n            best_val_loss = val_loss\n            best_model_state = student.state_dict()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered at epoch {epoch+1}. No improvement for {patience} epochs.\")\n                break\n    \n    # Load the best model state\n    student.load_state_dict(best_model_state)\n    \n    # Save the student model before pruning\n    torch.save(student.state_dict(), save_path)\n    print(f\"Student model saved before pruning at: {save_path}\")\n    \n    # Print total training time\n    total_time = time.time() - start_time\n    print(f\"Total Training Time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n    \n    return student\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:19.636417Z","iopub.execute_input":"2025-03-16T18:53:19.636739Z","iopub.status.idle":"2025-03-16T18:53:19.651248Z","shell.execute_reply.started":"2025-03-16T18:53:19.636715Z","shell.execute_reply":"2025-03-16T18:53:19.650072Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# 90% of Sparsity","metadata":{}},{"cell_type":"code","source":"student = retrain_with_sparsity(\n    pruned_student, train_loader, val_loader,\n    epochs=200,  save_path='/kaggle/working/retrained_student_model.pt',patience=10\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T14:29:28.266780Z","iopub.execute_input":"2025-03-14T14:29:28.267126Z","iopub.status.idle":"2025-03-14T16:01:49.716872Z","shell.execute_reply.started":"2025-03-14T14:29:28.267100Z","shell.execute_reply":"2025-03-14T16:01:49.716015Z"}},"outputs":[{"name":"stdout","text":"New best model saved with Val Accuracy: 35.90%\nEpoch 1/200 | Train Loss: 1.9981 | Train Acc: 24.29%\nValidation Loss: 1.6694 | Validation Acc: 35.90% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 43.64%\nEpoch 2/200 | Train Loss: 1.5134 | Train Acc: 42.51%\nValidation Loss: 1.6567 | Validation Acc: 43.64% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 52.91%\nEpoch 3/200 | Train Loss: 1.1877 | Train Acc: 56.86%\nValidation Loss: 1.2756 | Validation Acc: 52.91% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 64.16%\nEpoch 4/200 | Train Loss: 0.9905 | Train Acc: 64.30%\nValidation Loss: 1.0049 | Validation Acc: 64.16% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 69.85%\nEpoch 5/200 | Train Loss: 0.8679 | Train Acc: 68.94%\nValidation Loss: 0.8399 | Validation Acc: 69.85% | Sparsity: 89.94%\n\nEpoch 6/200 | Train Loss: 0.7788 | Train Acc: 72.17%\nValidation Loss: 0.8398 | Validation Acc: 69.71% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 72.64%\nEpoch 7/200 | Train Loss: 0.6919 | Train Acc: 75.53%\nValidation Loss: 0.7803 | Validation Acc: 72.64% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 73.68%\nEpoch 8/200 | Train Loss: 0.6242 | Train Acc: 78.12%\nValidation Loss: 0.7499 | Validation Acc: 73.68% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 76.23%\nEpoch 9/200 | Train Loss: 0.5608 | Train Acc: 80.42%\nValidation Loss: 0.6959 | Validation Acc: 76.23% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 77.14%\nEpoch 10/200 | Train Loss: 0.5066 | Train Acc: 82.27%\nValidation Loss: 0.6814 | Validation Acc: 77.14% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 78.18%\nEpoch 11/200 | Train Loss: 0.4624 | Train Acc: 84.03%\nValidation Loss: 0.6634 | Validation Acc: 78.18% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 79.18%\nEpoch 12/200 | Train Loss: 0.4192 | Train Acc: 85.30%\nValidation Loss: 0.6306 | Validation Acc: 79.18% | Sparsity: 89.94%\n\nEpoch 13/200 | Train Loss: 0.3736 | Train Acc: 86.98%\nValidation Loss: 0.8493 | Validation Acc: 73.87% | Sparsity: 89.94%\n\nEpoch 14/200 | Train Loss: 0.3408 | Train Acc: 88.15%\nValidation Loss: 0.7707 | Validation Acc: 76.87% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 79.63%\nEpoch 15/200 | Train Loss: 0.2971 | Train Acc: 89.55%\nValidation Loss: 0.6550 | Validation Acc: 79.63% | Sparsity: 89.94%\n\nEpoch 16/200 | Train Loss: 0.2659 | Train Acc: 90.83%\nValidation Loss: 0.8434 | Validation Acc: 75.39% | Sparsity: 89.94%\n\nEpoch 17/200 | Train Loss: 0.2303 | Train Acc: 91.98%\nValidation Loss: 0.7504 | Validation Acc: 77.83% | Sparsity: 89.94%\n\nEpoch 18/200 | Train Loss: 0.2064 | Train Acc: 92.84%\nValidation Loss: 0.7417 | Validation Acc: 78.83% | Sparsity: 89.94%\n\nEpoch 19/200 | Train Loss: 0.1796 | Train Acc: 93.70%\nValidation Loss: 0.7403 | Validation Acc: 78.85% | Sparsity: 89.94%\n\nEpoch 20/200 | Train Loss: 0.1574 | Train Acc: 94.55%\nValidation Loss: 0.7749 | Validation Acc: 79.53% | Sparsity: 89.94%\n\nEpoch 21/200 | Train Loss: 0.1403 | Train Acc: 95.16%\nValidation Loss: 0.8763 | Validation Acc: 77.50% | Sparsity: 89.94%\n\nNew best model saved with Val Accuracy: 80.42%\nEpoch 22/200 | Train Loss: 0.1253 | Train Acc: 95.82%\nValidation Loss: 0.7595 | Validation Acc: 80.42% | Sparsity: 89.94%\n\nEpoch 23/200 | Train Loss: 0.1150 | Train Acc: 96.01%\nValidation Loss: 0.8028 | Validation Acc: 79.68% | Sparsity: 89.94%\n\nEpoch 24/200 | Train Loss: 0.1072 | Train Acc: 96.32%\nValidation Loss: 0.9162 | Validation Acc: 77.79% | Sparsity: 89.94%\n\nEpoch 25/200 | Train Loss: 0.0957 | Train Acc: 96.75%\nValidation Loss: 0.8475 | Validation Acc: 80.37% | Sparsity: 89.94%\n\nEpoch 26/200 | Train Loss: 0.0869 | Train Acc: 96.99%\nValidation Loss: 0.8037 | Validation Acc: 80.19% | Sparsity: 89.94%\n\nEpoch 27/200 | Train Loss: 0.0790 | Train Acc: 97.35%\nValidation Loss: 0.8280 | Validation Acc: 80.10% | Sparsity: 89.94%\n\nEpoch 28/200 | Train Loss: 0.0716 | Train Acc: 97.61%\nValidation Loss: 0.9285 | Validation Acc: 79.99% | Sparsity: 89.94%\n\nEpoch 29/200 | Train Loss: 0.0691 | Train Acc: 97.62%\nValidation Loss: 0.9001 | Validation Acc: 79.65% | Sparsity: 89.94%\n\nEpoch 30/200 | Train Loss: 0.0663 | Train Acc: 97.72%\nValidation Loss: 0.9037 | Validation Acc: 80.25% | Sparsity: 89.94%\n\nEpoch 31/200 | Train Loss: 0.0633 | Train Acc: 97.87%\nValidation Loss: 0.9368 | Validation Acc: 80.13% | Sparsity: 89.94%\n\nEarly stopping triggered at epoch 32. No improvement for 10 epochs.\nBest Validation Accuracy: 80.42% | Best Model Saved at: /kaggle/working/retrained_student_model.pt\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"student_accuracy = evaluate(student, test_loader, device)\nprint(f\"Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T23:31:52.929615Z","iopub.execute_input":"2025-03-14T23:31:52.929875Z","iopub.status.idle":"2025-03-14T23:32:14.464739Z","shell.execute_reply.started":"2025-03-14T23:31:52.929850Z","shell.execute_reply":"2025-03-14T23:32:14.463856Z"}},"outputs":[{"name":"stdout","text":"Pruned Student Model Test Accuracy: 92.71%\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"sparsity = calculate_sparsity(student)\nprint(f\"Sparsity: {sparsity * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T23:32:14.465949Z","iopub.execute_input":"2025-03-14T23:32:14.466230Z","iopub.status.idle":"2025-03-14T23:32:14.476729Z","shell.execute_reply.started":"2025-03-14T23:32:14.466207Z","shell.execute_reply":"2025-03-14T23:32:14.476092Z"}},"outputs":[{"name":"stdout","text":"Sparsity: 80.95%\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"\npruned_student_inference_time = measure_inference_time(student, test_loader, device)\nprint(f\"Pruned Student Model Inference Time: {pruned_student_inference_time * 1000:.2f} ms per batch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T23:32:14.477592Z","iopub.execute_input":"2025-03-14T23:32:14.477894Z","iopub.status.idle":"2025-03-14T23:33:31.015401Z","shell.execute_reply.started":"2025-03-14T23:32:14.477867Z","shell.execute_reply":"2025-03-14T23:33:31.014593Z"}},"outputs":[{"name":"stdout","text":"Pruned Student Model Inference Time: 6.02 ms per batch\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"# Retrain with KD","metadata":{}},{"cell_type":"code","source":"# Load pretrained ResNet-18 (Student Model)\nstudent = models.resnet34(pretrained=True)\n# Modify the final fully connected layer for 10 classes (CIFAR-10)\nstudent.fc = nn.Linear(student.fc.in_features, 10)\nstudent = student.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:11:10.385747Z","iopub.execute_input":"2025-03-16T13:11:10.386058Z","iopub.status.idle":"2025-03-16T13:11:10.766682Z","shell.execute_reply.started":"2025-03-16T13:11:10.386027Z","shell.execute_reply":"2025-03-16T13:11:10.765175Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"\nmodel_path = '/kaggle/input/student_resnet_34/pytorch/default/1/student_before_pruning_Res34.pth'\n# Load the model weights\nstudent.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:13:55.987690Z","iopub.execute_input":"2025-03-16T13:13:55.987999Z","iopub.status.idle":"2025-03-16T13:13:56.067560Z","shell.execute_reply.started":"2025-03-16T13:13:55.987970Z","shell.execute_reply":"2025-03-16T13:13:56.066722Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-39-5d6657d12f2d>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"\n# model_path = '/kaggle/working/pruned_student_unstructured_90%_res34.pth'\n# # Load the model weights\n# student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:05:32.403182Z","iopub.execute_input":"2025-03-14T17:05:32.403470Z","iopub.status.idle":"2025-03-14T17:05:32.473876Z","shell.execute_reply.started":"2025-03-14T17:05:32.403451Z","shell.execute_reply":"2025-03-14T17:05:32.472927Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-58-0660dd01e2d9>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"# Pruning\nprint(\"Calculating Important Scores\")\nstart_time = time.time()\nimportance_scores = compute_gradient_importance(\n    teacher, student, train_loader, device, temperature=5.0, alpha=0.5\n)\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n\nprint(\"Pruning the model\")\nstart_time = time.time()\n\npruned_student = prune_model(student, importance_scores, prune_ratio=0.9,pruning_type='unstructured')\n\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\nstudent = student.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:14:02.655201Z","iopub.execute_input":"2025-03-16T13:14:02.655593Z","iopub.status.idle":"2025-03-16T13:16:36.000315Z","shell.execute_reply.started":"2025-03-16T13:14:02.655565Z","shell.execute_reply":"2025-03-16T13:16:35.999593Z"}},"outputs":[{"name":"stdout","text":"Calculating Important Scores\nTotal Time take to calculate Important scores: 2m 33s\nPruning the model\nTotal Time take to prune the model scores: 0m 0s\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"pruned_student = retrain_with_KD(\n    teacher, pruned_student, train_loader, val_loader,\n    epochs=50, temperature=5.0, alpha=0.5, patience=5,save_path=\"pruned_student_retrain_KD_90%.pth\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:16:36.001246Z","iopub.execute_input":"2025-03-16T13:16:36.001530Z","iopub.status.idle":"2025-03-16T16:26:04.504186Z","shell.execute_reply.started":"2025-03-16T13:16:36.001507Z","shell.execute_reply":"2025-03-16T16:26:04.503252Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50 | Train Loss: 7.5166 | Train Acc: 18.06% | Val Loss: 1.8806 | Val Acc: 30.67% | Sparsity: 89.94%\nEpoch 2/50 | Train Loss: 5.1925 | Train Acc: 41.24% | Val Loss: 1.8175 | Val Acc: 48.11% | Sparsity: 89.94%\nEpoch 3/50 | Train Loss: 3.5202 | Train Acc: 62.22% | Val Loss: 1.2330 | Val Acc: 65.08% | Sparsity: 89.94%\nEpoch 4/50 | Train Loss: 2.6214 | Train Acc: 72.11% | Val Loss: 1.1596 | Val Acc: 71.92% | Sparsity: 89.94%\nEpoch 5/50 | Train Loss: 2.0768 | Train Acc: 78.52% | Val Loss: 0.9063 | Val Acc: 75.79% | Sparsity: 89.94%\nEpoch 6/50 | Train Loss: 1.7329 | Train Acc: 82.24% | Val Loss: 0.9131 | Val Acc: 77.25% | Sparsity: 89.94%\nEpoch 7/50 | Train Loss: 1.5006 | Train Acc: 84.49% | Val Loss: 0.8240 | Val Acc: 79.17% | Sparsity: 89.94%\nEpoch 8/50 | Train Loss: 1.3070 | Train Acc: 86.61% | Val Loss: 0.6317 | Val Acc: 83.51% | Sparsity: 89.94%\nEpoch 9/50 | Train Loss: 1.1481 | Train Acc: 88.51% | Val Loss: 0.8030 | Val Acc: 80.09% | Sparsity: 89.94%\nEpoch 10/50 | Train Loss: 1.0033 | Train Acc: 90.24% | Val Loss: 0.7553 | Val Acc: 82.14% | Sparsity: 89.94%\nEpoch 11/50 | Train Loss: 0.8786 | Train Acc: 91.83% | Val Loss: 0.8654 | Val Acc: 80.76% | Sparsity: 89.94%\nEpoch 12/50 | Train Loss: 0.7830 | Train Acc: 92.94% | Val Loss: 0.6443 | Val Acc: 84.63% | Sparsity: 89.94%\nEpoch 13/50 | Train Loss: 0.7118 | Train Acc: 93.87% | Val Loss: 0.7208 | Val Acc: 83.67% | Sparsity: 89.94%\nEpoch 14/50 | Train Loss: 0.6207 | Train Acc: 95.01% | Val Loss: 0.6488 | Val Acc: 84.66% | Sparsity: 89.94%\nEpoch 15/50 | Train Loss: 0.5521 | Train Acc: 96.09% | Val Loss: 0.6073 | Val Acc: 86.02% | Sparsity: 89.94%\nEpoch 16/50 | Train Loss: 0.5017 | Train Acc: 96.74% | Val Loss: 0.7520 | Val Acc: 83.39% | Sparsity: 89.94%\nEpoch 17/50 | Train Loss: 0.4494 | Train Acc: 97.39% | Val Loss: 0.5855 | Val Acc: 86.02% | Sparsity: 89.94%\nEpoch 18/50 | Train Loss: 0.4155 | Train Acc: 97.84% | Val Loss: 0.5390 | Val Acc: 86.91% | Sparsity: 89.94%\nEpoch 19/50 | Train Loss: 0.3721 | Train Acc: 98.43% | Val Loss: 0.5205 | Val Acc: 87.68% | Sparsity: 89.94%\nEpoch 20/50 | Train Loss: 0.3428 | Train Acc: 98.65% | Val Loss: 0.5103 | Val Acc: 87.54% | Sparsity: 89.94%\nEpoch 21/50 | Train Loss: 0.3146 | Train Acc: 98.91% | Val Loss: 0.5404 | Val Acc: 87.30% | Sparsity: 89.94%\nEpoch 22/50 | Train Loss: 0.2934 | Train Acc: 99.14% | Val Loss: 0.4984 | Val Acc: 87.87% | Sparsity: 89.94%\nEpoch 23/50 | Train Loss: 0.2864 | Train Acc: 99.11% | Val Loss: 0.5321 | Val Acc: 87.33% | Sparsity: 89.94%\nEpoch 24/50 | Train Loss: 0.2702 | Train Acc: 99.20% | Val Loss: 0.4823 | Val Acc: 88.14% | Sparsity: 89.94%\nEpoch 25/50 | Train Loss: 0.2606 | Train Acc: 99.24% | Val Loss: 0.4790 | Val Acc: 88.00% | Sparsity: 89.94%\nEpoch 26/50 | Train Loss: 0.2496 | Train Acc: 99.31% | Val Loss: 0.4610 | Val Acc: 88.39% | Sparsity: 89.94%\nEpoch 27/50 | Train Loss: 0.2409 | Train Acc: 99.32% | Val Loss: 0.4804 | Val Acc: 87.88% | Sparsity: 89.94%\nEpoch 28/50 | Train Loss: 0.2299 | Train Acc: 99.34% | Val Loss: 0.4802 | Val Acc: 88.04% | Sparsity: 89.94%\nEpoch 29/50 | Train Loss: 0.2193 | Train Acc: 99.40% | Val Loss: 0.4593 | Val Acc: 88.55% | Sparsity: 89.94%\nEpoch 30/50 | Train Loss: 0.2199 | Train Acc: 99.33% | Val Loss: 0.4541 | Val Acc: 88.59% | Sparsity: 89.94%\nEpoch 31/50 | Train Loss: 0.2128 | Train Acc: 99.39% | Val Loss: 0.4712 | Val Acc: 88.17% | Sparsity: 89.94%\nEpoch 32/50 | Train Loss: 0.2042 | Train Acc: 99.42% | Val Loss: 0.4513 | Val Acc: 88.55% | Sparsity: 89.94%\nEpoch 33/50 | Train Loss: 0.1979 | Train Acc: 99.39% | Val Loss: 0.4419 | Val Acc: 88.36% | Sparsity: 89.94%\nEpoch 34/50 | Train Loss: 0.1915 | Train Acc: 99.46% | Val Loss: 0.4496 | Val Acc: 88.67% | Sparsity: 89.94%\nEpoch 35/50 | Train Loss: 0.1869 | Train Acc: 99.43% | Val Loss: 0.4474 | Val Acc: 88.62% | Sparsity: 89.94%\nEpoch 36/50 | Train Loss: 0.1773 | Train Acc: 99.52% | Val Loss: 0.4466 | Val Acc: 88.73% | Sparsity: 89.94%\nEpoch 37/50 | Train Loss: 0.1748 | Train Acc: 99.50% | Val Loss: 0.4712 | Val Acc: 88.39% | Sparsity: 89.94%\nEpoch 38/50 | Train Loss: 0.1717 | Train Acc: 99.45% | Val Loss: 0.4803 | Val Acc: 88.05% | Sparsity: 89.94%\nEpoch 39/50 | Train Loss: 0.1669 | Train Acc: 99.48% | Val Loss: 0.4325 | Val Acc: 88.62% | Sparsity: 89.94%\nEpoch 40/50 | Train Loss: 0.1644 | Train Acc: 99.50% | Val Loss: 0.4592 | Val Acc: 88.46% | Sparsity: 89.94%\nEpoch 41/50 | Train Loss: 0.1592 | Train Acc: 99.51% | Val Loss: 0.4451 | Val Acc: 88.69% | Sparsity: 89.94%\nEpoch 42/50 | Train Loss: 0.1557 | Train Acc: 99.53% | Val Loss: 0.4334 | Val Acc: 88.78% | Sparsity: 89.94%\nEpoch 43/50 | Train Loss: 0.1486 | Train Acc: 99.56% | Val Loss: 0.4276 | Val Acc: 88.87% | Sparsity: 89.94%\nEpoch 44/50 | Train Loss: 0.1481 | Train Acc: 99.49% | Val Loss: 0.4345 | Val Acc: 88.83% | Sparsity: 89.94%\nEpoch 45/50 | Train Loss: 0.1437 | Train Acc: 99.60% | Val Loss: 0.4329 | Val Acc: 88.94% | Sparsity: 89.94%\nEpoch 46/50 | Train Loss: 0.1423 | Train Acc: 99.57% | Val Loss: 0.4408 | Val Acc: 88.67% | Sparsity: 89.94%\nEpoch 47/50 | Train Loss: 0.1387 | Train Acc: 99.59% | Val Loss: 0.4392 | Val Acc: 88.85% | Sparsity: 89.94%\nEpoch 48/50 | Train Loss: 0.1327 | Train Acc: 99.60% | Val Loss: 0.4281 | Val Acc: 88.79% | Sparsity: 89.94%\nEpoch 49/50 | Train Loss: 0.1302 | Train Acc: 99.59% | Val Loss: 0.4260 | Val Acc: 88.72% | Sparsity: 89.94%\nEpoch 50/50 | Train Loss: 0.1318 | Train Acc: 99.53% | Val Loss: 0.4310 | Val Acc: 88.77% | Sparsity: 89.94%\nStudent model saved before pruning at: pruned_student_retrain_KD_90%.pth\nTotal Training Time: 189m 28s\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"student_accuracy = evaluate(pruned_student, test_loader, device)\nprint(f\"Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T16:26:04.505727Z","iopub.execute_input":"2025-03-16T16:26:04.506016Z","iopub.status.idle":"2025-03-16T16:26:25.793426Z","shell.execute_reply.started":"2025-03-16T16:26:04.505994Z","shell.execute_reply":"2025-03-16T16:26:25.792497Z"}},"outputs":[{"name":"stdout","text":"Pruned Student Model Test Accuracy: 88.55%\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"sparsity = calculate_sparsity(student)\nprint(f\"Sparsity: {sparsity * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T17:05:48.321395Z","iopub.execute_input":"2025-03-14T17:05:48.321709Z","iopub.status.idle":"2025-03-14T17:05:48.334089Z","shell.execute_reply.started":"2025-03-14T17:05:48.321686Z","shell.execute_reply":"2025-03-14T17:05:48.333316Z"}},"outputs":[{"name":"stdout","text":"Sparsity: 89.94%\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:45:46.757856Z","iopub.execute_input":"2025-03-16T06:45:46.758171Z","iopub.status.idle":"2025-03-16T06:45:46.770488Z","shell.execute_reply.started":"2025-03-16T06:45:46.758149Z","shell.execute_reply":"2025-03-16T06:45:46.769692Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# 80% of Sparsity","metadata":{}},{"cell_type":"code","source":"# Load pretrained ResNet-18 (Student Model)\nstudent = models.resnet34(pretrained=True)\n# Modify the final fully connected layer for 10 classes (CIFAR-10)\nstudent.fc = nn.Linear(student.fc.in_features, 10)\nstudent = student.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:06:42.712857Z","iopub.execute_input":"2025-03-16T10:06:42.713083Z","iopub.status.idle":"2025-03-16T10:06:43.082738Z","shell.execute_reply.started":"2025-03-16T10:06:42.713063Z","shell.execute_reply":"2025-03-16T10:06:43.082064Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"\nmodel_path = '/kaggle/input/student_resnet_34/pytorch/default/1/student_before_pruning_Res34.pth'\n# Load the model weights\nstudent.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:06:43.083530Z","iopub.execute_input":"2025-03-16T10:06:43.083786Z","iopub.status.idle":"2025-03-16T10:06:43.168991Z","shell.execute_reply.started":"2025-03-16T10:06:43.083765Z","shell.execute_reply":"2025-03-16T10:06:43.168086Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-32-5d6657d12f2d>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# Measure inference times\nteacher_inference_time = measure_inference_time(teacher, test_loader, device)\nstudent_inference_time = measure_inference_time(student, test_loader, device)\nprint(f\"Teacher Model Inference Time: {teacher_inference_time * 1000:.2f} ms per batch\")\nprint(f\"Student Model Inference Time(Before Pruning): {student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T12:32:14.127905Z","iopub.execute_input":"2025-03-01T12:32:14.128216Z","iopub.status.idle":"2025-03-01T12:35:08.372987Z","shell.execute_reply.started":"2025-03-01T12:32:14.128192Z","shell.execute_reply":"2025-03-01T12:35:08.372099Z"}},"outputs":[{"name":"stdout","text":"Teacher Model Inference Time: 0.06 ms per batch\nStudent Model Inference Time(Before Pruning): 0.03 ms per batch\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Calculate sparsity\nsparsity = calculate_sparsity(student)\nprint(f\"Sparsity Before Pruning: {sparsity * 100:.2f}%\")\n\nteacher_accuracy = evaluate(teacher, test_loader, device)\nstudent_accuracy = evaluate(student, test_loader, device)\nprint(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\nprint(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:54:08.230499Z","iopub.execute_input":"2025-03-14T18:54:08.230804Z","iopub.status.idle":"2025-03-14T18:55:01.518212Z","shell.execute_reply.started":"2025-03-14T18:54:08.230781Z","shell.execute_reply":"2025-03-14T18:55:01.517446Z"}},"outputs":[{"name":"stdout","text":"Sparsity Before Pruning: 0.00%\nTeacher Model Test Accuracy: 95.31%\nStudent Model Test Accuracy Before Pruning: 96.55%\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# Pruning\nprint(\"Calculating Important Scores\")\nstart_time = time.time()\nimportance_scores = compute_gradient_importance(\n    teacher, student, train_loader, device, temperature=5.0, alpha=0.5\n)\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n\nprint(\"Pruning the model\")\nstart_time = time.time()\npruned_student = prune_model(student, importance_scores, prune_ratio=0.81,pruning_type='unstructured')\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\nstudent = student.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:06:43.169852Z","iopub.execute_input":"2025-03-16T10:06:43.170107Z","iopub.status.idle":"2025-03-16T10:09:16.358590Z","shell.execute_reply.started":"2025-03-16T10:06:43.170074Z","shell.execute_reply":"2025-03-16T10:09:16.357962Z"}},"outputs":[{"name":"stdout","text":"Calculating Important Scores\nTotal Time take to calculate Important scores: 2m 33s\nPruning the model\nTotal Time take to prune the model scores: 0m 0s\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"student_accuracy = evaluate(pruned_student, test_loader, device)\nprint(f\"Student Model Test Accuracy After Pruning: {student_accuracy:.2f}%\")\nsparsity = calculate_sparsity(pruned_student)\nprint(f\"Sparsity After Pruning: {sparsity * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T07:28:54.304269Z","iopub.execute_input":"2025-03-15T07:28:54.304626Z","iopub.status.idle":"2025-03-15T07:29:18.191771Z","shell.execute_reply.started":"2025-03-15T07:28:54.304591Z","shell.execute_reply":"2025-03-15T07:29:18.190931Z"}},"outputs":[{"name":"stdout","text":"Student Model Test Accuracy After Pruning: 10.00%\nSparsity After Pruning: 80.95%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"torch.save(pruned_student.state_dict(), \"pruned_student_unstructured_80%_res34.pth\")\nprint(\"Model saved as pruned_student_unstructured.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T19:03:39.338085Z","iopub.execute_input":"2025-03-14T19:03:39.338389Z","iopub.status.idle":"2025-03-14T19:03:39.468623Z","shell.execute_reply.started":"2025-03-14T19:03:39.338368Z","shell.execute_reply":"2025-03-14T19:03:39.467842Z"}},"outputs":[{"name":"stdout","text":"Model saved as pruned_student_unstructured.pth\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"retrained_student = retrain_with_sparsity(\n    pruned_student, train_loader, val_loader,\n    epochs=200,  save_path='/kaggle/working/retrained_student_model.pt',patience=10\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T19:03:51.441625Z","iopub.execute_input":"2025-03-14T19:03:51.441987Z","iopub.status.idle":"2025-03-14T23:31:31.643897Z","shell.execute_reply.started":"2025-03-14T19:03:51.441959Z","shell.execute_reply":"2025-03-14T23:31:31.643004Z"}},"outputs":[{"name":"stdout","text":"New best model saved with Val Accuracy: 59.97%\nEpoch 1/200 | Train Loss: 1.3459 | Train Acc: 48.86%\nValidation Loss: 1.1134 | Validation Acc: 59.97% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 69.04%\nEpoch 2/200 | Train Loss: 0.9368 | Train Acc: 66.44%\nValidation Loss: 0.8673 | Validation Acc: 69.04% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 75.37%\nEpoch 3/200 | Train Loss: 0.6922 | Train Acc: 75.64%\nValidation Loss: 0.7140 | Validation Acc: 75.37% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 81.20%\nEpoch 4/200 | Train Loss: 0.5133 | Train Acc: 82.20%\nValidation Loss: 0.5655 | Validation Acc: 81.20% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 85.31%\nEpoch 5/200 | Train Loss: 0.3747 | Train Acc: 87.15%\nValidation Loss: 0.4407 | Validation Acc: 85.31% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 86.48%\nEpoch 6/200 | Train Loss: 0.2741 | Train Acc: 90.55%\nValidation Loss: 0.4096 | Validation Acc: 86.48% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 87.12%\nEpoch 7/200 | Train Loss: 0.1967 | Train Acc: 93.17%\nValidation Loss: 0.4002 | Validation Acc: 87.12% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 88.19%\nEpoch 8/200 | Train Loss: 0.1393 | Train Acc: 95.26%\nValidation Loss: 0.3697 | Validation Acc: 88.19% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 89.26%\nEpoch 9/200 | Train Loss: 0.0997 | Train Acc: 96.65%\nValidation Loss: 0.3755 | Validation Acc: 89.26% | Sparsity: 80.95%\n\nEpoch 10/200 | Train Loss: 0.0754 | Train Acc: 97.41%\nValidation Loss: 0.4043 | Validation Acc: 89.04% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 89.30%\nEpoch 11/200 | Train Loss: 0.0580 | Train Acc: 98.06%\nValidation Loss: 0.3814 | Validation Acc: 89.30% | Sparsity: 80.95%\n\nEpoch 12/200 | Train Loss: 0.0482 | Train Acc: 98.35%\nValidation Loss: 0.4235 | Validation Acc: 89.29% | Sparsity: 80.95%\n\nEpoch 13/200 | Train Loss: 0.0413 | Train Acc: 98.64%\nValidation Loss: 0.4873 | Validation Acc: 88.79% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 89.60%\nEpoch 14/200 | Train Loss: 0.0367 | Train Acc: 98.77%\nValidation Loss: 0.4536 | Validation Acc: 89.60% | Sparsity: 80.95%\n\nEpoch 15/200 | Train Loss: 0.0336 | Train Acc: 98.83%\nValidation Loss: 0.4638 | Validation Acc: 89.15% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 90.18%\nEpoch 16/200 | Train Loss: 0.0311 | Train Acc: 98.87%\nValidation Loss: 0.4365 | Validation Acc: 90.18% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 90.30%\nEpoch 17/200 | Train Loss: 0.0279 | Train Acc: 99.02%\nValidation Loss: 0.4343 | Validation Acc: 90.30% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 90.39%\nEpoch 18/200 | Train Loss: 0.0236 | Train Acc: 99.17%\nValidation Loss: 0.4343 | Validation Acc: 90.39% | Sparsity: 80.95%\n\nEpoch 19/200 | Train Loss: 0.0240 | Train Acc: 99.19%\nValidation Loss: 0.4322 | Validation Acc: 90.34% | Sparsity: 80.95%\n\nEpoch 20/200 | Train Loss: 0.0188 | Train Acc: 99.38%\nValidation Loss: 0.4840 | Validation Acc: 89.77% | Sparsity: 80.95%\n\nEpoch 21/200 | Train Loss: 0.0203 | Train Acc: 99.30%\nValidation Loss: 0.4724 | Validation Acc: 90.04% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 90.83%\nEpoch 22/200 | Train Loss: 0.0161 | Train Acc: 99.47%\nValidation Loss: 0.4379 | Validation Acc: 90.83% | Sparsity: 80.95%\n\nEpoch 23/200 | Train Loss: 0.0158 | Train Acc: 99.50%\nValidation Loss: 0.4662 | Validation Acc: 90.60% | Sparsity: 80.95%\n\nEpoch 24/200 | Train Loss: 0.0159 | Train Acc: 99.46%\nValidation Loss: 0.4548 | Validation Acc: 90.74% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 90.98%\nEpoch 25/200 | Train Loss: 0.0138 | Train Acc: 99.51%\nValidation Loss: 0.4533 | Validation Acc: 90.98% | Sparsity: 80.95%\n\nEpoch 26/200 | Train Loss: 0.0119 | Train Acc: 99.60%\nValidation Loss: 0.4837 | Validation Acc: 90.58% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 91.22%\nEpoch 27/200 | Train Loss: 0.0131 | Train Acc: 99.54%\nValidation Loss: 0.4488 | Validation Acc: 91.22% | Sparsity: 80.95%\n\nEpoch 28/200 | Train Loss: 0.0112 | Train Acc: 99.61%\nValidation Loss: 0.4770 | Validation Acc: 91.18% | Sparsity: 80.95%\n\nEpoch 29/200 | Train Loss: 0.0109 | Train Acc: 99.63%\nValidation Loss: 0.5086 | Validation Acc: 90.73% | Sparsity: 80.95%\n\nEpoch 30/200 | Train Loss: 0.0096 | Train Acc: 99.64%\nValidation Loss: 0.4900 | Validation Acc: 90.95% | Sparsity: 80.95%\n\nEpoch 31/200 | Train Loss: 0.0117 | Train Acc: 99.61%\nValidation Loss: 0.4705 | Validation Acc: 90.92% | Sparsity: 80.95%\n\nEpoch 32/200 | Train Loss: 0.0095 | Train Acc: 99.69%\nValidation Loss: 0.4675 | Validation Acc: 91.21% | Sparsity: 80.95%\n\nEpoch 33/200 | Train Loss: 0.0096 | Train Acc: 99.68%\nValidation Loss: 0.5000 | Validation Acc: 91.00% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 91.23%\nEpoch 34/200 | Train Loss: 0.0081 | Train Acc: 99.73%\nValidation Loss: 0.4735 | Validation Acc: 91.23% | Sparsity: 80.95%\n\nEpoch 35/200 | Train Loss: 0.0056 | Train Acc: 99.82%\nValidation Loss: 0.5037 | Validation Acc: 90.91% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 91.63%\nEpoch 36/200 | Train Loss: 0.0065 | Train Acc: 99.78%\nValidation Loss: 0.4626 | Validation Acc: 91.63% | Sparsity: 80.95%\n\nEpoch 37/200 | Train Loss: 0.0060 | Train Acc: 99.80%\nValidation Loss: 0.4701 | Validation Acc: 91.41% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 91.91%\nEpoch 38/200 | Train Loss: 0.0046 | Train Acc: 99.86%\nValidation Loss: 0.4602 | Validation Acc: 91.91% | Sparsity: 80.95%\n\nEpoch 39/200 | Train Loss: 0.0040 | Train Acc: 99.88%\nValidation Loss: 0.4854 | Validation Acc: 91.57% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 91.94%\nEpoch 40/200 | Train Loss: 0.0046 | Train Acc: 99.84%\nValidation Loss: 0.4689 | Validation Acc: 91.94% | Sparsity: 80.95%\n\nEpoch 41/200 | Train Loss: 0.0044 | Train Acc: 99.86%\nValidation Loss: 0.4569 | Validation Acc: 91.89% | Sparsity: 80.95%\n\nEpoch 42/200 | Train Loss: 0.0043 | Train Acc: 99.85%\nValidation Loss: 0.4882 | Validation Acc: 91.43% | Sparsity: 80.95%\n\nEpoch 43/200 | Train Loss: 0.0041 | Train Acc: 99.86%\nValidation Loss: 0.4531 | Validation Acc: 91.88% | Sparsity: 80.95%\n\nEpoch 44/200 | Train Loss: 0.0048 | Train Acc: 99.86%\nValidation Loss: 0.5049 | Validation Acc: 91.51% | Sparsity: 80.95%\n\nEpoch 45/200 | Train Loss: 0.0032 | Train Acc: 99.91%\nValidation Loss: 0.4837 | Validation Acc: 91.79% | Sparsity: 80.95%\n\nEpoch 46/200 | Train Loss: 0.0025 | Train Acc: 99.92%\nValidation Loss: 0.4531 | Validation Acc: 91.84% | Sparsity: 80.95%\n\nEpoch 47/200 | Train Loss: 0.0030 | Train Acc: 99.90%\nValidation Loss: 0.4774 | Validation Acc: 91.69% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 92.12%\nEpoch 48/200 | Train Loss: 0.0018 | Train Acc: 99.94%\nValidation Loss: 0.4470 | Validation Acc: 92.12% | Sparsity: 80.95%\n\nEpoch 49/200 | Train Loss: 0.0025 | Train Acc: 99.91%\nValidation Loss: 0.4769 | Validation Acc: 91.60% | Sparsity: 80.95%\n\nEpoch 50/200 | Train Loss: 0.0037 | Train Acc: 99.90%\nValidation Loss: 0.4875 | Validation Acc: 91.65% | Sparsity: 80.95%\n\nEpoch 51/200 | Train Loss: 0.0019 | Train Acc: 99.94%\nValidation Loss: 0.4632 | Validation Acc: 91.83% | Sparsity: 80.95%\n\nEpoch 52/200 | Train Loss: 0.0026 | Train Acc: 99.92%\nValidation Loss: 0.4690 | Validation Acc: 91.81% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 92.28%\nEpoch 53/200 | Train Loss: 0.0015 | Train Acc: 99.97%\nValidation Loss: 0.4722 | Validation Acc: 92.28% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 92.30%\nEpoch 54/200 | Train Loss: 0.0020 | Train Acc: 99.93%\nValidation Loss: 0.4648 | Validation Acc: 92.30% | Sparsity: 80.95%\n\nEpoch 55/200 | Train Loss: 0.0010 | Train Acc: 99.98%\nValidation Loss: 0.4580 | Validation Acc: 92.14% | Sparsity: 80.95%\n\nEpoch 56/200 | Train Loss: 0.0018 | Train Acc: 99.95%\nValidation Loss: 0.4611 | Validation Acc: 92.29% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 92.38%\nEpoch 57/200 | Train Loss: 0.0015 | Train Acc: 99.95%\nValidation Loss: 0.4550 | Validation Acc: 92.38% | Sparsity: 80.95%\n\nEpoch 58/200 | Train Loss: 0.0007 | Train Acc: 99.98%\nValidation Loss: 0.4639 | Validation Acc: 92.05% | Sparsity: 80.95%\n\nEpoch 59/200 | Train Loss: 0.0011 | Train Acc: 99.97%\nValidation Loss: 0.4601 | Validation Acc: 92.38% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 92.57%\nEpoch 60/200 | Train Loss: 0.0006 | Train Acc: 99.98%\nValidation Loss: 0.4633 | Validation Acc: 92.57% | Sparsity: 80.95%\n\nEpoch 61/200 | Train Loss: 0.0007 | Train Acc: 99.98%\nValidation Loss: 0.4828 | Validation Acc: 92.09% | Sparsity: 80.95%\n\nEpoch 62/200 | Train Loss: 0.0007 | Train Acc: 99.97%\nValidation Loss: 0.4738 | Validation Acc: 92.34% | Sparsity: 80.95%\n\nEpoch 63/200 | Train Loss: 0.0005 | Train Acc: 99.98%\nValidation Loss: 0.4816 | Validation Acc: 92.00% | Sparsity: 80.95%\n\nEpoch 64/200 | Train Loss: 0.0011 | Train Acc: 99.97%\nValidation Loss: 0.4789 | Validation Acc: 92.16% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 92.81%\nEpoch 65/200 | Train Loss: 0.0006 | Train Acc: 99.99%\nValidation Loss: 0.4700 | Validation Acc: 92.81% | Sparsity: 80.95%\n\nEpoch 66/200 | Train Loss: 0.0004 | Train Acc: 99.99%\nValidation Loss: 0.4813 | Validation Acc: 92.21% | Sparsity: 80.95%\n\nEpoch 67/200 | Train Loss: 0.0003 | Train Acc: 100.00%\nValidation Loss: 0.4706 | Validation Acc: 92.59% | Sparsity: 80.95%\n\nEpoch 68/200 | Train Loss: 0.0006 | Train Acc: 99.98%\nValidation Loss: 0.4620 | Validation Acc: 92.61% | Sparsity: 80.95%\n\nEpoch 69/200 | Train Loss: 0.0005 | Train Acc: 99.98%\nValidation Loss: 0.4611 | Validation Acc: 92.45% | Sparsity: 80.95%\n\nEpoch 70/200 | Train Loss: 0.0003 | Train Acc: 99.99%\nValidation Loss: 0.4540 | Validation Acc: 92.57% | Sparsity: 80.95%\n\nEpoch 71/200 | Train Loss: 0.0003 | Train Acc: 100.00%\nValidation Loss: 0.4602 | Validation Acc: 92.62% | Sparsity: 80.95%\n\nEpoch 72/200 | Train Loss: 0.0002 | Train Acc: 100.00%\nValidation Loss: 0.4542 | Validation Acc: 92.80% | Sparsity: 80.95%\n\nEpoch 73/200 | Train Loss: 0.0003 | Train Acc: 99.99%\nValidation Loss: 0.4672 | Validation Acc: 92.41% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 92.92%\nEpoch 74/200 | Train Loss: 0.0002 | Train Acc: 100.00%\nValidation Loss: 0.4461 | Validation Acc: 92.92% | Sparsity: 80.95%\n\nEpoch 75/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4493 | Validation Acc: 92.85% | Sparsity: 80.95%\n\nEpoch 76/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4468 | Validation Acc: 92.81% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 92.93%\nEpoch 77/200 | Train Loss: 0.0000 | Train Acc: 100.00%\nValidation Loss: 0.4463 | Validation Acc: 92.93% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 92.96%\nEpoch 78/200 | Train Loss: 0.0000 | Train Acc: 100.00%\nValidation Loss: 0.4437 | Validation Acc: 92.96% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 93.02%\nEpoch 79/200 | Train Loss: 0.0000 | Train Acc: 100.00%\nValidation Loss: 0.4418 | Validation Acc: 93.02% | Sparsity: 80.95%\n\nEpoch 80/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4597 | Validation Acc: 92.70% | Sparsity: 80.95%\n\nEpoch 81/200 | Train Loss: 0.0000 | Train Acc: 100.00%\nValidation Loss: 0.4502 | Validation Acc: 92.86% | Sparsity: 80.95%\n\nNew best model saved with Val Accuracy: 93.03%\nEpoch 82/200 | Train Loss: 0.0000 | Train Acc: 100.00%\nValidation Loss: 0.4420 | Validation Acc: 93.03% | Sparsity: 80.95%\n\nEpoch 83/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4588 | Validation Acc: 92.83% | Sparsity: 80.95%\n\nEpoch 84/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4543 | Validation Acc: 92.85% | Sparsity: 80.95%\n\nEpoch 85/200 | Train Loss: 0.0003 | Train Acc: 100.00%\nValidation Loss: 0.4534 | Validation Acc: 92.83% | Sparsity: 80.95%\n\nEpoch 86/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4451 | Validation Acc: 92.51% | Sparsity: 80.95%\n\nEpoch 87/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4603 | Validation Acc: 92.62% | Sparsity: 80.95%\n\nEpoch 88/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4626 | Validation Acc: 92.53% | Sparsity: 80.95%\n\nEpoch 89/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4679 | Validation Acc: 92.52% | Sparsity: 80.95%\n\nEpoch 90/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4680 | Validation Acc: 92.76% | Sparsity: 80.95%\n\nEpoch 91/200 | Train Loss: 0.0001 | Train Acc: 100.00%\nValidation Loss: 0.4585 | Validation Acc: 92.75% | Sparsity: 80.95%\n\nEarly stopping triggered at epoch 92. No improvement for 10 epochs.\nBest Validation Accuracy: 93.03% | Best Model Saved at: /kaggle/working/retrained_student_model.pt\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"student_accuracy = evaluate(retrained_student, test_loader, device)\nprint(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T23:31:31.645234Z","iopub.execute_input":"2025-03-14T23:31:31.645558Z","iopub.status.idle":"2025-03-14T23:31:52.927848Z","shell.execute_reply.started":"2025-03-14T23:31:31.645533Z","shell.execute_reply":"2025-03-14T23:31:52.926969Z"}},"outputs":[{"name":"stdout","text":" Retrained Pruned Student Model Test Accuracy: 92.71%\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"# Measure inference times\npruned_student_inference_time = measure_inference_time(retrained_student, test_loader, device)\nprint(f\" Retrained pruned Student Model Inference Time: {pruned_student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T23:33:31.016247Z","iopub.execute_input":"2025-03-14T23:33:31.016556Z","iopub.status.idle":"2025-03-14T23:34:48.205535Z","shell.execute_reply.started":"2025-03-14T23:33:31.016525Z","shell.execute_reply":"2025-03-14T23:34:48.204681Z"}},"outputs":[{"name":"stdout","text":" Retrained pruned Student Model Inference Time: 6.01 ms per batch\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"torch.save(retrained_student.state_dict(), \"pruned_retrained_student_unstructured_80%.pth\")\nprint(\"Model saved as pruned_student_unstructured.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T23:34:48.206431Z","iopub.execute_input":"2025-03-14T23:34:48.206710Z","iopub.status.idle":"2025-03-14T23:34:48.338259Z","shell.execute_reply.started":"2025-03-14T23:34:48.206685Z","shell.execute_reply":"2025-03-14T23:34:48.337349Z"}},"outputs":[{"name":"stdout","text":"Model saved as pruned_student_unstructured.pth\n","output_type":"stream"}],"execution_count":82},{"cell_type":"markdown","source":"# Retrain With KD","metadata":{}},{"cell_type":"code","source":"pruned_student = retrain_with_KD(\n    teacher, pruned_student, train_loader, val_loader,\n    epochs=50, temperature=5.0, alpha=0.5, patience=5,save_path=\"pruned_student_retrain_KD_80%.pth\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:09:16.360145Z","iopub.execute_input":"2025-03-16T10:09:16.360401Z","iopub.status.idle":"2025-03-16T13:10:48.806028Z","shell.execute_reply.started":"2025-03-16T10:09:16.360380Z","shell.execute_reply":"2025-03-16T13:10:48.805093Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50 | Train Loss: 6.7128 | Train Acc: 26.93% | Val Loss: 2.0609 | Val Acc: 35.80% | Sparsity: 80.95%\nEpoch 2/50 | Train Loss: 3.1727 | Train Acc: 66.32% | Val Loss: 0.8355 | Val Acc: 77.17% | Sparsity: 80.95%\nEpoch 3/50 | Train Loss: 1.5575 | Train Acc: 83.73% | Val Loss: 0.5588 | Val Acc: 84.83% | Sparsity: 80.95%\nEpoch 4/50 | Train Loss: 1.0419 | Train Acc: 89.58% | Val Loss: 0.4963 | Val Acc: 87.12% | Sparsity: 80.95%\nEpoch 5/50 | Train Loss: 0.7580 | Train Acc: 93.16% | Val Loss: 0.4160 | Val Acc: 89.33% | Sparsity: 80.95%\nEpoch 6/50 | Train Loss: 0.5649 | Train Acc: 95.55% | Val Loss: 0.4072 | Val Acc: 90.07% | Sparsity: 80.95%\nEpoch 7/50 | Train Loss: 0.4459 | Train Acc: 97.14% | Val Loss: 0.3546 | Val Acc: 90.61% | Sparsity: 80.95%\nEpoch 8/50 | Train Loss: 0.3572 | Train Acc: 98.38% | Val Loss: 0.3098 | Val Acc: 91.93% | Sparsity: 80.95%\nEpoch 9/50 | Train Loss: 0.3078 | Train Acc: 98.87% | Val Loss: 0.2786 | Val Acc: 92.77% | Sparsity: 80.95%\nEpoch 10/50 | Train Loss: 0.2713 | Train Acc: 99.16% | Val Loss: 0.2636 | Val Acc: 92.89% | Sparsity: 80.95%\nEpoch 11/50 | Train Loss: 0.2428 | Train Acc: 99.33% | Val Loss: 0.2555 | Val Acc: 92.94% | Sparsity: 80.95%\nEpoch 12/50 | Train Loss: 0.2232 | Train Acc: 99.37% | Val Loss: 0.2492 | Val Acc: 93.29% | Sparsity: 80.95%\nEpoch 13/50 | Train Loss: 0.2039 | Train Acc: 99.48% | Val Loss: 0.2472 | Val Acc: 93.44% | Sparsity: 80.95%\nEpoch 14/50 | Train Loss: 0.1899 | Train Acc: 99.45% | Val Loss: 0.2366 | Val Acc: 93.47% | Sparsity: 80.95%\nEpoch 15/50 | Train Loss: 0.1839 | Train Acc: 99.50% | Val Loss: 0.2420 | Val Acc: 93.32% | Sparsity: 80.95%\nEpoch 16/50 | Train Loss: 0.1704 | Train Acc: 99.53% | Val Loss: 0.2323 | Val Acc: 93.38% | Sparsity: 80.95%\nEpoch 17/50 | Train Loss: 0.1623 | Train Acc: 99.55% | Val Loss: 0.2365 | Val Acc: 93.38% | Sparsity: 80.95%\nEpoch 18/50 | Train Loss: 0.1533 | Train Acc: 99.56% | Val Loss: 0.2285 | Val Acc: 93.77% | Sparsity: 80.95%\nEpoch 19/50 | Train Loss: 0.1465 | Train Acc: 99.56% | Val Loss: 0.2271 | Val Acc: 93.45% | Sparsity: 80.95%\nEpoch 20/50 | Train Loss: 0.1443 | Train Acc: 99.56% | Val Loss: 0.2299 | Val Acc: 93.39% | Sparsity: 80.95%\nEpoch 21/50 | Train Loss: 0.1372 | Train Acc: 99.58% | Val Loss: 0.2306 | Val Acc: 93.42% | Sparsity: 80.95%\nEpoch 22/50 | Train Loss: 0.1305 | Train Acc: 99.61% | Val Loss: 0.2318 | Val Acc: 93.35% | Sparsity: 80.95%\nEpoch 23/50 | Train Loss: 0.1240 | Train Acc: 99.59% | Val Loss: 0.2213 | Val Acc: 93.54% | Sparsity: 80.95%\nEpoch 24/50 | Train Loss: 0.1216 | Train Acc: 99.65% | Val Loss: 0.2244 | Val Acc: 93.66% | Sparsity: 80.95%\nEpoch 25/50 | Train Loss: 0.1157 | Train Acc: 99.62% | Val Loss: 0.2286 | Val Acc: 93.57% | Sparsity: 80.95%\nEpoch 26/50 | Train Loss: 0.1134 | Train Acc: 99.63% | Val Loss: 0.2300 | Val Acc: 93.67% | Sparsity: 80.95%\nEpoch 27/50 | Train Loss: 0.1104 | Train Acc: 99.61% | Val Loss: 0.2289 | Val Acc: 93.38% | Sparsity: 80.95%\nEpoch 28/50 | Train Loss: 0.1073 | Train Acc: 99.64% | Val Loss: 0.2259 | Val Acc: 93.37% | Sparsity: 80.95%\nEpoch 29/50 | Train Loss: 0.1011 | Train Acc: 99.67% | Val Loss: 0.2317 | Val Acc: 93.62% | Sparsity: 80.95%\nEpoch 30/50 | Train Loss: 0.0989 | Train Acc: 99.67% | Val Loss: 0.2327 | Val Acc: 93.41% | Sparsity: 80.95%\nEpoch 31/50 | Train Loss: 0.0982 | Train Acc: 99.63% | Val Loss: 0.2245 | Val Acc: 93.63% | Sparsity: 80.95%\nEpoch 32/50 | Train Loss: 0.0977 | Train Acc: 99.66% | Val Loss: 0.2302 | Val Acc: 93.34% | Sparsity: 80.95%\nEpoch 33/50 | Train Loss: 0.0919 | Train Acc: 99.66% | Val Loss: 0.2257 | Val Acc: 93.40% | Sparsity: 80.95%\nEpoch 34/50 | Train Loss: 0.0904 | Train Acc: 99.68% | Val Loss: 0.2324 | Val Acc: 93.42% | Sparsity: 80.95%\nEpoch 35/50 | Train Loss: 0.0876 | Train Acc: 99.67% | Val Loss: 0.2249 | Val Acc: 93.47% | Sparsity: 80.95%\nEpoch 36/50 | Train Loss: 0.0849 | Train Acc: 99.72% | Val Loss: 0.2178 | Val Acc: 93.64% | Sparsity: 80.95%\nEpoch 37/50 | Train Loss: 0.0821 | Train Acc: 99.66% | Val Loss: 0.2234 | Val Acc: 93.36% | Sparsity: 80.95%\nEpoch 38/50 | Train Loss: 0.0830 | Train Acc: 99.69% | Val Loss: 0.2195 | Val Acc: 93.73% | Sparsity: 80.95%\nEpoch 39/50 | Train Loss: 0.0814 | Train Acc: 99.70% | Val Loss: 0.2208 | Val Acc: 93.71% | Sparsity: 80.95%\nEpoch 40/50 | Train Loss: 0.0796 | Train Acc: 99.67% | Val Loss: 0.2192 | Val Acc: 93.49% | Sparsity: 80.95%\nEpoch 41/50 | Train Loss: 0.0774 | Train Acc: 99.68% | Val Loss: 0.2231 | Val Acc: 93.46% | Sparsity: 80.95%\nEpoch 42/50 | Train Loss: 0.0740 | Train Acc: 99.74% | Val Loss: 0.2292 | Val Acc: 93.59% | Sparsity: 80.95%\nEpoch 43/50 | Train Loss: 0.0732 | Train Acc: 99.70% | Val Loss: 0.2176 | Val Acc: 93.71% | Sparsity: 80.95%\nEpoch 44/50 | Train Loss: 0.0700 | Train Acc: 99.69% | Val Loss: 0.2226 | Val Acc: 93.44% | Sparsity: 80.95%\nEpoch 45/50 | Train Loss: 0.0693 | Train Acc: 99.72% | Val Loss: 0.2242 | Val Acc: 93.53% | Sparsity: 80.95%\nEpoch 46/50 | Train Loss: 0.0678 | Train Acc: 99.71% | Val Loss: 0.2220 | Val Acc: 93.45% | Sparsity: 80.95%\nEpoch 47/50 | Train Loss: 0.0681 | Train Acc: 99.67% | Val Loss: 0.2211 | Val Acc: 93.48% | Sparsity: 80.95%\nEpoch 48/50 | Train Loss: 0.0665 | Train Acc: 99.72% | Val Loss: 0.2227 | Val Acc: 93.49% | Sparsity: 80.95%\nEarly stopping triggered at epoch 48. No improvement for 5 epochs.\nStudent model saved before pruning at: pruned_student_retrain_KD_80%.pth\nTotal Training Time: 181m 32s\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"student_accuracy = evaluate(pruned_student, test_loader, device)\nprint(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:10:48.807281Z","iopub.execute_input":"2025-03-16T13:10:48.807531Z","iopub.status.idle":"2025-03-16T13:11:10.384730Z","shell.execute_reply.started":"2025-03-16T13:10:48.807510Z","shell.execute_reply":"2025-03-16T13:11:10.383821Z"}},"outputs":[{"name":"stdout","text":" Retrained Pruned Student Model Test Accuracy: 93.28%\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"# 70% Sparsity","metadata":{}},{"cell_type":"code","source":"# Load pretrained ResNet-34 (Student Model)\nstudent = models.resnet34(pretrained=True)\n# Modify the final fully connected layer for 10 classes (CIFAR-10)\nstudent.fc = nn.Linear(student.fc.in_features, 10)\nstudent = student.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:45:55.807151Z","iopub.execute_input":"2025-03-16T06:45:55.807479Z","iopub.status.idle":"2025-03-16T06:45:56.173824Z","shell.execute_reply.started":"2025-03-16T06:45:55.807453Z","shell.execute_reply":"2025-03-16T06:45:56.173121Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\nmodel_path = '/kaggle/input/student_resnet_34/pytorch/default/1/student_before_pruning_Res34.pth'\n# Load the model weights\nstudent.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:45:56.215560Z","iopub.execute_input":"2025-03-16T06:45:56.215881Z","iopub.status.idle":"2025-03-16T06:45:57.385438Z","shell.execute_reply.started":"2025-03-16T06:45:56.215848Z","shell.execute_reply":"2025-03-16T06:45:57.384660Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-18-5d6657d12f2d>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Measure inference times\nteacher_inference_time = measure_inference_time(teacher, test_loader, device)\nstudent_inference_time = measure_inference_time(student, test_loader, device)\nprint(f\"Teacher Model Inference Time: {teacher_inference_time * 1000:.2f} ms per batch\")\nprint(f\"Student Model Inference Time(Before Pruning): {student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:12:06.427999Z","iopub.execute_input":"2025-03-15T16:12:06.428344Z","iopub.status.idle":"2025-03-15T16:14:43.817699Z","shell.execute_reply.started":"2025-03-15T16:12:06.428314Z","shell.execute_reply":"2025-03-15T16:14:43.816987Z"}},"outputs":[{"name":"stdout","text":"Teacher Model Inference Time: 7.57 ms per batch\nStudent Model Inference Time(Before Pruning): 6.03 ms per batch\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Calculate sparsity\nsparsity = calculate_sparsity(student)\nprint(f\"Sparsity Before Pruning: {sparsity * 100:.2f}%\")\n\nteacher_accuracy = evaluate(teacher, test_loader, device)\nstudent_accuracy = evaluate(student, test_loader, device)\nprint(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\nprint(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:46:05.533072Z","iopub.execute_input":"2025-03-16T06:46:05.533414Z","iopub.status.idle":"2025-03-16T06:47:00.671709Z","shell.execute_reply.started":"2025-03-16T06:46:05.533386Z","shell.execute_reply":"2025-03-16T06:47:00.670806Z"}},"outputs":[{"name":"stdout","text":"Sparsity Before Pruning: 0.00%\nTeacher Model Test Accuracy: 95.41%\nStudent Model Test Accuracy Before Pruning: 96.55%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Pruning\nprint(\"Calculating Important Scores\")\nstart_time = time.time()\nimportance_scores = compute_gradient_importance(\n    teacher, student, train_loader, device, temperature=5.0, alpha=0.5\n)\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n\nprint(\"Pruning the model\")\nstart_time = time.time()\npruned_student = prune_model(student, importance_scores, prune_ratio=0.707,pruning_type='unstructured')\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\nstudent = student.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:47:00.673030Z","iopub.execute_input":"2025-03-16T06:47:00.673378Z","iopub.status.idle":"2025-03-16T06:49:31.831589Z","shell.execute_reply.started":"2025-03-16T06:47:00.673345Z","shell.execute_reply":"2025-03-16T06:49:31.830696Z"}},"outputs":[{"name":"stdout","text":"Calculating Important Scores\nTotal Time take to calculate Important scores: 2m 31s\nPruning the model\nTotal Time take to prune the model scores: 0m 0s\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"student_accuracy = evaluate(pruned_student, test_loader, device)\nprint(f\"Student Model Test Accuracy After Pruning: {student_accuracy:.2f}%\")\nsparsity = calculate_sparsity(pruned_student)\nprint(f\"Sparsity After Pruning: {sparsity * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:49:31.833072Z","iopub.execute_input":"2025-03-16T06:49:31.833331Z","iopub.status.idle":"2025-03-16T06:49:55.568163Z","shell.execute_reply.started":"2025-03-16T06:49:31.833310Z","shell.execute_reply":"2025-03-16T06:49:55.567433Z"}},"outputs":[{"name":"stdout","text":"Student Model Test Accuracy After Pruning: 10.00%\nSparsity After Pruning: 70.65%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Measure inference times\npruned_student_inference_time = measure_inference_time(pruned_student, test_loader, device)\nprint(f\"Student Model Inference Time(After Pruning): {pruned_student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:49:55.569108Z","iopub.execute_input":"2025-03-16T06:49:55.569325Z","iopub.status.idle":"2025-03-16T06:51:24.618666Z","shell.execute_reply.started":"2025-03-16T06:49:55.569307Z","shell.execute_reply":"2025-03-16T06:51:24.617743Z"}},"outputs":[{"name":"stdout","text":"Student Model Inference Time(After Pruning): 5.95 ms per batch\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"retrained_student = retrain_with_sparsity(\n    pruned_student, train_loader, val_loader,\n    epochs=200,  save_path='/kaggle/working/retrained_student_model_70%.pt',patience=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:51:24.619670Z","iopub.execute_input":"2025-03-16T06:51:24.619995Z","iopub.status.idle":"2025-03-16T07:40:41.639751Z","shell.execute_reply.started":"2025-03-16T06:51:24.619962Z","shell.execute_reply":"2025-03-16T07:40:41.638881Z"}},"outputs":[{"name":"stdout","text":"New best model saved with Val Accuracy: 52.21%\nEpoch 1/200 | Train Loss: 1.8875 | Train Acc: 32.25%\nValidation Loss: 1.3421 | Validation Acc: 52.21% | Sparsity: 70.65%\n\nNew best model saved with Val Accuracy: 91.50%\nEpoch 2/200 | Train Loss: 0.5644 | Train Acc: 80.40%\nValidation Loss: 0.2579 | Validation Acc: 91.50% | Sparsity: 70.65%\n\nNew best model saved with Val Accuracy: 93.10%\nEpoch 3/200 | Train Loss: 0.1795 | Train Acc: 94.03%\nValidation Loss: 0.2114 | Validation Acc: 93.10% | Sparsity: 70.65%\n\nNew best model saved with Val Accuracy: 93.94%\nEpoch 4/200 | Train Loss: 0.0901 | Train Acc: 96.93%\nValidation Loss: 0.1904 | Validation Acc: 93.94% | Sparsity: 70.65%\n\nNew best model saved with Val Accuracy: 94.28%\nEpoch 5/200 | Train Loss: 0.0523 | Train Acc: 98.26%\nValidation Loss: 0.1911 | Validation Acc: 94.28% | Sparsity: 70.65%\n\nEpoch 6/200 | Train Loss: 0.0315 | Train Acc: 98.99%\nValidation Loss: 0.2178 | Validation Acc: 94.04% | Sparsity: 70.65%\n\nEpoch 7/200 | Train Loss: 0.0247 | Train Acc: 99.21%\nValidation Loss: 0.2247 | Validation Acc: 94.07% | Sparsity: 70.65%\n\nEpoch 8/200 | Train Loss: 0.0190 | Train Acc: 99.37%\nValidation Loss: 0.2369 | Validation Acc: 94.06% | Sparsity: 70.65%\n\nNew best model saved with Val Accuracy: 94.36%\nEpoch 9/200 | Train Loss: 0.0156 | Train Acc: 99.46%\nValidation Loss: 0.2249 | Validation Acc: 94.36% | Sparsity: 70.65%\n\nNew best model saved with Val Accuracy: 94.51%\nEpoch 10/200 | Train Loss: 0.0123 | Train Acc: 99.60%\nValidation Loss: 0.2395 | Validation Acc: 94.51% | Sparsity: 70.65%\n\nEpoch 11/200 | Train Loss: 0.0108 | Train Acc: 99.66%\nValidation Loss: 0.2460 | Validation Acc: 94.45% | Sparsity: 70.65%\n\nNew best model saved with Val Accuracy: 94.84%\nEpoch 12/200 | Train Loss: 0.0090 | Train Acc: 99.68%\nValidation Loss: 0.2225 | Validation Acc: 94.84% | Sparsity: 70.65%\n\nEpoch 13/200 | Train Loss: 0.0076 | Train Acc: 99.77%\nValidation Loss: 0.2417 | Validation Acc: 94.78% | Sparsity: 70.65%\n\nEpoch 14/200 | Train Loss: 0.0073 | Train Acc: 99.76%\nValidation Loss: 0.2557 | Validation Acc: 94.31% | Sparsity: 70.65%\n\nEpoch 15/200 | Train Loss: 0.0064 | Train Acc: 99.79%\nValidation Loss: 0.2473 | Validation Acc: 94.75% | Sparsity: 70.65%\n\nEpoch 16/200 | Train Loss: 0.0056 | Train Acc: 99.83%\nValidation Loss: 0.2472 | Validation Acc: 94.60% | Sparsity: 70.65%\n\nEarly stopping triggered at epoch 17. No improvement for 5 epochs.\nBest Validation Accuracy: 94.84% | Best Model Saved at: /kaggle/working/retrained_student_model_70%.pt\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"student_accuracy = evaluate(retrained_student, test_loader, device)\nprint(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:40:41.640531Z","iopub.execute_input":"2025-03-16T07:40:41.640742Z","iopub.status.idle":"2025-03-16T07:41:03.225405Z","shell.execute_reply.started":"2025-03-16T07:40:41.640725Z","shell.execute_reply":"2025-03-16T07:41:03.224691Z"}},"outputs":[{"name":"stdout","text":" Retrained Pruned Student Model Test Accuracy: 93.55%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Measure inference times\npruned_student_inference_time = measure_inference_time(retrained_student, test_loader, device)\nprint(f\" Retrained pruned Student Model Inference Time: {pruned_student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:41:03.226137Z","iopub.execute_input":"2025-03-16T07:41:03.226437Z","iopub.status.idle":"2025-03-16T07:42:21.402028Z","shell.execute_reply.started":"2025-03-16T07:41:03.226414Z","shell.execute_reply":"2025-03-16T07:42:21.401178Z"}},"outputs":[{"name":"stdout","text":" Retrained pruned Student Model Inference Time: 5.89 ms per batch\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"torch.save(retrained_student.state_dict(), \"pruned_retrained_student_unstructured_80%.pth\")\nprint(\"Model saved as pruned_student_unstructured.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T10:04:53.260028Z","iopub.execute_input":"2025-03-02T10:04:53.260235Z","iopub.status.idle":"2025-03-02T10:04:53.369413Z","shell.execute_reply.started":"2025-03-02T10:04:53.260216Z","shell.execute_reply":"2025-03-02T10:04:53.368494Z"}},"outputs":[{"name":"stdout","text":"Model saved as pruned_student_unstructured.pth\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Retrain with KD","metadata":{}},{"cell_type":"code","source":"# Load pretrained ResNet-18 (Student Model)\nstudent = models.resnet34(pretrained=True)\n# Modify the final fully connected layer for 10 classes (CIFAR-10)\nstudent.fc = nn.Linear(student.fc.in_features, 10)\nstudent = student.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:42:49.807378Z","iopub.execute_input":"2025-03-16T07:42:49.807666Z","iopub.status.idle":"2025-03-16T07:42:50.179671Z","shell.execute_reply.started":"2025-03-16T07:42:49.807644Z","shell.execute_reply":"2025-03-16T07:42:50.178989Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"\nmodel_path = '/kaggle/input/student_resnet_34/pytorch/default/1/student_before_pruning_Res34.pth'\n# Load the model weights\nstudent.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:42:50.534408Z","iopub.execute_input":"2025-03-16T07:42:50.534702Z","iopub.status.idle":"2025-03-16T07:42:50.625349Z","shell.execute_reply.started":"2025-03-16T07:42:50.534680Z","shell.execute_reply":"2025-03-16T07:42:50.624415Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-27-5d6657d12f2d>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# Pruning\nprint(\"Calculating Important Scores\")\nstart_time = time.time()\nimportance_scores = compute_gradient_importance(\n    teacher, student, train_loader, device, temperature=5.0, alpha=0.5\n)\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n\nprint(\"Pruning the model\")\nstart_time = time.time()\npruned_student = prune_model(student, importance_scores, prune_ratio=0.707,pruning_type='unstructured')\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\nstudent = student.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:43:33.266972Z","iopub.execute_input":"2025-03-16T07:43:33.267298Z","iopub.status.idle":"2025-03-16T07:46:06.504340Z","shell.execute_reply.started":"2025-03-16T07:43:33.267274Z","shell.execute_reply":"2025-03-16T07:46:06.503560Z"}},"outputs":[{"name":"stdout","text":"Calculating Important Scores\nTotal Time take to calculate Important scores: 2m 33s\nPruning the model\nTotal Time take to prune the model scores: 0m 0s\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"pruned_student = retrain_with_KD(\n    teacher, pruned_student, train_loader, val_loader,\n    epochs=50, temperature=5.0, alpha=0.5, patience=5,save_path=\"pruned_student_retrain_KD_80%.pth\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T07:46:06.505287Z","iopub.execute_input":"2025-03-16T07:46:06.505583Z","iopub.status.idle":"2025-03-16T10:06:21.189289Z","shell.execute_reply.started":"2025-03-16T07:46:06.505561Z","shell.execute_reply":"2025-03-16T10:06:21.188469Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50 | Train Loss: 4.7593 | Train Acc: 50.07% | Val Loss: 0.5529 | Val Acc: 85.22% | Sparsity: 70.65%\nEpoch 2/50 | Train Loss: 0.8204 | Train Acc: 92.30% | Val Loss: 0.2575 | Val Acc: 92.50% | Sparsity: 70.65%\nEpoch 3/50 | Train Loss: 0.5003 | Train Acc: 96.36% | Val Loss: 0.2004 | Val Acc: 93.96% | Sparsity: 70.65%\nEpoch 4/50 | Train Loss: 0.3455 | Train Acc: 98.28% | Val Loss: 0.1458 | Val Acc: 95.59% | Sparsity: 70.65%\nEpoch 5/50 | Train Loss: 0.2706 | Train Acc: 99.10% | Val Loss: 0.1482 | Val Acc: 95.45% | Sparsity: 70.65%\nEpoch 6/50 | Train Loss: 0.2312 | Train Acc: 99.33% | Val Loss: 0.1271 | Val Acc: 95.97% | Sparsity: 70.65%\nEpoch 7/50 | Train Loss: 0.2022 | Train Acc: 99.41% | Val Loss: 0.1407 | Val Acc: 95.84% | Sparsity: 70.65%\nEpoch 8/50 | Train Loss: 0.1881 | Train Acc: 99.52% | Val Loss: 0.1220 | Val Acc: 96.02% | Sparsity: 70.65%\nEpoch 9/50 | Train Loss: 0.1775 | Train Acc: 99.45% | Val Loss: 0.1239 | Val Acc: 96.20% | Sparsity: 70.65%\nEpoch 10/50 | Train Loss: 0.1578 | Train Acc: 99.61% | Val Loss: 0.1175 | Val Acc: 96.29% | Sparsity: 70.65%\nEpoch 11/50 | Train Loss: 0.1481 | Train Acc: 99.56% | Val Loss: 0.1213 | Val Acc: 96.29% | Sparsity: 70.65%\nEpoch 12/50 | Train Loss: 0.1395 | Train Acc: 99.60% | Val Loss: 0.1108 | Val Acc: 96.56% | Sparsity: 70.65%\nEpoch 13/50 | Train Loss: 0.1316 | Train Acc: 99.59% | Val Loss: 0.1087 | Val Acc: 96.63% | Sparsity: 70.65%\nEpoch 14/50 | Train Loss: 0.1219 | Train Acc: 99.58% | Val Loss: 0.1114 | Val Acc: 96.36% | Sparsity: 70.65%\nEpoch 15/50 | Train Loss: 0.1132 | Train Acc: 99.64% | Val Loss: 0.1114 | Val Acc: 96.58% | Sparsity: 70.65%\nEpoch 16/50 | Train Loss: 0.1116 | Train Acc: 99.61% | Val Loss: 0.1106 | Val Acc: 96.56% | Sparsity: 70.65%\nEpoch 17/50 | Train Loss: 0.1037 | Train Acc: 99.62% | Val Loss: 0.1067 | Val Acc: 96.60% | Sparsity: 70.65%\nEpoch 18/50 | Train Loss: 0.0996 | Train Acc: 99.66% | Val Loss: 0.1119 | Val Acc: 96.41% | Sparsity: 70.65%\nEpoch 19/50 | Train Loss: 0.0982 | Train Acc: 99.69% | Val Loss: 0.1153 | Val Acc: 96.50% | Sparsity: 70.65%\nEpoch 20/50 | Train Loss: 0.0920 | Train Acc: 99.64% | Val Loss: 0.1109 | Val Acc: 96.58% | Sparsity: 70.65%\nEpoch 21/50 | Train Loss: 0.0935 | Train Acc: 99.65% | Val Loss: 0.1067 | Val Acc: 96.56% | Sparsity: 70.65%\nEpoch 22/50 | Train Loss: 0.0869 | Train Acc: 99.71% | Val Loss: 0.1067 | Val Acc: 96.64% | Sparsity: 70.65%\nEpoch 23/50 | Train Loss: 0.0841 | Train Acc: 99.70% | Val Loss: 0.1077 | Val Acc: 96.59% | Sparsity: 70.65%\nEpoch 24/50 | Train Loss: 0.0833 | Train Acc: 99.66% | Val Loss: 0.1075 | Val Acc: 96.73% | Sparsity: 70.65%\nEpoch 25/50 | Train Loss: 0.0793 | Train Acc: 99.69% | Val Loss: 0.1094 | Val Acc: 96.62% | Sparsity: 70.65%\nEpoch 26/50 | Train Loss: 0.0776 | Train Acc: 99.69% | Val Loss: 0.1140 | Val Acc: 96.37% | Sparsity: 70.65%\nEpoch 27/50 | Train Loss: 0.0740 | Train Acc: 99.71% | Val Loss: 0.1071 | Val Acc: 96.53% | Sparsity: 70.65%\nEpoch 28/50 | Train Loss: 0.0712 | Train Acc: 99.73% | Val Loss: 0.1057 | Val Acc: 96.64% | Sparsity: 70.65%\nEpoch 29/50 | Train Loss: 0.0718 | Train Acc: 99.69% | Val Loss: 0.1036 | Val Acc: 96.62% | Sparsity: 70.65%\nEpoch 30/50 | Train Loss: 0.0716 | Train Acc: 99.71% | Val Loss: 0.1084 | Val Acc: 96.46% | Sparsity: 70.65%\nEpoch 31/50 | Train Loss: 0.0680 | Train Acc: 99.70% | Val Loss: 0.1034 | Val Acc: 96.56% | Sparsity: 70.65%\nEpoch 32/50 | Train Loss: 0.0657 | Train Acc: 99.72% | Val Loss: 0.1050 | Val Acc: 96.70% | Sparsity: 70.65%\nEpoch 33/50 | Train Loss: 0.0638 | Train Acc: 99.72% | Val Loss: 0.1078 | Val Acc: 96.62% | Sparsity: 70.65%\nEpoch 34/50 | Train Loss: 0.0616 | Train Acc: 99.72% | Val Loss: 0.1066 | Val Acc: 96.59% | Sparsity: 70.65%\nEpoch 35/50 | Train Loss: 0.0617 | Train Acc: 99.72% | Val Loss: 0.1084 | Val Acc: 96.57% | Sparsity: 70.65%\nEpoch 36/50 | Train Loss: 0.0609 | Train Acc: 99.74% | Val Loss: 0.1111 | Val Acc: 96.50% | Sparsity: 70.65%\nEpoch 37/50 | Train Loss: 0.0604 | Train Acc: 99.69% | Val Loss: 0.1056 | Val Acc: 96.69% | Sparsity: 70.65%\nEarly stopping triggered at epoch 37. No improvement for 5 epochs.\nStudent model saved before pruning at: pruned_student_retrain_KD_80%.pth\nTotal Training Time: 140m 15s\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"student_accuracy = evaluate(pruned_student, test_loader, device)\nprint(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T10:06:21.190858Z","iopub.execute_input":"2025-03-16T10:06:21.191170Z","iopub.status.idle":"2025-03-16T10:06:42.711713Z","shell.execute_reply.started":"2025-03-16T10:06:21.191144Z","shell.execute_reply":"2025-03-16T10:06:42.710815Z"}},"outputs":[{"name":"stdout","text":" Retrained Pruned Student Model Test Accuracy: 95.38%\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# 50% Sparsity","metadata":{}},{"cell_type":"code","source":"# Load pretrained ResNet-18 (Student Model)\nstudent = models.resnet34(pretrained=True)\n# Modify the final fully connected layer for 10 classes (CIFAR-10)\nstudent.fc = nn.Linear(student.fc.in_features, 10)\nstudent = student.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:34.230094Z","iopub.execute_input":"2025-03-16T18:53:34.230436Z","iopub.status.idle":"2025-03-16T18:53:34.595918Z","shell.execute_reply.started":"2025-03-16T18:53:34.230408Z","shell.execute_reply":"2025-03-16T18:53:34.595281Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\nmodel_path = '/kaggle/input/student_resnet_34/pytorch/default/1/student_before_pruning_Res34.pth'\n# Load the model weights\nstudent.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:53:34.628973Z","iopub.execute_input":"2025-03-16T18:53:34.629379Z","iopub.status.idle":"2025-03-16T18:53:35.830013Z","shell.execute_reply.started":"2025-03-16T18:53:34.629344Z","shell.execute_reply":"2025-03-16T18:53:35.829176Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-18-5d6657d12f2d>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Measure inference times\nteacher_inference_time = measure_inference_time(teacher, test_loader, device)\nstudent_inference_time = measure_inference_time(student, test_loader, device)\nprint(f\"Teacher Model Inference Time: {teacher_inference_time * 1000:.2f} ms per batch\")\nprint(f\"Student Model Inference Time(Before Pruning): {student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:37:39.068875Z","iopub.execute_input":"2025-03-02T14:37:39.069163Z","iopub.status.idle":"2025-03-02T14:40:32.043628Z","shell.execute_reply.started":"2025-03-02T14:37:39.069143Z","shell.execute_reply":"2025-03-02T14:40:32.042717Z"}},"outputs":[{"name":"stdout","text":"Teacher Model Inference Time: 7.42 ms per batch\nStudent Model Inference Time(Before Pruning): 3.67 ms per batch\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Calculate sparsity\nsparsity = calculate_sparsity(student)\nprint(f\"Sparsity Before Pruning: {sparsity * 100:.2f}%\")\n\nteacher_accuracy = evaluate(teacher, test_loader, device)\nstudent_accuracy = evaluate(student, test_loader, device)\nprint(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\nprint(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:40:32.044802Z","iopub.execute_input":"2025-03-02T14:40:32.045047Z","iopub.status.idle":"2025-03-02T14:41:23.907346Z","shell.execute_reply.started":"2025-03-02T14:40:32.045028Z","shell.execute_reply":"2025-03-02T14:41:23.906453Z"}},"outputs":[{"name":"stdout","text":"Sparsity Before Pruning: 0.00%\nTeacher Model Test Accuracy: 95.41%\nStudent Model Test Accuracy Before Pruning: 95.67%\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Calculate sparsity\nsparsity = calculate_sparsity(student)\nprint(f\"Sparsity Before Pruning: {sparsity * 100:.2f}%\")\n\nteacher_accuracy = evaluate(teacher, test_loader, device)\nstudent_accuracy = evaluate(student, test_loader, device)\nprint(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\nprint(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:41:23.909047Z","iopub.execute_input":"2025-03-02T14:41:23.909293Z","iopub.status.idle":"2025-03-02T14:42:15.962703Z","shell.execute_reply.started":"2025-03-02T14:41:23.909253Z","shell.execute_reply":"2025-03-02T14:42:15.961854Z"}},"outputs":[{"name":"stdout","text":"Sparsity Before Pruning: 0.00%\nTeacher Model Test Accuracy: 95.41%\nStudent Model Test Accuracy Before Pruning: 95.67%\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Pruning\nprint(\"Calculating Important Scores\")\nstart_time = time.time()\nimportance_scores = compute_gradient_importance(\n    teacher, student, train_loader, device, temperature=5.0, alpha=0.5\n)\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n\nprint(\"Pruning the model\")\nstart_time = time.time()\npruned_student = prune_model(student, importance_scores, prune_ratio=0.505,pruning_type='unstructured')\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\nstudent = student.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:54:12.473582Z","iopub.execute_input":"2025-03-16T18:54:12.473898Z","iopub.status.idle":"2025-03-16T18:56:51.876577Z","shell.execute_reply.started":"2025-03-16T18:54:12.473877Z","shell.execute_reply":"2025-03-16T18:56:51.875603Z"}},"outputs":[{"name":"stdout","text":"Calculating Important Scores\nTotal Time take to calculate Important scores: 2m 39s\nPruning the model\nTotal Time take to prune the model scores: 0m 0s\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"student_accuracy = evaluate(pruned_student, test_loader, device)\nprint(f\"Student Model Test Accuracy After Pruning: {student_accuracy:.2f}%\")\nsparsity = calculate_sparsity(pruned_student)\nprint(f\"Sparsity After Pruning: {sparsity * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T17:54:42.496810Z","iopub.execute_input":"2025-03-16T17:54:42.497021Z","iopub.status.idle":"2025-03-16T17:55:01.275081Z","shell.execute_reply.started":"2025-03-16T17:54:42.496990Z","shell.execute_reply":"2025-03-16T17:55:01.274386Z"}},"outputs":[{"name":"stdout","text":"Student Model Test Accuracy After Pruning: 10.00%\nSparsity After Pruning: 50.47%\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# Calculate sparsity\nsparsity = calculate_sparsity(student)\nprint(f\"Sparsity Before Pruning: {sparsity * 100:.2f}%\")\n\nteacher_accuracy = evaluate(teacher, test_loader, device)\nstudent_accuracy = evaluate(student, test_loader, device)\nprint(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\nprint(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:44:34.014169Z","iopub.execute_input":"2025-03-02T14:44:34.014460Z","iopub.status.idle":"2025-03-02T14:45:25.955374Z","shell.execute_reply.started":"2025-03-02T14:44:34.014426Z","shell.execute_reply":"2025-03-02T14:45:25.954414Z"}},"outputs":[{"name":"stdout","text":"Sparsity Before Pruning: 50.46%\nTeacher Model Test Accuracy: 95.41%\nStudent Model Test Accuracy Before Pruning: 10.00%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Measure inference times\npruned_student_inference_time = measure_inference_time(pruned_student, test_loader, device)\nprint(f\"Student Model Inference Time(After Pruning): {pruned_student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T14:45:47.380482Z","iopub.execute_input":"2025-03-02T14:45:47.380708Z","iopub.status.idle":"2025-03-02T14:47:15.650808Z","shell.execute_reply.started":"2025-03-02T14:45:47.380689Z","shell.execute_reply":"2025-03-02T14:47:15.650052Z"}},"outputs":[{"name":"stdout","text":"Student Model Inference Time(After Pruning): 3.70 ms per batch\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"retrained_student = retrain_with_sparsity(\n    pruned_student, train_loader, val_loader,\n    epochs=200,  save_path='/kaggle/working/retrained_student_model_50%.pt',patience=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:56:51.877889Z","iopub.execute_input":"2025-03-16T18:56:51.878271Z","iopub.status.idle":"2025-03-16T19:45:00.677093Z","shell.execute_reply.started":"2025-03-16T18:56:51.878237Z","shell.execute_reply":"2025-03-16T19:45:00.676060Z"}},"outputs":[{"name":"stdout","text":"New best model saved with Val Accuracy: 97.55%\nEpoch 1/200 | Train Loss: 0.2492 | Train Acc: 91.97%\nValidation Loss: 0.0762 | Validation Acc: 97.55% | Sparsity: 50.47%\n\nEpoch 2/200 | Train Loss: 0.0402 | Train Acc: 98.74%\nValidation Loss: 0.0758 | Validation Acc: 97.50% | Sparsity: 50.47%\n\nNew best model saved with Val Accuracy: 97.87%\nEpoch 3/200 | Train Loss: 0.0150 | Train Acc: 99.53%\nValidation Loss: 0.0688 | Validation Acc: 97.87% | Sparsity: 50.47%\n\nNew best model saved with Val Accuracy: 97.98%\nEpoch 4/200 | Train Loss: 0.0078 | Train Acc: 99.77%\nValidation Loss: 0.0695 | Validation Acc: 97.98% | Sparsity: 50.47%\n\nNew best model saved with Val Accuracy: 98.08%\nEpoch 5/200 | Train Loss: 0.0051 | Train Acc: 99.89%\nValidation Loss: 0.0681 | Validation Acc: 98.08% | Sparsity: 50.47%\n\nNew best model saved with Val Accuracy: 98.11%\nEpoch 6/200 | Train Loss: 0.0023 | Train Acc: 99.95%\nValidation Loss: 0.0679 | Validation Acc: 98.11% | Sparsity: 50.47%\n\nNew best model saved with Val Accuracy: 98.16%\nEpoch 7/200 | Train Loss: 0.0015 | Train Acc: 99.97%\nValidation Loss: 0.0670 | Validation Acc: 98.16% | Sparsity: 50.47%\n\nNew best model saved with Val Accuracy: 98.18%\nEpoch 8/200 | Train Loss: 0.0010 | Train Acc: 99.99%\nValidation Loss: 0.0683 | Validation Acc: 98.18% | Sparsity: 50.47%\n\nNew best model saved with Val Accuracy: 98.20%\nEpoch 9/200 | Train Loss: 0.0009 | Train Acc: 99.99%\nValidation Loss: 0.0672 | Validation Acc: 98.20% | Sparsity: 50.47%\n\nEpoch 10/200 | Train Loss: 0.0012 | Train Acc: 99.97%\nValidation Loss: 0.0685 | Validation Acc: 98.18% | Sparsity: 50.47%\n\nNew best model saved with Val Accuracy: 98.30%\nEpoch 11/200 | Train Loss: 0.0008 | Train Acc: 99.99%\nValidation Loss: 0.0646 | Validation Acc: 98.30% | Sparsity: 50.47%\n\nEpoch 12/200 | Train Loss: 0.0004 | Train Acc: 100.00%\nValidation Loss: 0.0646 | Validation Acc: 98.25% | Sparsity: 50.47%\n\nEpoch 13/200 | Train Loss: 0.0003 | Train Acc: 100.00%\nValidation Loss: 0.0676 | Validation Acc: 98.13% | Sparsity: 50.47%\n\nEpoch 14/200 | Train Loss: 0.0002 | Train Acc: 100.00%\nValidation Loss: 0.0715 | Validation Acc: 98.20% | Sparsity: 50.47%\n\nEpoch 15/200 | Train Loss: 0.0002 | Train Acc: 100.00%\nValidation Loss: 0.0709 | Validation Acc: 98.22% | Sparsity: 50.47%\n\nEarly stopping triggered at epoch 16. No improvement for 5 epochs.\nBest Validation Accuracy: 98.30% | Best Model Saved at: /kaggle/working/retrained_student_model_50%.pt\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"student_accuracy = evaluate(retrained_student, test_loader, device)\nprint(f\"Student Model Test Accuracy After retraining: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T19:45:00.678793Z","iopub.execute_input":"2025-03-16T19:45:00.679018Z","iopub.status.idle":"2025-03-16T19:45:24.839897Z","shell.execute_reply.started":"2025-03-16T19:45:00.679000Z","shell.execute_reply":"2025-03-16T19:45:24.839116Z"}},"outputs":[{"name":"stdout","text":"Student Model Test Accuracy After retraining: 95.93%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Retraining with KD","metadata":{}},{"cell_type":"code","source":"# Load pretrained ResNet-18 (Student Model)\nstudent = models.resnet34(pretrained=True)\n# Modify the final fully connected layer for 10 classes (CIFAR-10)\nstudent.fc = nn.Linear(student.fc.in_features, 10)\nstudent = student.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T19:45:24.841086Z","iopub.execute_input":"2025-03-16T19:45:24.841379Z","iopub.status.idle":"2025-03-16T19:45:25.252464Z","shell.execute_reply.started":"2025-03-16T19:45:24.841356Z","shell.execute_reply":"2025-03-16T19:45:25.251548Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"\nmodel_path = '/kaggle/input/student_resnet_34/pytorch/default/1/student_before_pruning_Res34.pth'\n# Load the model weights\nstudent.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T19:45:25.253290Z","iopub.execute_input":"2025-03-16T19:45:25.253548Z","iopub.status.idle":"2025-03-16T19:45:25.344979Z","shell.execute_reply.started":"2025-03-16T19:45:25.253520Z","shell.execute_reply":"2025-03-16T19:45:25.344131Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-23-5d6657d12f2d>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# Pruning\nprint(\"Calculating Important Scores\")\nstart_time = time.time()\nimportance_scores = compute_gradient_importance(\n    teacher, student, train_loader, device, temperature=5.0, alpha=0.5\n)\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n\nprint(\"Pruning the model\")\nstart_time = time.time()\npruned_student = prune_model(student, importance_scores, prune_ratio=0.505,pruning_type='unstructured')\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\nstudent = student.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T19:45:25.345983Z","iopub.execute_input":"2025-03-16T19:45:25.346261Z","iopub.status.idle":"2025-03-16T19:48:03.630436Z","shell.execute_reply.started":"2025-03-16T19:45:25.346234Z","shell.execute_reply":"2025-03-16T19:48:03.629724Z"}},"outputs":[{"name":"stdout","text":"Calculating Important Scores\nTotal Time take to calculate Important scores: 2m 38s\nPruning the model\nTotal Time take to prune the model scores: 0m 0s\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"pruned_student = retrain_with_KD(\n    teacher, pruned_student, train_loader, val_loader,\n    epochs=50, temperature=5.0, alpha=0.5, patience=5,save_path=\"pruned_student_retrain_KD_80%.pth\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T19:48:03.631311Z","iopub.execute_input":"2025-03-16T19:48:03.631633Z","iopub.status.idle":"2025-03-16T21:50:26.743255Z","shell.execute_reply.started":"2025-03-16T19:48:03.631603Z","shell.execute_reply":"2025-03-16T21:50:26.742291Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50 | Train Loss: 0.6577 | Train Acc: 93.51% | Val Loss: 0.0915 | Val Acc: 96.89% | Sparsity: 50.47%\nEpoch 2/50 | Train Loss: 0.2328 | Train Acc: 98.49% | Val Loss: 0.0687 | Val Acc: 97.78% | Sparsity: 50.47%\nEpoch 3/50 | Train Loss: 0.1589 | Train Acc: 99.28% | Val Loss: 0.0626 | Val Acc: 97.99% | Sparsity: 50.47%\nEpoch 4/50 | Train Loss: 0.1247 | Train Acc: 99.49% | Val Loss: 0.0573 | Val Acc: 98.04% | Sparsity: 50.47%\nEpoch 5/50 | Train Loss: 0.1069 | Train Acc: 99.57% | Val Loss: 0.0550 | Val Acc: 98.07% | Sparsity: 50.47%\nEpoch 6/50 | Train Loss: 0.0957 | Train Acc: 99.53% | Val Loss: 0.0569 | Val Acc: 97.99% | Sparsity: 50.47%\nEpoch 7/50 | Train Loss: 0.0871 | Train Acc: 99.63% | Val Loss: 0.0518 | Val Acc: 98.26% | Sparsity: 50.47%\nEpoch 8/50 | Train Loss: 0.0805 | Train Acc: 99.60% | Val Loss: 0.0522 | Val Acc: 98.24% | Sparsity: 50.47%\nEpoch 9/50 | Train Loss: 0.0762 | Train Acc: 99.64% | Val Loss: 0.0521 | Val Acc: 98.24% | Sparsity: 50.47%\nEpoch 10/50 | Train Loss: 0.0727 | Train Acc: 99.62% | Val Loss: 0.0529 | Val Acc: 98.11% | Sparsity: 50.47%\nEpoch 11/50 | Train Loss: 0.0697 | Train Acc: 99.58% | Val Loss: 0.0520 | Val Acc: 98.20% | Sparsity: 50.47%\nEpoch 12/50 | Train Loss: 0.0670 | Train Acc: 99.61% | Val Loss: 0.0500 | Val Acc: 98.28% | Sparsity: 50.47%\nEpoch 13/50 | Train Loss: 0.0630 | Train Acc: 99.65% | Val Loss: 0.0512 | Val Acc: 98.30% | Sparsity: 50.47%\nEpoch 14/50 | Train Loss: 0.0621 | Train Acc: 99.65% | Val Loss: 0.0539 | Val Acc: 98.17% | Sparsity: 50.47%\nEpoch 15/50 | Train Loss: 0.0605 | Train Acc: 99.63% | Val Loss: 0.0524 | Val Acc: 98.27% | Sparsity: 50.47%\nEpoch 16/50 | Train Loss: 0.0589 | Train Acc: 99.63% | Val Loss: 0.0510 | Val Acc: 98.29% | Sparsity: 50.47%\nEpoch 17/50 | Train Loss: 0.0568 | Train Acc: 99.64% | Val Loss: 0.0512 | Val Acc: 98.25% | Sparsity: 50.47%\nEpoch 18/50 | Train Loss: 0.0563 | Train Acc: 99.64% | Val Loss: 0.0508 | Val Acc: 98.26% | Sparsity: 50.47%\nEpoch 19/50 | Train Loss: 0.0545 | Train Acc: 99.67% | Val Loss: 0.0507 | Val Acc: 98.25% | Sparsity: 50.47%\nEpoch 20/50 | Train Loss: 0.0532 | Train Acc: 99.67% | Val Loss: 0.0514 | Val Acc: 98.33% | Sparsity: 50.47%\nEpoch 21/50 | Train Loss: 0.0523 | Train Acc: 99.69% | Val Loss: 0.0509 | Val Acc: 98.29% | Sparsity: 50.47%\nEpoch 22/50 | Train Loss: 0.0522 | Train Acc: 99.68% | Val Loss: 0.0523 | Val Acc: 98.22% | Sparsity: 50.47%\nEpoch 23/50 | Train Loss: 0.0508 | Train Acc: 99.64% | Val Loss: 0.0536 | Val Acc: 98.18% | Sparsity: 50.47%\nEpoch 24/50 | Train Loss: 0.0494 | Train Acc: 99.67% | Val Loss: 0.0533 | Val Acc: 98.20% | Sparsity: 50.47%\nEpoch 25/50 | Train Loss: 0.0491 | Train Acc: 99.65% | Val Loss: 0.0501 | Val Acc: 98.36% | Sparsity: 50.47%\nEpoch 26/50 | Train Loss: 0.0477 | Train Acc: 99.75% | Val Loss: 0.0506 | Val Acc: 98.28% | Sparsity: 50.47%\nEpoch 27/50 | Train Loss: 0.0475 | Train Acc: 99.65% | Val Loss: 0.0519 | Val Acc: 98.29% | Sparsity: 50.47%\nEpoch 28/50 | Train Loss: 0.0465 | Train Acc: 99.66% | Val Loss: 0.0514 | Val Acc: 98.18% | Sparsity: 50.47%\nEpoch 29/50 | Train Loss: 0.0467 | Train Acc: 99.69% | Val Loss: 0.0518 | Val Acc: 98.33% | Sparsity: 50.47%\nEpoch 30/50 | Train Loss: 0.0456 | Train Acc: 99.67% | Val Loss: 0.0514 | Val Acc: 98.20% | Sparsity: 50.47%\nEarly stopping triggered at epoch 30. No improvement for 5 epochs.\nStudent model saved before pruning at: pruned_student_retrain_KD_80%.pth\nTotal Training Time: 122m 23s\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"student_accuracy = evaluate(pruned_student, test_loader, device)\nprint(f\"Student Model Test Accuracy After retraining: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T21:50:26.744963Z","iopub.execute_input":"2025-03-16T21:50:26.745228Z","iopub.status.idle":"2025-03-16T21:50:50.520164Z","shell.execute_reply.started":"2025-03-16T21:50:26.745186Z","shell.execute_reply":"2025-03-16T21:50:50.519138Z"}},"outputs":[{"name":"stdout","text":"Student Model Test Accuracy After retraining: 96.22%\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Retrain with 20% Training Data","metadata":{}},{"cell_type":"code","source":"# Load pretrained ResNet-18 (Student Model)\nstudent = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n# Modify the final fully connected layer for 10 classes (CIFAR-10)\nstudent.fc = nn.Linear(student.fc.in_features, 10)\nstudent = student.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T16:09:58.572978Z","iopub.execute_input":"2025-03-03T16:09:58.573255Z","iopub.status.idle":"2025-03-03T16:09:58.768945Z","shell.execute_reply.started":"2025-03-03T16:09:58.573236Z","shell.execute_reply":"2025-03-03T16:09:58.768213Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\nmodel_path = '/kaggle/input/best_student_resnet18/pytorch/default/1/student_before_pruning.pth'\n# Load the model weights\nstudent.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T16:09:58.853025Z","iopub.execute_input":"2025-03-03T16:09:58.853293Z","iopub.status.idle":"2025-03-03T16:09:59.608391Z","shell.execute_reply.started":"2025-03-03T16:09:58.853272Z","shell.execute_reply":"2025-03-03T16:09:59.607650Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-17-b666940780fb>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  student.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Measure inference times\nteacher_inference_time = measure_inference_time(teacher, test_loader, device)\nstudent_inference_time = measure_inference_time(student, test_loader, device)\nprint(f\"Teacher Model Inference Time: {teacher_inference_time * 1000:.2f} ms per batch\")\nprint(f\"Student Model Inference Time(Before Pruning): {student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate sparsity\nsparsity = calculate_sparsity(student)\nprint(f\"Sparsity Before Pruning: {sparsity * 100:.2f}%\")\n\nteacher_accuracy = evaluate(teacher, test_loader, device)\nstudent_accuracy = evaluate(student, test_loader, device)\nprint(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\nprint(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate sparsity\nsparsity = calculate_sparsity(student)\nprint(f\"Sparsity Before Pruning: {sparsity * 100:.2f}%\")\n\nteacher_accuracy = evaluate(teacher, test_loader, device)\nstudent_accuracy = evaluate(student, test_loader, device)\nprint(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\nprint(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pruning\nprint(\"Calculating Important Scores\")\nstart_time = time.time()\nimportance_scores = compute_gradient_importance(\n    teacher, student, train_loader, device, temperature=5.0, alpha=0.5\n)\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to calculate Important scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n\nprint(\"Pruning the model\")\nstart_time = time.time()\npruned_student = prune_model(student, importance_scores, prune_ratio=0.9,pruning_type='unstructured')\ntotal_time = time.time() - start_time\nprint(f\"Total Time take to prune the model scores: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\nstudent = student.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T16:10:48.173244Z","iopub.execute_input":"2025-03-03T16:10:48.173546Z","iopub.status.idle":"2025-03-03T16:11:20.361150Z","shell.execute_reply.started":"2025-03-03T16:10:48.173525Z","shell.execute_reply":"2025-03-03T16:11:20.360159Z"}},"outputs":[{"name":"stdout","text":"Calculating Important Scores\nTotal Time take to calculate Important scores: 0m 32s\nPruning the model\nTotal Time take to prune the model scores: 0m 0s\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"student_accuracy = evaluate(pruned_student, test_loader, device)\nprint(f\"Student Model Test Accuracy After Pruning: {student_accuracy:.2f}%\")\nsparsity = calculate_sparsity(pruned_student)\nprint(f\"Sparsity After Pruning: {sparsity * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T16:11:20.362398Z","iopub.execute_input":"2025-03-03T16:11:20.362702Z","iopub.status.idle":"2025-03-03T16:11:41.636582Z","shell.execute_reply.started":"2025-03-03T16:11:20.362680Z","shell.execute_reply":"2025-03-03T16:11:41.635834Z"}},"outputs":[{"name":"stdout","text":"Student Model Test Accuracy After Pruning: 10.00%\nSparsity After Pruning: 89.92%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\nteacher_accuracy = evaluate(teacher, test_loader, device)\nstudent_accuracy = evaluate(student, test_loader, device)\nprint(f\"Teacher Model Test Accuracy: {teacher_accuracy:.2f}%\")\nprint(f\"Student Model Test Accuracy Before Pruning: {student_accuracy:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Measure inference times\npruned_student_inference_time = measure_inference_time(pruned_student, test_loader, device)\nprint(f\"Student Model Inference Time(After Pruning): {pruned_student_inference_time * 1000:.2f} ms per batch\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\nfrom torchvision import datasets, transforms\n\n# Data augmentation for training\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n    transforms.RandomCrop(32, padding=4),  # Randomly crop the image\n    transforms.Resize(224),  # Resize to 224x224 for ResNet\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# No augmentation for validation and test\nval_test_transform = transforms.Compose([\n    transforms.Resize(224),  # Resize to 224x224 for ResNet\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load CIFAR-10 dataset\ntrain_val_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=val_test_transform)\n\n# Split train_val_dataset into train (20%) and validation (20% of original)\ntrain_size = int(0.2 * len(train_val_dataset))  # 20% of 50,000 = 10,000 samples\nremaining_size = len(train_val_dataset) - train_size\n\n# Use random_split to preserve class distribution\ntrain_dataset, remaining_data = random_split(train_val_dataset, [train_size, remaining_size])\n\n# Split remaining_data into validation (20% of original)\nval_size = int(0.2 * len(train_val_dataset))  # 20% of 50,000 = 10,000 samples\n_, val_dataset = random_split(remaining_data, [remaining_size - val_size, val_size])\n\n# Apply val_test_transform to the validation set\nval_dataset.dataset.transform = val_test_transform\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\n# Print dataset sizes\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T16:11:52.791336Z","iopub.execute_input":"2025-03-03T16:11:52.791634Z","iopub.status.idle":"2025-03-03T16:11:54.420239Z","shell.execute_reply.started":"2025-03-03T16:11:52.791612Z","shell.execute_reply":"2025-03-03T16:11:54.419329Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nTrain dataset size: 10000\nValidation dataset size: 10000\nTest dataset size: 10000\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"retrained_student = retrain_with_sparsity(\n    pruned_student, train_loader, val_loader,\n    epochs=200,  save_path='/kaggle/working/retrained_student_model_90%_20%_Train_Data.pt',patience=20\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T16:12:21.884151Z","iopub.execute_input":"2025-03-03T16:12:21.884455Z","iopub.status.idle":"2025-03-03T17:53:41.969896Z","shell.execute_reply.started":"2025-03-03T16:12:21.884430Z","shell.execute_reply":"2025-03-03T17:53:41.969052Z"}},"outputs":[{"name":"stdout","text":"New best model saved with Val Accuracy: 19.90%\nEpoch 1/200 | Train Loss: 2.1372 | Train Acc: 18.24%\nValidation Loss: 2.1238 | Validation Acc: 19.90% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 21.76%\nEpoch 2/200 | Train Loss: 1.9587 | Train Acc: 21.95%\nValidation Loss: 1.9937 | Validation Acc: 21.76% | Sparsity: 89.92%\n\nEpoch 3/200 | Train Loss: 1.9046 | Train Acc: 23.17%\nValidation Loss: 2.1815 | Validation Acc: 20.42% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 27.65%\nEpoch 4/200 | Train Loss: 1.8598 | Train Acc: 26.08%\nValidation Loss: 1.8995 | Validation Acc: 27.65% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 30.07%\nEpoch 5/200 | Train Loss: 1.7397 | Train Acc: 31.92%\nValidation Loss: 1.8803 | Validation Acc: 30.07% | Sparsity: 89.92%\n\nEpoch 6/200 | Train Loss: 1.6891 | Train Acc: 34.55%\nValidation Loss: 2.2614 | Validation Acc: 25.26% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 32.22%\nEpoch 7/200 | Train Loss: 1.6415 | Train Acc: 36.30%\nValidation Loss: 1.7675 | Validation Acc: 32.22% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 35.16%\nEpoch 8/200 | Train Loss: 1.6196 | Train Acc: 37.54%\nValidation Loss: 1.7185 | Validation Acc: 35.16% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 35.30%\nEpoch 9/200 | Train Loss: 1.5606 | Train Acc: 39.89%\nValidation Loss: 1.9490 | Validation Acc: 35.30% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 38.06%\nEpoch 10/200 | Train Loss: 1.5195 | Train Acc: 41.52%\nValidation Loss: 1.6523 | Validation Acc: 38.06% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 38.69%\nEpoch 11/200 | Train Loss: 1.4829 | Train Acc: 43.67%\nValidation Loss: 1.6188 | Validation Acc: 38.69% | Sparsity: 89.92%\n\nEpoch 12/200 | Train Loss: 1.4483 | Train Acc: 45.32%\nValidation Loss: 1.8757 | Validation Acc: 34.71% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 41.04%\nEpoch 13/200 | Train Loss: 1.4003 | Train Acc: 47.25%\nValidation Loss: 1.6041 | Validation Acc: 41.04% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 45.12%\nEpoch 14/200 | Train Loss: 1.3697 | Train Acc: 48.55%\nValidation Loss: 1.4356 | Validation Acc: 45.12% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 47.87%\nEpoch 15/200 | Train Loss: 1.3571 | Train Acc: 48.79%\nValidation Loss: 1.4217 | Validation Acc: 47.87% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 48.41%\nEpoch 16/200 | Train Loss: 1.2996 | Train Acc: 51.31%\nValidation Loss: 1.3734 | Validation Acc: 48.41% | Sparsity: 89.92%\n\nEpoch 17/200 | Train Loss: 1.2747 | Train Acc: 52.41%\nValidation Loss: 1.6438 | Validation Acc: 41.96% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 49.21%\nEpoch 18/200 | Train Loss: 1.2583 | Train Acc: 52.79%\nValidation Loss: 1.4143 | Validation Acc: 49.21% | Sparsity: 89.92%\n\nEpoch 19/200 | Train Loss: 1.2110 | Train Acc: 55.78%\nValidation Loss: 1.4690 | Validation Acc: 47.68% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 51.89%\nEpoch 20/200 | Train Loss: 1.1807 | Train Acc: 56.50%\nValidation Loss: 1.3188 | Validation Acc: 51.89% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 55.28%\nEpoch 21/200 | Train Loss: 1.1479 | Train Acc: 57.75%\nValidation Loss: 1.2263 | Validation Acc: 55.28% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 55.95%\nEpoch 22/200 | Train Loss: 1.1263 | Train Acc: 58.93%\nValidation Loss: 1.2696 | Validation Acc: 55.95% | Sparsity: 89.92%\n\nEpoch 23/200 | Train Loss: 1.0830 | Train Acc: 60.12%\nValidation Loss: 1.2581 | Validation Acc: 55.41% | Sparsity: 89.92%\n\nEpoch 24/200 | Train Loss: 1.0639 | Train Acc: 61.16%\nValidation Loss: 1.4308 | Validation Acc: 52.77% | Sparsity: 89.92%\n\nEpoch 25/200 | Train Loss: 1.0400 | Train Acc: 62.10%\nValidation Loss: 1.2835 | Validation Acc: 55.52% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 57.35%\nEpoch 26/200 | Train Loss: 1.0186 | Train Acc: 63.20%\nValidation Loss: 1.2047 | Validation Acc: 57.35% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 59.21%\nEpoch 27/200 | Train Loss: 0.9882 | Train Acc: 64.09%\nValidation Loss: 1.1661 | Validation Acc: 59.21% | Sparsity: 89.92%\n\nEpoch 28/200 | Train Loss: 0.9732 | Train Acc: 65.17%\nValidation Loss: 1.2347 | Validation Acc: 58.69% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 59.51%\nEpoch 29/200 | Train Loss: 0.9675 | Train Acc: 65.65%\nValidation Loss: 1.1385 | Validation Acc: 59.51% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 60.09%\nEpoch 30/200 | Train Loss: 0.9322 | Train Acc: 66.20%\nValidation Loss: 1.1109 | Validation Acc: 60.09% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 61.46%\nEpoch 31/200 | Train Loss: 0.9001 | Train Acc: 66.85%\nValidation Loss: 1.0758 | Validation Acc: 61.46% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 63.07%\nEpoch 32/200 | Train Loss: 0.8822 | Train Acc: 68.45%\nValidation Loss: 1.0465 | Validation Acc: 63.07% | Sparsity: 89.92%\n\nEpoch 33/200 | Train Loss: 0.9045 | Train Acc: 67.73%\nValidation Loss: 1.1635 | Validation Acc: 62.87% | Sparsity: 89.92%\n\nEpoch 34/200 | Train Loss: 0.8746 | Train Acc: 68.85%\nValidation Loss: 1.1855 | Validation Acc: 58.17% | Sparsity: 89.92%\n\nEpoch 35/200 | Train Loss: 0.8463 | Train Acc: 69.80%\nValidation Loss: 1.1033 | Validation Acc: 62.38% | Sparsity: 89.92%\n\nEpoch 36/200 | Train Loss: 0.8108 | Train Acc: 70.74%\nValidation Loss: 1.1955 | Validation Acc: 60.50% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 66.62%\nEpoch 37/200 | Train Loss: 0.8203 | Train Acc: 70.98%\nValidation Loss: 0.9392 | Validation Acc: 66.62% | Sparsity: 89.92%\n\nEpoch 38/200 | Train Loss: 0.8073 | Train Acc: 71.05%\nValidation Loss: 0.9972 | Validation Acc: 65.10% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 66.74%\nEpoch 39/200 | Train Loss: 0.7802 | Train Acc: 72.09%\nValidation Loss: 0.9630 | Validation Acc: 66.74% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 67.31%\nEpoch 40/200 | Train Loss: 0.7578 | Train Acc: 72.59%\nValidation Loss: 0.9306 | Validation Acc: 67.31% | Sparsity: 89.92%\n\nEpoch 41/200 | Train Loss: 0.7722 | Train Acc: 71.97%\nValidation Loss: 0.9763 | Validation Acc: 66.51% | Sparsity: 89.92%\n\nEpoch 42/200 | Train Loss: 0.7439 | Train Acc: 73.52%\nValidation Loss: 1.0510 | Validation Acc: 64.84% | Sparsity: 89.92%\n\nEpoch 43/200 | Train Loss: 0.7350 | Train Acc: 74.07%\nValidation Loss: 0.9608 | Validation Acc: 66.53% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 69.01%\nEpoch 44/200 | Train Loss: 0.7161 | Train Acc: 74.59%\nValidation Loss: 0.9123 | Validation Acc: 69.01% | Sparsity: 89.92%\n\nEpoch 45/200 | Train Loss: 0.7054 | Train Acc: 74.94%\nValidation Loss: 0.9357 | Validation Acc: 68.91% | Sparsity: 89.92%\n\nEpoch 46/200 | Train Loss: 0.6911 | Train Acc: 75.49%\nValidation Loss: 0.9998 | Validation Acc: 67.27% | Sparsity: 89.92%\n\nEpoch 47/200 | Train Loss: 0.6754 | Train Acc: 75.88%\nValidation Loss: 1.3372 | Validation Acc: 59.25% | Sparsity: 89.92%\n\nEpoch 48/200 | Train Loss: 0.6736 | Train Acc: 75.66%\nValidation Loss: 0.9203 | Validation Acc: 68.37% | Sparsity: 89.92%\n\nEpoch 49/200 | Train Loss: 0.6575 | Train Acc: 76.61%\nValidation Loss: 1.0371 | Validation Acc: 65.33% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 69.38%\nEpoch 50/200 | Train Loss: 0.6483 | Train Acc: 77.53%\nValidation Loss: 0.9127 | Validation Acc: 69.38% | Sparsity: 89.92%\n\nEpoch 51/200 | Train Loss: 0.6362 | Train Acc: 77.58%\nValidation Loss: 1.0692 | Validation Acc: 66.72% | Sparsity: 89.92%\n\nEpoch 52/200 | Train Loss: 0.6249 | Train Acc: 78.37%\nValidation Loss: 1.0418 | Validation Acc: 66.79% | Sparsity: 89.92%\n\nEpoch 53/200 | Train Loss: 0.6042 | Train Acc: 78.32%\nValidation Loss: 1.0159 | Validation Acc: 67.29% | Sparsity: 89.92%\n\nEpoch 54/200 | Train Loss: 0.5962 | Train Acc: 78.51%\nValidation Loss: 0.9941 | Validation Acc: 67.58% | Sparsity: 89.92%\n\nEpoch 55/200 | Train Loss: 0.6005 | Train Acc: 79.43%\nValidation Loss: 0.9688 | Validation Acc: 69.22% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 71.50%\nEpoch 56/200 | Train Loss: 0.5647 | Train Acc: 80.28%\nValidation Loss: 0.8503 | Validation Acc: 71.50% | Sparsity: 89.92%\n\nEpoch 57/200 | Train Loss: 0.5704 | Train Acc: 80.26%\nValidation Loss: 1.0224 | Validation Acc: 67.85% | Sparsity: 89.92%\n\nEpoch 58/200 | Train Loss: 0.5632 | Train Acc: 80.53%\nValidation Loss: 1.0259 | Validation Acc: 68.83% | Sparsity: 89.92%\n\nEpoch 59/200 | Train Loss: 0.5564 | Train Acc: 80.57%\nValidation Loss: 0.8706 | Validation Acc: 69.16% | Sparsity: 89.92%\n\nEpoch 60/200 | Train Loss: 0.5310 | Train Acc: 81.53%\nValidation Loss: 1.0218 | Validation Acc: 68.51% | Sparsity: 89.92%\n\nEpoch 61/200 | Train Loss: 0.5202 | Train Acc: 81.92%\nValidation Loss: 0.9208 | Validation Acc: 71.43% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 72.56%\nEpoch 62/200 | Train Loss: 0.5078 | Train Acc: 82.19%\nValidation Loss: 0.8672 | Validation Acc: 72.56% | Sparsity: 89.92%\n\nEpoch 63/200 | Train Loss: 0.4999 | Train Acc: 82.65%\nValidation Loss: 0.9286 | Validation Acc: 69.01% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 73.31%\nEpoch 64/200 | Train Loss: 0.4966 | Train Acc: 82.52%\nValidation Loss: 0.8423 | Validation Acc: 73.31% | Sparsity: 89.92%\n\nEpoch 65/200 | Train Loss: 0.4907 | Train Acc: 83.17%\nValidation Loss: 0.9364 | Validation Acc: 70.33% | Sparsity: 89.92%\n\nEpoch 66/200 | Train Loss: 0.4723 | Train Acc: 83.40%\nValidation Loss: 0.8796 | Validation Acc: 72.38% | Sparsity: 89.92%\n\nEpoch 67/200 | Train Loss: 0.4766 | Train Acc: 83.23%\nValidation Loss: 0.8453 | Validation Acc: 72.63% | Sparsity: 89.92%\n\nEpoch 68/200 | Train Loss: 0.4665 | Train Acc: 83.67%\nValidation Loss: 1.0177 | Validation Acc: 70.51% | Sparsity: 89.92%\n\nEpoch 69/200 | Train Loss: 0.4557 | Train Acc: 84.28%\nValidation Loss: 0.9592 | Validation Acc: 71.23% | Sparsity: 89.92%\n\nEpoch 70/200 | Train Loss: 0.4591 | Train Acc: 83.77%\nValidation Loss: 0.9672 | Validation Acc: 69.97% | Sparsity: 89.92%\n\nEpoch 71/200 | Train Loss: 0.4491 | Train Acc: 84.52%\nValidation Loss: 0.9165 | Validation Acc: 71.82% | Sparsity: 89.92%\n\nEpoch 72/200 | Train Loss: 0.4317 | Train Acc: 84.95%\nValidation Loss: 0.9026 | Validation Acc: 71.16% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 73.89%\nEpoch 73/200 | Train Loss: 0.4269 | Train Acc: 84.95%\nValidation Loss: 0.8220 | Validation Acc: 73.89% | Sparsity: 89.92%\n\nEpoch 74/200 | Train Loss: 0.4123 | Train Acc: 85.55%\nValidation Loss: 0.8559 | Validation Acc: 73.47% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 74.53%\nEpoch 75/200 | Train Loss: 0.4132 | Train Acc: 85.94%\nValidation Loss: 0.8284 | Validation Acc: 74.53% | Sparsity: 89.92%\n\nEpoch 76/200 | Train Loss: 0.4025 | Train Acc: 86.13%\nValidation Loss: 0.8803 | Validation Acc: 73.25% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 75.44%\nEpoch 77/200 | Train Loss: 0.3953 | Train Acc: 86.39%\nValidation Loss: 0.7990 | Validation Acc: 75.44% | Sparsity: 89.92%\n\nEpoch 78/200 | Train Loss: 0.3782 | Train Acc: 87.13%\nValidation Loss: 0.9216 | Validation Acc: 72.71% | Sparsity: 89.92%\n\nEpoch 79/200 | Train Loss: 0.3886 | Train Acc: 86.53%\nValidation Loss: 0.9556 | Validation Acc: 71.50% | Sparsity: 89.92%\n\nEpoch 80/200 | Train Loss: 0.3786 | Train Acc: 86.67%\nValidation Loss: 0.7905 | Validation Acc: 74.80% | Sparsity: 89.92%\n\nEpoch 81/200 | Train Loss: 0.3568 | Train Acc: 87.59%\nValidation Loss: 0.8690 | Validation Acc: 74.11% | Sparsity: 89.92%\n\nEpoch 82/200 | Train Loss: 0.3605 | Train Acc: 87.08%\nValidation Loss: 0.8442 | Validation Acc: 74.51% | Sparsity: 89.92%\n\nEpoch 83/200 | Train Loss: 0.3596 | Train Acc: 87.87%\nValidation Loss: 0.9788 | Validation Acc: 71.55% | Sparsity: 89.92%\n\nEpoch 84/200 | Train Loss: 0.3575 | Train Acc: 87.29%\nValidation Loss: 0.9999 | Validation Acc: 71.14% | Sparsity: 89.92%\n\nEpoch 85/200 | Train Loss: 0.3375 | Train Acc: 88.09%\nValidation Loss: 0.9123 | Validation Acc: 74.22% | Sparsity: 89.92%\n\nEpoch 86/200 | Train Loss: 0.3280 | Train Acc: 88.51%\nValidation Loss: 0.8298 | Validation Acc: 75.06% | Sparsity: 89.92%\n\nEpoch 87/200 | Train Loss: 0.3333 | Train Acc: 88.58%\nValidation Loss: 0.8975 | Validation Acc: 73.35% | Sparsity: 89.92%\n\nEpoch 88/200 | Train Loss: 0.3274 | Train Acc: 88.72%\nValidation Loss: 0.8452 | Validation Acc: 74.44% | Sparsity: 89.92%\n\nEpoch 89/200 | Train Loss: 0.3220 | Train Acc: 88.97%\nValidation Loss: 0.9576 | Validation Acc: 73.53% | Sparsity: 89.92%\n\nEpoch 90/200 | Train Loss: 0.3132 | Train Acc: 89.14%\nValidation Loss: 0.9366 | Validation Acc: 74.76% | Sparsity: 89.92%\n\nNew best model saved with Val Accuracy: 75.83%\nEpoch 91/200 | Train Loss: 0.2932 | Train Acc: 89.85%\nValidation Loss: 0.8272 | Validation Acc: 75.83% | Sparsity: 89.92%\n\nEpoch 92/200 | Train Loss: 0.2891 | Train Acc: 90.04%\nValidation Loss: 0.8341 | Validation Acc: 75.83% | Sparsity: 89.92%\n\nEpoch 93/200 | Train Loss: 0.2989 | Train Acc: 89.60%\nValidation Loss: 0.9260 | Validation Acc: 74.44% | Sparsity: 89.92%\n\nEpoch 94/200 | Train Loss: 0.2847 | Train Acc: 90.38%\nValidation Loss: 0.9328 | Validation Acc: 73.63% | Sparsity: 89.92%\n\nEpoch 95/200 | Train Loss: 0.2859 | Train Acc: 90.29%\nValidation Loss: 0.9200 | Validation Acc: 74.27% | Sparsity: 89.92%\n\nEpoch 96/200 | Train Loss: 0.2783 | Train Acc: 90.36%\nValidation Loss: 0.9576 | Validation Acc: 73.38% | Sparsity: 89.92%\n\nEpoch 97/200 | Train Loss: 0.2894 | Train Acc: 90.14%\nValidation Loss: 0.8934 | Validation Acc: 74.47% | Sparsity: 89.92%\n\nEpoch 98/200 | Train Loss: 0.2803 | Train Acc: 90.06%\nValidation Loss: 0.9221 | Validation Acc: 74.39% | Sparsity: 89.92%\n\nEpoch 99/200 | Train Loss: 0.2598 | Train Acc: 90.92%\nValidation Loss: 0.9292 | Validation Acc: 75.56% | Sparsity: 89.92%\n\nEpoch 100/200 | Train Loss: 0.2668 | Train Acc: 90.75%\nValidation Loss: 1.0076 | Validation Acc: 73.28% | Sparsity: 89.92%\n\nEpoch 101/200 | Train Loss: 0.2558 | Train Acc: 91.32%\nValidation Loss: 0.9175 | Validation Acc: 74.93% | Sparsity: 89.92%\n\nEpoch 102/200 | Train Loss: 0.2476 | Train Acc: 91.09%\nValidation Loss: 0.9650 | Validation Acc: 74.56% | Sparsity: 89.92%\n\nEpoch 103/200 | Train Loss: 0.2630 | Train Acc: 91.17%\nValidation Loss: 0.9678 | Validation Acc: 72.85% | Sparsity: 89.92%\n\nEpoch 104/200 | Train Loss: 0.2328 | Train Acc: 91.89%\nValidation Loss: 0.9219 | Validation Acc: 75.11% | Sparsity: 89.92%\n\nEpoch 105/200 | Train Loss: 0.2431 | Train Acc: 91.56%\nValidation Loss: 0.8821 | Validation Acc: 75.65% | Sparsity: 89.92%\n\nEpoch 106/200 | Train Loss: 0.2404 | Train Acc: 91.44%\nValidation Loss: 0.9949 | Validation Acc: 75.70% | Sparsity: 89.92%\n\nEpoch 107/200 | Train Loss: 0.2405 | Train Acc: 91.51%\nValidation Loss: 1.0554 | Validation Acc: 73.17% | Sparsity: 89.92%\n\nEpoch 108/200 | Train Loss: 0.2253 | Train Acc: 91.88%\nValidation Loss: 0.9483 | Validation Acc: 74.80% | Sparsity: 89.92%\n\nEpoch 109/200 | Train Loss: 0.2274 | Train Acc: 92.29%\nValidation Loss: 0.9702 | Validation Acc: 73.79% | Sparsity: 89.92%\n\nEpoch 110/200 | Train Loss: 0.2138 | Train Acc: 92.58%\nValidation Loss: 0.9656 | Validation Acc: 74.76% | Sparsity: 89.92%\n\nEarly stopping triggered at epoch 111. No improvement for 20 epochs.\nBest Validation Accuracy: 75.83% | Best Model Saved at: /kaggle/working/retrained_student_model_90%_20%_Train_Data.pt\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"student_accuracy = evaluate(retrained_student, test_loader, device)\nprint(f\" Retrained Pruned Student Model Test Accuracy: {student_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T17:54:03.117737Z","iopub.execute_input":"2025-03-03T17:54:03.117992Z","iopub.status.idle":"2025-03-03T17:54:23.955897Z","shell.execute_reply.started":"2025-03-03T17:54:03.117971Z","shell.execute_reply":"2025-03-03T17:54:23.954996Z"}},"outputs":[{"name":"stdout","text":" Retrained Pruned Student Model Test Accuracy: 75.18%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}